{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C4JU0h6Y2Gf"
   },
   "source": [
    "Gamper et al. (2019) [1], the PanNuke dataset is an open pan-cancer histology dataset for nuclei instance segmentation and classification.\n",
    "\n",
    "Gamper et al. (2020) [2], the PanNuke dataset has been extended with additional annotations and features to support more advanced applications of nuclei classification and segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] J. Gamper, N. Alemi Koohbanani, K. Benes, A. Khurram, and N. Rajpoot, \"PanNuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification,\" in *European Congress on Digital Pathology*, Springer, 2019, pp. 11-19. doi: 10.1007/978-3-030-25970-8_2\n",
    "\n",
    "[2] J. Gamper, N. Alemi Koohbanani, S. Graham, M. Jahanifar, S. A. Khurram, A. Azam, K. Hewitt, and N. Rajpoot, \"PanNuke Dataset Extension, Insights and Baselines,\" *arXiv preprint arXiv:2003.10778*, 2020.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROjatJ8ZRefV",
    "outputId": "08def790-e32b-4c2f-85aa-cca6c9ff6c1c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "589pevGfdP-a",
    "outputId": "023da047-6c3f-4a96-9501-29c563db9082"
   },
   "outputs": [],
   "source": [
    "# get the data\n",
    "\n",
    "#!unzip \"/content/drive/MyDrive/pannuke_processed.zip\"\n",
    "#!unzip \"/content/drive/MyDrive/pannuke_over_times_3.zip\" -d \"/content/pannuke_over_times_3\"\n",
    "!unzip \"/content/drive/MyDrive/pannuke_instances.zip\" -d \"/content/pannuke_over_times_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqElM2MY-ueF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------#\n",
    "# Same Code as in hrnet_v2_ocr_pannuke_multi                    #\n",
    "# but with different training methods and data/ data preperation#\n",
    "#---------------------------------------------------------------#\n",
    "class Data:\n",
    "    def load_npy(path, r, size):\n",
    "        data_ = np.load(path)\n",
    "        data = data_[:r]\n",
    "        resized_data = []\n",
    "        for d in data:\n",
    "            resized_d = cv2.resize(d, (size, size))\n",
    "            resized_data.append(resized_d)\n",
    "        del data_\n",
    "        return np.array(resized_data, dtype=np.float32)\n",
    "\n",
    "    def generator(path, r, size, batch_size):\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while True:\n",
    "            data_ = np.load(path)\n",
    "            data = data_[start:end]\n",
    "            resized_data = []\n",
    "            for d in data:\n",
    "                resized_d = cv2.resize(d, (size, size))\n",
    "                resized_data.append(resized_d)\n",
    "            yield np.array(resized_data)\n",
    "            start = end\n",
    "            end += batch_size\n",
    "            if end > r:\n",
    "                break\n",
    "\n",
    "    def load_numpys(directory_path):\n",
    "        # Initialize empty list to hold data arrays\n",
    "        data = []\n",
    "\n",
    "        # Loop through files in directory\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".npy\"):\n",
    "                # Load data from .npy file and append to list\n",
    "                data.append(np.load(os.path.join(directory_path, filename)))\n",
    "\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        return data\n",
    "\n",
    "    def pipe(x, y, size, num_classes=6, r=None, normalize=True):\n",
    "        fold_1_im = np.load(x)\n",
    "        fold_1_ma = np.load(y)\n",
    "        if r is not None:\n",
    "            fold_1_im[:r]\n",
    "            fold_1_ma[:r]\n",
    "        input_width, input_height = size, size\n",
    "        preprocessed_images = []\n",
    "        preprocessed_masks = []\n",
    "        for i in tqdm(range(len(fold_1_im)), colour=\"#d13516\"):\n",
    "            # Resize the image and mask to the input size of your model\n",
    "            resized_image = cv2.resize(fold_1_im[i], (input_width, input_height))\n",
    "            resized_mask = cv2.resize(fold_1_ma[i], (input_width, input_height))\n",
    "\n",
    "            # One-hot encode the mask\n",
    "            one_hot_mask = np.zeros(\n",
    "                (input_width, input_height, num_classes), dtype=np.float32\n",
    "            )\n",
    "            for c in range(num_classes):\n",
    "                one_hot_mask[:, :, c] = (resized_mask[:, :, 0] == c).astype(np.float32)\n",
    "\n",
    "            # Normalize the pixel values of the image\n",
    "            normalized_image = resized_image / 255.0\n",
    "\n",
    "            # Append the preprocessed image and mask to the list\n",
    "            preprocessed_images.append(normalized_image)\n",
    "            preprocessed_masks.append(one_hot_mask)\n",
    "\n",
    "        # Convert the preprocessed images and masks to numpy arrays\n",
    "        preprocessed_images = np.array(preprocessed_images)\n",
    "        preprocessed_masks = np.array(preprocessed_masks)\n",
    "        del fold_1_im\n",
    "        del fold_1_ma\n",
    "        # Print the dimensions of the preprocessed data\n",
    "        print(\"Preprocessed images shape:\", preprocessed_images.shape)\n",
    "        print(\"Preprocessed masks shape:\", preprocessed_masks.shape)\n",
    "        return preprocessed_images, preprocessed_masks\n",
    "\n",
    "    def normalize(array):\n",
    "        array = array / 255.0\n",
    "        return array\n",
    "\n",
    "    def masking(masks):\n",
    "        for i in range(0, len(masks)):\n",
    "            masks[i] = np.where(masks[i] > 0, 1, masks[i])\n",
    "\n",
    "    def display(img, mask, de_norm=True):\n",
    "        if de_norm:\n",
    "            img *= 255.0\n",
    "        img = img.astype(np.uint8)\n",
    "        mask = mask.astype(np.uint8)\n",
    "        mask = mask[:, :, :3]\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"image\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask)\n",
    "        plt.title(\"mask\")\n",
    "        plt.show()\n",
    "\n",
    "    def split(x, y):\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            x, y, test_size=0.2, randotm_state=42\n",
    "        )\n",
    "        del x\n",
    "        del y\n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def load_data(path, r=0):\n",
    "        data = [\n",
    "            np.array(Image.open(os.path.join(path, file)))\n",
    "            for file in tqdm(os.listdir(path)[r:])\n",
    "        ]\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        return data\n",
    "\n",
    "    def load_resize(path, resize=None, r=0):\n",
    "        if resize is not None:\n",
    "            shape = (resize, resize)\n",
    "        else:\n",
    "            shape = None\n",
    "\n",
    "        data = []\n",
    "        for file in tqdm(os.listdir(path)[r:]):\n",
    "            image = Image.open(os.path.join(path, file))\n",
    "            if shape is not None:\n",
    "                image = image.resize(shape)\n",
    "            data.append(np.array(image))\n",
    "\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        return data\n",
    "\n",
    "    def resize_data(data, shape=(150, 150)):\n",
    "        resized_data = []\n",
    "        for d in data:\n",
    "            resized_data.append(resize(d, shape, anti_aliasing=True))\n",
    "        return np.array(resized_data)\n",
    "\n",
    "    # computationally inefficient\n",
    "    def one_hot(y, size, load=True, r=None):\n",
    "        # Load the dataset into memory\n",
    "        if load:\n",
    "            dataset = np.load(y)\n",
    "        else:\n",
    "            dataset = y\n",
    "        if r is not None:\n",
    "            dataset = dataset[:r]\n",
    "        # Set the desired output size\n",
    "        output_size = (size, size)\n",
    "\n",
    "        # Resize each mask in the dataset\n",
    "        resized_masks = []\n",
    "        for mask in dataset:\n",
    "            # Resize the mask using Lanczos interpolation\n",
    "            resized_mask = resize(mask, output_size, order=1, anti_aliasing=False)\n",
    "\n",
    "            # One-hot encode the mask\n",
    "            one_hot_mask = tf.one_hot(np.argmax(resized_mask, axis=-1), depth=6)\n",
    "\n",
    "            # Append the one-hot encoded mask to the list\n",
    "            resized_masks.append(one_hot_mask)\n",
    "\n",
    "        # Convert the list of resized masks to a NumPy array\n",
    "        fold_ma = np.array(resized_masks)\n",
    "        del resized_masks\n",
    "        del dataset\n",
    "        return fold_ma\n",
    "\n",
    "    def check(x, y):\n",
    "        print(f\"shapes: x: {x.shape}, y: {y.shape}\")\n",
    "        print(f\"norms: x: {np.min(x), np.max(x)}, y: {np.min(y), np.max(y)}\")\n",
    "        print(f\"types: x: {type(x)}, y; {type(y)}\")\n",
    "\n",
    "    def load_masks(path, r=0):\n",
    "        # Load the masks\n",
    "        mask_files = sorted(os.listdir(path))[r:]\n",
    "        masks = []\n",
    "        for file in tqdm(mask_files):\n",
    "            mask = np.array(Image.open(os.path.join(path, file)), dtype=np.uint8)\n",
    "            # Perform one-hot encoding using Keras\n",
    "            one_hot_mask = to_categorical(mask, num_classes=6)\n",
    "            masks.append(one_hot_mask)\n",
    "        masks = np.array(masks, dtype=np.float32)\n",
    "        #  masks /= 255.0\n",
    "        return masks\n",
    "\n",
    "    def load_data(path, r=0):\n",
    "        data = [\n",
    "            np.array(Image.open(os.path.join(path, file)))\n",
    "            for file in tqdm(os.listdir(path)[r:])\n",
    "        ]\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        return data\n",
    "\n",
    "\n",
    "png_paths = {\n",
    "    \"fold_1_images\": \"/content/pannuke_processed/Fold 1/images\",\n",
    "    \"fold_1_inst\": \"/content/pannuke_processed/Fold 1/inst_masks\",\n",
    "    \"fold_1_seg\": \"/content/pannuke_processed/Fold 1/sem_masks\",\n",
    "    \"fold_2_images\": \"/content/pannuke_processed/Fold 2/images\",\n",
    "    \"fold_2_inst\": \"/content/pannuke_processed/Fold 2/inst_masks\",\n",
    "    \"fold_2_seg\": \"/content/pannuke_processed/Fold 2/sem_masks\",\n",
    "    \"fold_3_images\": \"/content/pannuke_processed/Fold 3/images\",\n",
    "    \"fold_3_inst\": \"/content/pannuke_processed/Fold 2/inst_masks\",\n",
    "    \"fold_3_seg\": \"/content/pannuke_processed/Fold 2/sem_masks\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "avrBUwYBI3m3",
    "outputId": "febb61e8-5dba-496d-daeb-7d8f1ac2c922"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "\n",
    "# Custom data generator class following the keras.utils.Sequence structure\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_path,\n",
    "        y_path,\n",
    "        batch_size,\n",
    "        num_classes=6,\n",
    "        shuffle=True,\n",
    "        augment=False,\n",
    "        target_size=256,\n",
    "    ):\n",
    "        self.x_path = x_path\n",
    "        self.y_path = y_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.image_files = sorted(os.listdir(x_path))\n",
    "        self.mask_files = sorted(os.listdir(y_path))\n",
    "        self.on_epoch_end()\n",
    "        self.augment = augment\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
    "\n",
    "\n",
    "    # load and preprocess the data\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in indexes:\n",
    "            img = Image.open(os.path.join(self.x_path, self.image_files[i])).convert(\n",
    "                \"RGB\"\n",
    "            )\n",
    "            mask = Image.open(os.path.join(self.y_path, self.mask_files[i]))\n",
    "            img = img.resize((self.target_size, self.target_size))\n",
    "            mask = mask.resize(\n",
    "                (self.target_size, self.target_size), resample=Image.NEAREST\n",
    "            )  \n",
    "            img = np.array(img)\n",
    "            mask = np.array(mask)\n",
    "            mask = np.where(mask > 0, 1, 0)\n",
    "            # after normalizing the masks pixels, further steps can also be neglacted\n",
    "            mask_onehot = np.zeros((mask.shape[0], mask.shape[1], self.num_classes))\n",
    "            for j in range(self.num_classes):\n",
    "                mask_onehot[:, :, j] = (mask == j).astype(int)\n",
    "            if self.augment:\n",
    "                data = {\"image\": img, \"mask\": mask_onehot}\n",
    "                data = self.augmentation_transform(**data)\n",
    "                img = data[\"image\"]\n",
    "                mask_onehot = data[\"mask\"]\n",
    "            x.append(img)\n",
    "            y.append(mask_onehot)\n",
    "        x = np.array(x) / 255.0\n",
    "        y = np.array(y)\n",
    "        y[:, :, :, [0, 5]] = y[:, :, :, [5, 0]]  # swap channel 0 and channel 5\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.image_files))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def set_augmentation_transform(self, augmentation_transform):\n",
    "        self.augmentation_transform = augmentation_transform\n",
    "\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "augmentation_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.25, brightness_limit=0.1, contrast_limit=0.1),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        #   A.CoarseDropout(max_holes=2, max_height=16, max_width=16, min_holes=1, p=0.1, mask_fill_value=0),\n",
    "        #   A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=20, p=0.5),\n",
    "        #   A.ColorJitter(p=0.1, brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "        #    A.RandomCrop(height=256, width=256, p=0.5),\n",
    "    ]\n",
    ")\n",
    "batch_size = 8\n",
    "\n",
    "# creating all generators at once\n",
    "def create_gens(\n",
    "    fold_x,\n",
    "    fold_y,\n",
    "    fold_x_2,\n",
    "    fold_y_2,\n",
    "    fold_x_3,\n",
    "    fold_y_3,\n",
    "    batch_size=batch_size,\n",
    "    mode=\"VAL\",\n",
    "    shuffle=True,\n",
    "    target_size=256,\n",
    "):\n",
    "    if mode not in [\"VAL\", \"TRAIN\"]:\n",
    "        raise Exception(f\"Provide either VAL or TRAIN as mode argument, not {mode}\")\n",
    "    elif mode == \"VAL\":\n",
    "        augment = False\n",
    "        val_generator_f_1 = CustomDataGenerator(\n",
    "            fold_x,\n",
    "            fold_y,\n",
    "            batch_size,\n",
    "            augment=augment,\n",
    "            shuffle=shuffle,\n",
    "            target_size=target_size,\n",
    "        )\n",
    "        val_generator_f_2 = CustomDataGenerator(\n",
    "            fold_x_2,\n",
    "            fold_y_2,\n",
    "            batch_size,\n",
    "            augment=augment,\n",
    "            shuffle=shuffle,\n",
    "            target_size=target_size,\n",
    "        )\n",
    "        val_generator_f_3 = CustomDataGenerator(\n",
    "            fold_x_3,\n",
    "            fold_y_3,\n",
    "            batch_size,\n",
    "            augment=augment,\n",
    "            shuffle=shuffle,\n",
    "            target_size=target_size,\n",
    "        )\n",
    "        return val_generator_f_1, val_generator_f_2, val_generator_f_3\n",
    "    elif mode == \"TRAIN\":\n",
    "        augment = True\n",
    "        train_generator_f_1 = CustomDataGenerator(\n",
    "            fold_x,\n",
    "            fold_y,\n",
    "            batch_size,\n",
    "            augment=augment,\n",
    "            shuffle=shuffle,\n",
    "            target_size=target_size,\n",
    "        )\n",
    "        train_generator_f_2 = CustomDataGenerator(\n",
    "            fold_x_2,\n",
    "            fold_y_2,\n",
    "            batch_size,\n",
    "            augment=augment,\n",
    "            shuffle=shuffle,\n",
    "            target_size=target_size,\n",
    "        )\n",
    "        train_generator_f_3 = CustomDataGenerator(\n",
    "            fold_x_3,\n",
    "            fold_y_3,\n",
    "            batch_size,\n",
    "            augment=augment,\n",
    "            shuffle=shuffle,\n",
    "            target_size=target_size,\n",
    "        )\n",
    "        train_generator_f_1.set_augmentation_transform(augmentation_transform)\n",
    "        train_generator_f_2.set_augmentation_transform(augmentation_transform)\n",
    "        train_generator_f_3.set_augmentation_transform(augmentation_transform)\n",
    "        return train_generator_f_1, train_generator_f_2, train_generator_f_3\n",
    "\n",
    "\n",
    "# upsampled\n",
    "\"\"\" image_path = \"/content/pannuke_over/Fold 1/images\"\n",
    "mask_path = \"/content/pannuke_over/Fold 1/sem_masks\"\n",
    "image_path1 = \"/content/pannuke_over/Fold 2/images\"\n",
    "mask_path1 = \"/content/pannuke_over/Fold 2/sem_masks\"\n",
    "image_path2 = \"/content/pannuke_over/Fold 3/images\"\n",
    "mask_path2 = \"/content/pannuke_over/Fold 3/sem_masks\" \"\"\"\n",
    "# normal\n",
    "\"\"\" image_path = \"/content/pannuke_processed/Fold 1/images\"\n",
    "mask_path = \"/content/pannuke_processed/Fold 1/sem_masks\"\n",
    "image_path1 = \"/content/pannuke_processed/Fold 2/images\"\n",
    "mask_path1 = \"/content/pannuke_processed/Fold 2/sem_masks\"\n",
    "image_path2 = \"/content/pannuke_processed/Fold 3/images\"\n",
    "mask_path2 = \"/content/pannuke_processed/Fold 3/sem_masks\" \"\"\"\n",
    "\n",
    "\"\"\" image_path = \"/content/pannuke_over_times_3/pannuke_over/Fold 1/images\"\n",
    "mask_path = \"/content/pannuke_over_times_3/pannuke_over/Fold 1/sem_masks\"\n",
    "image_path1 = \"/content/pannuke_over_times_3/pannuke_over/Fold 2/images\"\n",
    "mask_path1 = \"/content/pannuke_over_times_3/pannuke_over/Fold 2/sem_masks\"\n",
    "image_path2 = \"/content/pannuke_over_times_3/pannuke_over/Fold 3/images\"\n",
    "mask_path2 = \"/content/pannuke_over_times_3/pannuke_over/Fold 3/sem_masks\"  \"\"\"\n",
    "\n",
    "image_path = \"/content/pannuke_over_times_3/pannuke_instances/Fold 1/images\"\n",
    "mask_path = \"/content/pannuke_over_times_3/pannuke_instances/Fold 1/inst_masks\"\n",
    "image_path1 = \"/content/pannuke_over_times_3/pannuke_instances/Fold 2/images\"\n",
    "mask_path1 = \"/content/pannuke_over_times_3/pannuke_instances/Fold 2/inst_masks\"\n",
    "image_path2 = \"/content/pannuke_over_times_3/pannuke_instances/Fold 3/images\"\n",
    "mask_path2 = \"/content/pannuke_over_times_3/pannuke_instances/Fold 3/inst_masks\"\n",
    "\n",
    "vg_1, vg_2, vg_3 = create_gens(\n",
    "    image_path, mask_path, image_path1, mask_path1, image_path2, mask_path2, mode=\"VAL\"\n",
    ")\n",
    "tg_1, tg_2, tg_3 = create_gens(\n",
    "    image_path,\n",
    "    mask_path,\n",
    "    image_path1,\n",
    "    mask_path1,\n",
    "    image_path2,\n",
    "    mask_path2,\n",
    "    mode=\"TRAIN\",\n",
    ")\n",
    "# non shuffle, only for quick test\n",
    "vg_1_shuffle, vg_2_shuffle, vg_3_shuffle = create_gens(\n",
    "    image_path,\n",
    "    mask_path,\n",
    "    image_path1,\n",
    "    mask_path1,\n",
    "    image_path2,\n",
    "    mask_path2,\n",
    "    mode=\"VAL\",\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wY4UT-jwCVwU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a batch of images and masks\n",
    "batch_x, batch_y = tg_1.__getitem__(0)\n",
    "batch_x1, batch_y1 = vg_1.__getitem__(0)\n",
    "batch_x2, batch_y2 = vg_1_shuffle.__getitem__(0)\n",
    "Data.check(batch_x, batch_y)\n",
    "print(len(tg_1) * batch_size, len(tg_2) * batch_size, len(tg_3) * batch_size)\n",
    "Data.check(batch_x1, batch_y1)\n",
    "Data.check(batch_x2, batch_y2)\n",
    "\n",
    "# display some test images\n",
    "def dis_gen(x, y):\n",
    "    for i in range(len(x)):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(x[i])\n",
    "        plt.axis(False)\n",
    "        plt.title(\"image\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(y[i][:, :, :3])\n",
    "        plt.title(\"mask\")\n",
    "        plt.axis(False)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "dis_gen(batch_x, batch_y)\n",
    "dis_gen(batch_x1, batch_y1)\n",
    "dis_gen(batch_x2, batch_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA9-9NqokZPZ"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import keras.backend as K\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.0\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2.0 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1.0 - score\n",
    "\n",
    "# metrics \n",
    "def dice_score(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def jaccard_index(y_true, y_pred, smooth=1):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def cross_entropy_balanced(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    count_neg = tf.reduce_sum(1.0 - y_true)\n",
    "    count_pos = tf.reduce_sum(y_true)\n",
    "\n",
    "    beta = count_neg / (count_pos + count_neg)\n",
    "\n",
    "    pos_weight = beta / (1 - beta)\n",
    "\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        logits=y_pred, labels=y_true, pos_weight=pos_weight\n",
    "    )\n",
    "\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "\n",
    "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n",
    "\n",
    "\n",
    "def pixel_error(y_true, y_pred):\n",
    "    pred = tf.cast(tf.greater(y_pred, 0.5), tf.int32)\n",
    "    error = tf.cast(tf.not_equal(pred, tf.cast(y_true, tf.int32)), tf.float32)\n",
    "    return tf.reduce_mean(error)\n",
    "\n",
    "\n",
    "def get_fast_pq(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.int32)\n",
    "    n_classes = y_pred.shape[-1]\n",
    "    tp = tf.Variable([0.0] * n_classes, dtype=tf.float32)\n",
    "    fp = tf.Variable([0.0] * n_classes, dtype=tf.float32)\n",
    "    fn = tf.Variable([0.0] * n_classes, dtype=tf.float32)\n",
    "    for i in range(n_classes):\n",
    "        tp[i].assign_add(\n",
    "            tf.reduce_sum(tf.cast((y_true == i) & (y_pred == i), tf.float32))\n",
    "        )\n",
    "        fp[i].assign_add(\n",
    "            tf.reduce_sum(tf.cast((y_true != i) & (y_pred == i), tf.float32))\n",
    "        )\n",
    "        fn[i].assign_add(\n",
    "            tf.reduce_sum(tf.cast((y_true == i) & (y_pred != i), tf.float32))\n",
    "        )\n",
    "    pq = tf.reduce_mean(tp / (tp + 0.5 * fp + 0.5 * fn))\n",
    "    return pq\n",
    "\n",
    "\n",
    "class PanopticQuality(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"pq\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.pq_score = self.add_weight(name=\"pq_score\", initializer=\"zeros\")\n",
    "        self.num_samples = self.add_weight(name=\"num_samples\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.convert_to_tensor(y_true)\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        pq = get_fast_pq(y_true, y_pred)\n",
    "        self.pq_score.assign_add(tf.reduce_sum(pq))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.pq_score / self.num_samples\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.pq_score.assign(0)\n",
    "        self.num_samples.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI5VEic0YjzM"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "import keras.layers as kl\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "def attention_block(input_tensor, filters, name, dr=0.0):\n",
    "    x = Conv2D(\n",
    "        filters,\n",
    "        (3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        name=f\"conv_att{name}_1\",\n",
    "    )(input_tensor)\n",
    "    x = BatchNormalization(name=f\"batch_att{name}_1\")(x)\n",
    "    x = Activation(\"Mish\", name=f\"ac_att{name}_1\")(x)\n",
    "    x = Conv2D(\n",
    "        filters,\n",
    "        (3, 3),\n",
    "        strides=(1, 1),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        name=f\"conv_att{name}_2\",\n",
    "    )(x)\n",
    "    x = BatchNormalization(name=f\"batch_att{name}_2\")(x)\n",
    "    x = Activation(\"sigmoid\", name=f\"ac_att{name}_2\")(x)\n",
    "    x = Dropout(dr)(x)\n",
    "    x = Multiply()([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "# using Mish as activation instead of relu or other\n",
    "class Mish(Activation):\n",
    "    \"\"\"\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = \"Mish\"\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "\n",
    "get_custom_objects().update({\"Mish\": Mish(mish)})\n",
    "\n",
    "GNOISE = 0.001\n",
    "\n",
    "\n",
    "def conv3x3(x, out_filters, strides=(1, 1)):\n",
    "    x = Conv2D(\n",
    "        out_filters,\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        strides=strides,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def basic_Block(\n",
    "    input, out_filters, strides=(1, 1), with_conv_shortcut=False, dropout=0.0\n",
    "):\n",
    "    x = conv3x3(input, out_filters, strides)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = kl.GaussianNoise(GNOISE)(x)  # NCFC\n",
    "    x = Activation(\"Mish\")(x)\n",
    "\n",
    "    x = conv3x3(x, out_filters)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = kl.GaussianNoise(GNOISE)(x)  # NCFC\n",
    "\n",
    "    if with_conv_shortcut:\n",
    "        residual = Conv2D(\n",
    "            out_filters,\n",
    "            1,\n",
    "            strides=strides,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=\"he_normal\",\n",
    "        )(input)\n",
    "        residual = BatchNormalization(axis=3)(residual)\n",
    "        x = add([x, residual])\n",
    "    else:\n",
    "        x = add([x, input])\n",
    "\n",
    "    x = Activation(\"Mish\")(x)\n",
    "    if dropout > 0.0:\n",
    "        x = Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def bottleneck_Block(\n",
    "    input, out_filters, strides=(1, 1), with_conv_shortcut=False, dropout=0.0\n",
    "):\n",
    "    expansion = 4\n",
    "    de_filters = int(out_filters / expansion)\n",
    "\n",
    "    x = Conv2D(de_filters, 1, use_bias=False, kernel_initializer=\"he_normal\")(input)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = kl.GaussianNoise(GNOISE)(x)\n",
    "    x = Activation(\"Mish\")(x)\n",
    "\n",
    "    x = Conv2D(\n",
    "        de_filters,\n",
    "        3,\n",
    "        strides=strides,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = kl.GaussianNoise(GNOISE)(x)\n",
    "    x = Activation(\"Mish\")(x)\n",
    "\n",
    "    x = Conv2D(out_filters, 1, use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = kl.GaussianNoise(GNOISE)(x)\n",
    "\n",
    "    if with_conv_shortcut:\n",
    "        residual = Conv2D(\n",
    "            out_filters,\n",
    "            1,\n",
    "            strides=strides,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=\"he_normal\",\n",
    "        )(input)\n",
    "        residual = BatchNormalization(axis=3)(residual)\n",
    "        x = add([x, residual])\n",
    "    else:\n",
    "        x = add([x, input])\n",
    "\n",
    "    x = Activation(\"Mish\")(x)\n",
    "    if dropout > 0.0:\n",
    "        x = Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def stem_net(input):\n",
    "    x = Conv2D(\n",
    "        64,\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(input)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = kl.GaussianNoise(GNOISE)(x)\n",
    "    x = Activation(\"Mish\")(x)\n",
    "\n",
    "    x = bottleneck_Block(x, 256, with_conv_shortcut=True)  # changed to false\n",
    "    x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n",
    "    x = bottleneck_Block(\n",
    "        x, 256, with_conv_shortcut=False\n",
    "    )  # chang some shortcuts to true, maybe integrate more params inside\n",
    "    x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_layer1(x, out_filters_list=[32, 64], dropout=0.0):\n",
    "    x0 = Conv2D(\n",
    "        out_filters_list[0],\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x)\n",
    "    x0 = BatchNormalization(axis=3)(x0)\n",
    "    x0 = Activation(\"Mish\")(x0)\n",
    "\n",
    "    x1 = Conv2D(\n",
    "        out_filters_list[1],\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x)\n",
    "    x1 = BatchNormalization(axis=3)(x1)\n",
    "    x1 = Activation(\"Mish\")(x1)\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    return [x0, x1]\n",
    "\n",
    "\n",
    "def make_branch1_0(x, out_filters=32):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_branch1_1(x, out_filters=64):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fuse_layer1(x):\n",
    "    x0_0 = x[0]\n",
    "    x0_1 = Conv2D(32, 1, use_bias=False, kernel_initializer=\"he_normal\")(x[1])\n",
    "    x0_1 = BatchNormalization(axis=3)(x0_1)\n",
    "    x0_1 = UpSampling2D(size=(2, 2))(x0_1)\n",
    "    x0 = add([x0_0, x0_1])\n",
    "\n",
    "    x1_0 = Conv2D(\n",
    "        64,\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[0])\n",
    "    x1_0 = BatchNormalization(axis=3)(x1_0)\n",
    "    x1_1 = x[1]\n",
    "    x1 = add([x1_0, x1_1])\n",
    "    return [x0, x1]\n",
    "\n",
    "\n",
    "def transition_layer2(x, out_filters_list=[32, 64, 128], dropout=0.0):\n",
    "    x0 = Conv2D(\n",
    "        out_filters_list[0],\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[0])\n",
    "    x0 = BatchNormalization(axis=3)(x0)\n",
    "    x0 = Activation(\"Mish\")(x0)\n",
    "\n",
    "    x1 = Conv2D(\n",
    "        out_filters_list[1],\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[1])\n",
    "    x1 = BatchNormalization(axis=3)(x1)\n",
    "    x1 = Activation(\"Mish\")(x1)\n",
    "\n",
    "    x2 = Conv2D(\n",
    "        out_filters_list[2],\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[1])\n",
    "    x2 = BatchNormalization(axis=3)(x2)\n",
    "    x2 = Activation(\"Mish\")(x2)\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    return [x0, x1, x2]\n",
    "\n",
    "\n",
    "def make_branch2_0(x, out_filters=32):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_branch2_1(x, out_filters=64):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_branch2_2(x, out_filters=128):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fuse_layer2(x):\n",
    "    x0_0 = x[0]\n",
    "    x0_1 = Conv2D(32, 1, use_bias=False, kernel_initializer=\"he_normal\")(x[1])\n",
    "    x0_1 = BatchNormalization(axis=3)(x0_1)\n",
    "    x0_1 = UpSampling2D(size=(2, 2))(x0_1)\n",
    "    x0_2 = Conv2D(32, 1, use_bias=False, kernel_initializer=\"he_normal\")(x[2])\n",
    "    x0_2 = BatchNormalization(axis=3)(x0_2)\n",
    "    x0_2 = UpSampling2D(size=(4, 4))(x0_2)\n",
    "    x0 = add([x0_0, x0_1, x0_2])\n",
    "\n",
    "    x1_0 = Conv2D(\n",
    "        64,\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[0])\n",
    "    x1_0 = BatchNormalization(axis=3)(x1_0)\n",
    "    x1_1 = x[1]\n",
    "    x1_2 = Conv2D(64, 1, use_bias=False, kernel_initializer=\"he_normal\")(x[2])\n",
    "    x1_2 = BatchNormalization(axis=3)(x1_2)\n",
    "    x1_2 = UpSampling2D(size=(2, 2))(x1_2)\n",
    "    x1 = add([x1_0, x1_1, x1_2])\n",
    "\n",
    "    x2_0 = Conv2D(\n",
    "        32,\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[0])\n",
    "    x2_0 = BatchNormalization(axis=3)(x2_0)\n",
    "    x2_0 = Activation(\"Mish\")(x2_0)\n",
    "    x2_0 = Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x2_0)\n",
    "    x2_0 = BatchNormalization(axis=3)(x2_0)\n",
    "    x2_1 = Conv2D(\n",
    "        128,\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[1])\n",
    "    x2_1 = BatchNormalization(axis=3)(x2_1)\n",
    "    x2_2 = x[2]\n",
    "    x2 = add([x2_0, x2_1, x2_2])\n",
    "    return [x0, x1, x2]\n",
    "\n",
    "\n",
    "def transition_layer3(x, out_filters_list=[32, 64, 128, 256], dropout=0.0):\n",
    "    x0 = Conv2D(\n",
    "        out_filters_list[0],\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[0])\n",
    "    x0 = BatchNormalization(axis=3)(x0)\n",
    "    x0 = Activation(\"Mish\")(x0)\n",
    "\n",
    "    x1 = Conv2D(\n",
    "        out_filters_list[1],\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[1])\n",
    "    x1 = BatchNormalization(axis=3)(x1)\n",
    "    x1 = Activation(\"Mish\")(x1)\n",
    "\n",
    "    x2 = Conv2D(\n",
    "        out_filters_list[2],\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[2])\n",
    "    x2 = BatchNormalization(axis=3)(x2)\n",
    "    x2 = Activation(\"Mish\")(x2)\n",
    "\n",
    "    x3 = Conv2D(\n",
    "        out_filters_list[3],\n",
    "        3,\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "    )(x[2])\n",
    "    x3 = BatchNormalization(axis=3)(x3)\n",
    "    x3 = Activation(\"Mish\")(x3)\n",
    "    if dropout > 0.0:\n",
    "        x = Dropout(dropout)(x)\n",
    "\n",
    "    return [x0, x1, x2, x3]\n",
    "\n",
    "\n",
    "def make_branch3_0(x, out_filters=32):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_branch3_1(x, out_filters=64):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_branch3_2(x, out_filters=128):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_branch3_3(x, out_filters=256):\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=True)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "def fuse_layer3(x):\n",
    "    x0_0 = x[0]\n",
    "    x0_1 = Conv2D(32, 1, use_bias=False, kernel_initializer=\"he_normal\")(x[1])\n",
    "    x0_1 = BatchNormalization(axis=3)(x0_1)\n",
    "    x0_1 = UpSampling2D(size=(2, 2))(x0_1)\n",
    "    x0_2 = Conv2D(32, 1, use_bias=False, kernel_initializer=\"he_normal\")(x[2])\n",
    "    x0_2 = BatchNormalization(axis=3)(x0_2)\n",
    "    x0_2 = UpSampling2D(size=(4, 4))(x0_2)\n",
    "    x0_3 = Conv2D(32, 1, use_bias=False, kernel_initializer=\"he_normal\")(x[3])\n",
    "    x0_3 = BatchNormalization(axis=3)(x0_3)\n",
    "    x0_3 = UpSampling2D(size=(8, 8))(x0_3)\n",
    "    x0 = concatenate([x0_0, x0_1, x0_2, x0_3], axis=-1)\n",
    "    return x0\n",
    "\n",
    "\n",
    "# NCFC: Added naming convention to support finetuning\n",
    "def final_layer(x, classes=1, layernameprefix=\"model\", activation=\"softmax\"):\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(\n",
    "        classes,\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        name=layernameprefix + \"_conv2d\",\n",
    "    )(x)\n",
    "    x = BatchNormalization(axis=3, name=layernameprefix + \"_bnclass\")(x)\n",
    "    x = Activation(activation, name=layernameprefix + \"_classification\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def object_attention_context(feature, prob, scale=1):\n",
    "    feature = tf.keras.layers.Reshape([-1, tf.keras.backend.int_shape(feature)[-1]])(\n",
    "        feature\n",
    "    )\n",
    "    feature = tf.keras.layers.Permute([2, 1])(feature)\n",
    "    prob = tf.keras.layers.Reshape([-1, tf.keras.backend.int_shape(prob)[-1]])(prob)\n",
    "    prob = tf.keras.activations.softmax(prob * scale, axis=-2)\n",
    "    context = tf.keras.layers.Dot([2, 1])(\n",
    "        [feature, prob]\n",
    "    )  # batch x featrue ch x prob ch\n",
    "    context = tf.keras.backend.expand_dims(\n",
    "        context, axis=-2\n",
    "    )  # batch x featrue ch x 1 x prob ch\n",
    "    return context\n",
    "\n",
    "\n",
    "def object_attention(feature, prob, n_feature=256, scale=1, **kwargs):\n",
    "    if 1 < scale:\n",
    "        feature = tf.keras.layers.MaxPooling2D((scale, scale), padding=\"same\")(feature)\n",
    "    proxy_context = object_attention_context(feature, prob, scale)\n",
    "\n",
    "    query = tf.keras.layers.Conv2D(\n",
    "        n_feature,\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "    )(feature)\n",
    "    query = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(\n",
    "        query\n",
    "    )\n",
    "    query = tf.keras.layers.Activation(\"Mish\")(query)\n",
    "    query = tf.keras.layers.Conv2D(\n",
    "        n_feature,\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "    )(query)\n",
    "    query = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(\n",
    "        query\n",
    "    )\n",
    "    query = tf.keras.layers.Activation(\"Mish\")(query)\n",
    "    query = tf.keras.layers.Reshape([-1, tf.keras.backend.int_shape(query)[-1]])(query)\n",
    "\n",
    "    key = tf.keras.layers.Conv2D(\n",
    "        n_feature,\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "    )(proxy_context)\n",
    "    key = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(key)\n",
    "    key = tf.keras.layers.Activation(\"Mish\")(key)\n",
    "    key = tf.keras.layers.Conv2D(\n",
    "        n_feature,\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "    )(key)\n",
    "    key = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(key)\n",
    "    key = tf.keras.layers.Activation(\"Mish\")(key)\n",
    "    key = tf.keras.layers.Reshape([-1, tf.keras.backend.int_shape(key)[-1]])(key)\n",
    "    key = tf.keras.layers.Permute([2, 1])(key)\n",
    "\n",
    "    value = tf.keras.layers.Conv2D(\n",
    "        n_feature,\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "    )(proxy_context)\n",
    "    value = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(\n",
    "        value\n",
    "    )\n",
    "    value = tf.keras.layers.Activation(\"Mish\")(value)\n",
    "    value = tf.keras.layers.Reshape([-1, tf.keras.backend.int_shape(value)[-1]])(value)\n",
    "\n",
    "    sim = tf.keras.layers.Dot([2, 1])([query, key])\n",
    "    sim = sim * (n_feature**-0.5)\n",
    "    sim = tf.keras.activations.softmax(sim)\n",
    "\n",
    "    context = tf.keras.layers.Dot([2, 1])([sim, value])\n",
    "    context = tf.keras.layers.Reshape(\n",
    "        tf.keras.backend.int_shape(feature)[-3:-1] + (n_feature,)\n",
    "    )(context)\n",
    "    context = tf.keras.layers.Conv2D(\n",
    "        tf.keras.backend.int_shape(feature)[-1],\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "    )(context)\n",
    "    context = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(\n",
    "        context\n",
    "    )\n",
    "    context = tf.keras.layers.Activation(\"Mish\")(context)\n",
    "\n",
    "    if 1 < scale:\n",
    "        context = tf.keras.layers.UpSampling2D((scale, scale))(context)\n",
    "    return context\n",
    "\n",
    "\n",
    "def ocr_module(\n",
    "    feature,\n",
    "    prob,\n",
    "    n_feature=512,\n",
    "    n_attention_feature=256,\n",
    "    dropout_rate=0.05,\n",
    "    scale=1,\n",
    "    **kwargs,\n",
    "):\n",
    "    context = object_attention(feature, prob, n_attention_feature, scale, **kwargs)\n",
    "    out = tf.keras.layers.Concatenate(axis=-1)([feature, context])\n",
    "    out = tf.keras.layers.Conv2D(\n",
    "        n_feature,\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "    )(out)\n",
    "    out = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(out)\n",
    "    out = tf.keras.layers.Activation(\"Mish\")(out)\n",
    "    out = tf.keras.layers.Dropout(dropout_rate)(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "def seg_hrnet(\n",
    "    height,\n",
    "    width,\n",
    "    channel,\n",
    "    classes,\n",
    "    layername=\"model\",\n",
    "    last_activation=\"softmax\",\n",
    "    dropout_rate=0.1,\n",
    "    dropout=True,\n",
    "    mode=\"RES\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    @param mode: RES meaning the end operation of the OCR module is computed using residual style operations with other operations following that\n",
    "    @param mode: UP meaning the end operation of the the OCR module is upsampling2d, less computationally expensive\n",
    "    \"\"\"\n",
    "    assert mode in [\"RES\", \"UP\"]\n",
    "    inputs = Input(shape=(height, width, channel))  # NCFC: Removed fixed batch size\n",
    "\n",
    "    x = stem_net(inputs)\n",
    "    if dropout:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = transition_layer1(x)\n",
    "    x0 = make_branch1_0(x[0])\n",
    "    x1 = make_branch1_1(x[1])\n",
    "\n",
    "    x = fuse_layer1([x0, x1])\n",
    "\n",
    "    x = transition_layer2(x)\n",
    "    x0 = make_branch2_0(x[0])\n",
    "    x1 = make_branch2_1(x[1])\n",
    "    x2 = make_branch2_2(x[2])\n",
    "\n",
    "    x = fuse_layer2([x0, x1, x2])\n",
    "\n",
    "    # Add OCR module branch here\n",
    "    ocr_input = x[\n",
    "        -1\n",
    "    ]  # use output of last layer in HRNet backbone as input to OCR module\n",
    "    out = ocr_module(ocr_input, prob=x[-1])  # apply OCR module\n",
    "    if mode == \"RES\":\n",
    "        erosion_size = 2\n",
    "        dilation_size = 2\n",
    "        out = Conv2D(filters=128, kernel_size=3, padding=\"same\")(out)\n",
    "        out = BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(out)\n",
    "\n",
    "        # Define the residual block\n",
    "        residual = Conv2D(filters=128, kernel_size=3, padding=\"same\")(out)\n",
    "        residual = BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(residual)\n",
    "        residual = Activation(\"Mish\")(residual)\n",
    "\n",
    "        # Add \"erosion\" layer\n",
    "        residual = MaxPooling2D(pool_size=(erosion_size, erosion_size), padding=\"same\")(\n",
    "            residual\n",
    "        )\n",
    "\n",
    "        # Add dilation layer\n",
    "        residual = Conv2D(\n",
    "            filters=128, kernel_size=3, padding=\"same\", dilation_rate=dilation_size\n",
    "        )(residual)\n",
    "        residual = BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(residual)\n",
    "        residual = Activation(\"Mish\")(residual)\n",
    "\n",
    "        # Add \"erosion\" layer\n",
    "        residual = MaxPooling2D(pool_size=(erosion_size, erosion_size), padding=\"same\")(\n",
    "            residual\n",
    "        )\n",
    "\n",
    "        # Add dilation layer\n",
    "        residual = Conv2D(\n",
    "            filters=128, kernel_size=3, padding=\"same\", dilation_rate=dilation_size\n",
    "        )(residual)\n",
    "        residual = BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(residual)\n",
    "\n",
    "        # normal convolution\n",
    "        # out = Conv2D(filters=128, kernel_size=3, padding='same')(out)\n",
    "        # out = BatchNormalization(axis=-1, momentum=0.9, epsilon=1e-5)(out)\n",
    "\n",
    "        # Add skip connection\n",
    "        residual = UpSampling2D(size=(4, 4))(residual)\n",
    "        out = Add()([out, residual])\n",
    "        out = attention_block(out, 128, name=\"ocr_attention\")\n",
    "        out = Activation(\"Mish\")(out)\n",
    "\n",
    "        # if dropout:\n",
    "        #     cr_output = Dropout(dropout_rate//4)(out)\n",
    "\n",
    "        ocr_output = Conv2DTranspose(\n",
    "            filters=128, kernel_size=4, strides=4, padding=\"same\"\n",
    "        )(out)\n",
    "\n",
    "    else:\n",
    "        ocr_output = UpSampling2D(size=(4, 4))(out)\n",
    "\n",
    "    x = transition_layer3(x)\n",
    "    x0 = make_branch3_0(x[0])\n",
    "    x1 = make_branch3_1(x[1])\n",
    "    x2 = make_branch3_2(x[2])\n",
    "    x3 = make_branch3_3(x[3])\n",
    "\n",
    "    x = fuse_layer3([x0, x1, x2, x3])\n",
    "\n",
    "    # Merge outputs of OCR module and main backbone\n",
    "    x = Concatenate()([x, ocr_output])\n",
    "    if dropout:\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    out = final_layer(\n",
    "        x, classes=classes, layernameprefix=layername, activation=last_activation\n",
    "    )\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T7EtddQJ-cO"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/JanMarcelKezmann/TensorFlow-Advanced-Segmentation-Models.git\n",
    "from tensorflow.keras.losses import Loss, categorical_crossentropy\n",
    "import tensorflow_advanced_segmentation_models as tasm\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import optimizer\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Callable\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\"\"\" # number of nuclei in each class\n",
    "nuclei_counts = np.array([32276, 50585, 77403, 26572, 2908])\n",
    "\n",
    "# total number of nuclei\n",
    "total_nuclei = np.sum(nuclei_counts)\n",
    "\n",
    "# inverse frequency of each class\n",
    "class_weights = total_nuclei / (len(nuclei_counts) * nuclei_counts)\n",
    "\n",
    "# normalize the class weights\n",
    "class_weights = class_weights / np.sum(class_weights)\n",
    "\n",
    "print(class_weights) \"\"\"\n",
    "\n",
    "# different loss functions for different use cases\n",
    "CatFocDiceLoss = (\n",
    "    tasm.losses.CategoricalFocalLoss(alpha=0.25, gamma=2.0) + tasm.losses.DiceLoss()\n",
    ")\n",
    "TverskyFocalLoss = tasm.losses.FocalTverskyLoss()\n",
    "CELoss = tasm.losses.CategoricalCELoss()\n",
    "tf_CELoss = tf.keras.losses.CategoricalCrossentropy()\n",
    "CatFocLoss = tasm.losses.CategoricalFocalLoss()\n",
    "TverskyDice = tasm.losses.DiceLoss() + tasm.losses.FocalTverskyLoss()\n",
    "CeDice = tasm.losses.CategoricalCELoss() + tasm.losses.DiceLoss()\n",
    "JacardLoss = tasm.losses.JaccardLoss()\n",
    "\n",
    "\n",
    "def average(x, class_weights=None):\n",
    "    if class_weights is not None:\n",
    "        x = x * class_weights\n",
    "    return K.mean(x)\n",
    "\n",
    "\n",
    "def gather_channels(*xs):\n",
    "    return xs\n",
    "\n",
    "\n",
    "def round_if_needed(x, threshold):\n",
    "    if threshold is not None:\n",
    "        x = K.greater(x, threshold)\n",
    "        x = K.cast(x, K.floatx())\n",
    "    return x\n",
    "\n",
    "\n",
    "def dice_coefficient(\n",
    "    y_true, y_pred, beta=1.0, class_weights=1.0, smooth=1e-5, threshold=None\n",
    "):\n",
    "    # print(y_pred)\n",
    "    y_true, y_pred = gather_channels(y_true, y_pred)\n",
    "    y_pred = round_if_needed(y_pred, threshold)\n",
    "    axes = [1, 2] if K.image_data_format() == \"channels_last\" else [2, 3]\n",
    "\n",
    "    tp = K.sum(y_true * y_pred, axis=axes)\n",
    "    fp = K.sum(y_pred, axis=axes) - tp\n",
    "    fn = K.sum(y_true, axis=axes) - tp\n",
    "\n",
    "    score = ((1.0 + beta) * tp + smooth) / (\n",
    "        (1.0 + beta) * tp + (beta**2.0) * fn + fp + smooth\n",
    "    )\n",
    "    # print(\"Score, wo avg: \" + str(score))\n",
    "    score = average(score, class_weights)\n",
    "    # print(\"Score: \" + str(score))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# class_weights = [2.8309667, 15.249543, 7.3735404, 417.3982, 11.119026, 2.8309667]\n",
    "#  normalize the class weights\n",
    "# 0.06959552 0.04440575 0.02902039 0.08453504 0.77244329\n",
    "# class_weights /= np.sum(class_weights)\n",
    "# print(class_weights)\n",
    "class DiceLoss(Loss):\n",
    "    def __init__(self, beta=1.0, class_weights=None, smooth=1e-5):\n",
    "        super().__init__(name=\"dice_loss\")\n",
    "        self.beta = beta\n",
    "        self.class_weights = class_weights if class_weights is not None else 1.0\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        # print(y_pred)\n",
    "        return 1.0 - dice_coefficient(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            beta=self.beta,\n",
    "            class_weights=self.class_weights,\n",
    "            smooth=self.smooth,\n",
    "            threshold=None,\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"beta\": self.beta,\n",
    "            \"class_weights\": self.class_weights,\n",
    "            \"smooth\": self.smooth,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "# combination loss with dice and cross entropy incorporating class weights\n",
    "class CombinedLoss(Loss):\n",
    "    def __init__(self, class_weights=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cross_entropy = categorical_crossentropy\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        dice_loss = DiceLoss(class_weights=self.class_weights)(y_true, y_pred)\n",
    "        cross_entropy = self.cross_entropy(y_true, y_pred)\n",
    "        return cross_entropy + dice_loss\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"cross_entropy\": self.cross_entropy,\n",
    "            \"class_weights\": self.class_weights,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "# functions\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "# update functions\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def update_fn(p, grad, exp_avg, lr, wd, beta1, beta2):\n",
    "    # stepweight decay\n",
    "\n",
    "    p.assign(p * (1 - lr * wd))\n",
    "\n",
    "    # weight update\n",
    "\n",
    "    update = (\n",
    "        tf.raw_ops.LinSpace(start=1.0, stop=0.0, num=1, name=None)[0] * exp_avg\n",
    "        + (1 - tf.raw_ops.LinSpace(start=1.0, stop=0.0, num=1, name=None)[0]) * grad\n",
    "    )\n",
    "    p.assign_add(tf.sign(update) * -lr)\n",
    "\n",
    "    # decay the momentum running average coefficient\n",
    "\n",
    "    exp_avg.assign(exp_avg * beta2 + grad * (1 - beta2))\n",
    "\n",
    "\n",
    "def lerp(start, end, weight):\n",
    "    return start + weight * (end - start)\n",
    "\n",
    "\n",
    "def sparse_lerp(start, end, weight):\n",
    "    # Mathematically equivalent, but you can't subtract a dense Tensor from sparse\n",
    "    # IndexedSlices, so we have to flip it around.\n",
    "    return start + weight * -(start - end)\n",
    "\n",
    "\n",
    "class Lion(optimizer.Optimizer):\n",
    "    \"\"\"Optimizer that implements the Lion algorithm.\n",
    "    Lion was published in the paper \"Symbolic Discovery of Optimization Algorithms\"\n",
    "    which is available at https://arxiv.org/abs/2302.06675\n",
    "    Args:\n",
    "        learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n",
    "        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
    "        that takes no arguments and returns the actual value to use. The\n",
    "        learning rate. Defaults to 1e-4.\n",
    "        beta_1: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use. Factor\n",
    "        used to interpolate the current gradient and the momentum. Defaults to 0.9.\n",
    "        beta_2: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use. The\n",
    "        exponential decay rate for the momentum. Defaults to 0.99.\n",
    "    Notes:\n",
    "    The sparse implementation of this algorithm (used when the gradient is an\n",
    "    IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "    lookup in the forward pass) does apply momentum to variable slices even if\n",
    "    they were not used in the forward pass (meaning they have a gradient equal\n",
    "    to zero). Momentum decay (beta2) is also applied to the entire momentum\n",
    "    accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "    behavior (in contrast to some momentum implementations which ignore momentum\n",
    "    unless a variable slice was actually used).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.99,\n",
    "        weight_decay=None,\n",
    "        clipnorm=None,\n",
    "        clipvalue=None,\n",
    "        global_clipnorm=None,\n",
    "        jit_compile=True,\n",
    "        name=\"Lion\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            weight_decay=weight_decay,\n",
    "            clipnorm=clipnorm,\n",
    "            clipvalue=clipvalue,\n",
    "            global_clipnorm=global_clipnorm,\n",
    "            jit_compile=jit_compile,\n",
    "            **kwargs\n",
    "        )\n",
    "        self._learning_rate = self._build_learning_rate(learning_rate)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def build(self, var_list):\n",
    "        \"\"\"Initialize optimizer variables.\n",
    "        var_list: list of model variables to build Lion variables on.\n",
    "        \"\"\"\n",
    "        super().build(var_list)\n",
    "        if hasattr(self, \"_built\") and self._built:\n",
    "            return\n",
    "        self._built = True\n",
    "        self._emas = []\n",
    "        for var in var_list:\n",
    "            self._emas.append(\n",
    "                self.add_variable_from_reference(\n",
    "                    model_variable=var, variable_name=\"ema\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def update_step(self, gradient, variable):\n",
    "        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n",
    "        lr = tf.cast(self.learning_rate, variable.dtype)\n",
    "        beta_1 = tf.constant(self.beta_1, shape=(1,))\n",
    "        beta_2 = tf.constant(self.beta_2, shape=(1,))\n",
    "\n",
    "        var_key = self._var_key(variable)\n",
    "        ema = self._emas[self._index_dict[var_key]]\n",
    "\n",
    "        if isinstance(gradient, tf.IndexedSlices):\n",
    "            # Sparse gradients.\n",
    "            lerp_fn = sparse_lerp\n",
    "        else:\n",
    "            # Dense gradients.\n",
    "            lerp_fn = lerp\n",
    "\n",
    "        update = lerp_fn(ema, gradient, 1 - beta_1)\n",
    "        update = tf.sign(update)\n",
    "        variable.assign_sub(update * lr)\n",
    "\n",
    "        ema.assign(lerp_fn(ema, gradient, 1 - beta_2))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(self._learning_rate),\n",
    "                \"beta_1\": self.beta_1,\n",
    "                \"beta_2\": self.beta_2,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "# class_weights = [0.4794289668401082, 2.489686000012794, 1.202780949162393, 17.671771098807746, 1.8562545727590145, 0.4794289668401082]\n",
    "f3_class_weights = [\n",
    "    0.00748884,\n",
    "    0.04104853,\n",
    "    0.019121,\n",
    "    0.89586747,\n",
    "    0.02898535,\n",
    "    0.00748884,\n",
    "]\n",
    "f2_class_weights = [\n",
    "    0.00569605,\n",
    "    0.0317299,\n",
    "    0.01508198,\n",
    "    0.9191305,\n",
    "    0.02266551,\n",
    "    0.00569605,\n",
    "]\n",
    "f1_class_weights = [\n",
    "    0.00619736,\n",
    "    0.03338325,\n",
    "    0.01614165,\n",
    "    0.91373944,\n",
    "    0.024341,\n",
    "    0.00619736,\n",
    "]\n",
    "\n",
    "# combination loss out of dice and categorical crossentropy with class weights\n",
    "loss = CombinedLoss(class_weights=f1_class_weights)\n",
    "# loss = CombinedLoss()\n",
    "\n",
    "\n",
    "# opt = tf.keras.optimizers.experimental.AdamW(1e-3, weight_decay=1e-7)\n",
    "opt = Lion(learning_rate=5e-4)\n",
    "\n",
    "metric = [\n",
    "    dice_score,\n",
    "    pixel_error,\n",
    "    recall_m,\n",
    "    precision_m,\n",
    "    f1_m,\n",
    "    tf.keras.metrics.categorical_accuracy,\n",
    "    tasm.metrics.IOUScore(threshold=0.5),\n",
    "    jaccard_index,\n",
    "]\n",
    "model = seg_hrnet(\n",
    "    height=256,\n",
    "    width=256,\n",
    "    channel=3,\n",
    "    classes=6,\n",
    "    last_activation=\"softmax\",\n",
    "    dropout=True,\n",
    "    mode=\"RES\",\n",
    ")\n",
    "model.compile(loss=loss, optimizer=opt, metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRULRePEN9tW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.optimizers.experimental import AdamW\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    TensorBoard,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\"),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=4),\n",
    "    TensorBoard(log_dir=\"logs\"),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", patience=2, factor=0.1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOys6Z6H53ub"
   },
   "outputs": [],
   "source": [
    "STAGES = True\n",
    "\n",
    "import csv\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def save_eval_results(eval_results, filename):\n",
    "    # Open the file in write mode and write the evaluation results to it\n",
    "    with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(model.metrics_names)\n",
    "        writer.writerow(eval_results)\n",
    "\n",
    "# quick test operation to determine the current state of the model\n",
    "def quick_test(model_path, test_gen, call=\"fold_3\", random_batch=True, load=False):\n",
    "    if not os.path.exists(\"preds\"):\n",
    "        os.mkdir(\"preds\")\n",
    "    if load:\n",
    "        model_path = load_model(\n",
    "            f\"{model_path}\",\n",
    "            custom_objects={\n",
    "                \"Lion\": Lion,\n",
    "                \"CombinedLoss\": CombinedLoss(),\n",
    "                \"mish\": Mish(mish),\n",
    "                \"dice_loss\": dice_loss,\n",
    "                \"f1_m\": f1_m,\n",
    "                \"precision_m\": precision_m,\n",
    "                \"recall_m\": recall_m,\n",
    "                \"pixel_error\": pixel_error,\n",
    "                \"dice_score\": dice_score,\n",
    "                \"iou_score\": tasm.metrics.IOUScore(threshold=0.5),\n",
    "                \"focal_loss\": tasm.losses.CategoricalFocalLoss(),\n",
    "                \"dice_loss\": tasm.losses.DiceLoss(),\n",
    "                \"focal_loss_plus_dice_loss\": tasm.losses.CategoricalFocalLoss()\n",
    "                + tasm.losses.DiceLoss(),\n",
    "                \"jaccard_index\": jaccard_index,\n",
    "            },\n",
    "        )\n",
    "    if random_batch:\n",
    "        rand = np.random.randint(0, len(test_gen))\n",
    "        batch_x, batch_y = test_gen.__getitem__(rand)\n",
    "    else:\n",
    "        batch_x, batch_y = test_gen.__getitem__(0)\n",
    "    preds = model_path.predict(batch_x)\n",
    "    for i in range(0, len(batch_x)):\n",
    "        fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "        ax[0].imshow(batch_x[i])\n",
    "        ax[0].set_title(\"Image\")\n",
    "        ax[1].imshow(np.argmax(batch_y[i], axis=-1))\n",
    "        ax[1].set_title(\"Mask\")\n",
    "        ax[2].imshow(np.argmax(preds[i], axis=-1))\n",
    "        ax[2].set_title(\"Prediction\")\n",
    "        ax[3].imshow(batch_y[i][:, :, :3])\n",
    "        ax[3].set_title(\"Mask (RGB)\")\n",
    "        ax[4].imshow(preds[i][:, :, :3])\n",
    "        ax[4].set_title(\"Prediction (RGB)\")\n",
    "        for a in ax:\n",
    "            a.axis(\"off\")\n",
    "        plt.savefig(f\"./preds/preds_{str(i)}_{call}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_history(history, call=None):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(history.history), figsize=(25, 5))\n",
    "\n",
    "    for i, metric in enumerate(history.history.keys()):\n",
    "        axs[i].plot(history.history[metric])\n",
    "        axs[i].set_title(metric)\n",
    "        axs[i].set_xlabel(\"Epoch\")\n",
    "        axs[i].set_ylabel(metric)\n",
    "    plt.savefig(f\"{str(history)}_{call}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "opt = Lion(learning_rate=5e-4)\n",
    "\n",
    "model.compile(loss=loss, optimizer=opt, metrics=metric)\n",
    "# original split as described in here: https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke\n",
    "epochs = 5\n",
    "# increase dropout to 0.5 and opt by .1 / .2\n",
    "for i in range(epochs):\n",
    "    t_2_v_1_t_3 = model.fit(tg_1, callbacks=callbacks, validation_data=(vg_2))\n",
    "    model.save(\"model_F1_2.h5\")\n",
    "    quick_test(model, vg_3, call=\"CombinedLoss1\")\n",
    "\n",
    "\n",
    "eval_1 = model.evaluate(vg_3)\n",
    "save_eval_results(eval_1, \"t_1_v_2_t_3\")\n",
    "quick_test(\"model.h5\", vg_3_shuffle, call=\"CombinedLoss1\")\n",
    "plot_history(t_2_v_1_t_3, call=\"1\")\n",
    "quick_test(\"best_model.h5\", vg_3_shuffle, load=True)\n",
    "# training loops\n",
    "for i in range(epochs):\n",
    "    model = seg_hrnet(\n",
    "        height=256,\n",
    "        width=256,\n",
    "        channel=3,\n",
    "        classes=6,\n",
    "        last_activation=\"softmax\",\n",
    "        dropout=True,\n",
    "        mode=\"RES\",\n",
    "    )\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=metric)\n",
    "    t_2_v_1_t_3 = model.fit(tg_2, callbacks=callbacks, validation_data=(vg_1))\n",
    "    model.save(\"model1.h5\")\n",
    "    quick_test(model, vg_3_shuffle, call=\"CombinedLoss2\")\n",
    "\n",
    "eval_2 = model.evaluate(vg_3)\n",
    "save_eval_results(eval_2, \"t_2_v_1_t_3\")\n",
    "quick_test(\"model1.h5\", vg_3_shuffle)\n",
    "plot_history(t_2_v_1_t_3, call=\"2\")\n",
    "quick_test(\"best_model.h5\", vg_3_shuffle, load=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "    model = seg_hrnet(\n",
    "        height=256,\n",
    "        width=256,\n",
    "        channel=3,\n",
    "        classes=6,\n",
    "        last_activation=\"softmax\",\n",
    "        dropout=True,\n",
    "        mode=\"RES\",\n",
    "    )\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=metric)\n",
    "    t_3_v_2_t_1 = model.fit(tg_3, callbacks=callbacks, validation_data=(vg_2))\n",
    "    model.save(\"model2.h5\")\n",
    "    quick_test(model, vg_3_shuffle, call=\"CombinedLoss3\")\n",
    "\n",
    "eval_3 = model.evaluate(vg_1)\n",
    "save_eval_results(eval_3, \"t_3_v_2_t_1\")\n",
    "quick_test(\"model2.h5\", vg_1_shuffle)\n",
    "plot_history(t_3_v_2_t_1, call=\"3\")\n",
    "quick_test(\"best_model.h5\", vg_3_shuffle, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9LTQW6cbbbp"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "# moving files to drive\n",
    "MOVE = True\n",
    "\n",
    "if MOVE:\n",
    "    # models\n",
    "    shutil.move(\"/content/best_model.h5\", \"/content/drive/MyDrive/inst_mod_best.h5\")\n",
    "    shutil.move(\"/content/model_F1_2.h5\", \"/content/drive/MyDrive/inst_mod.h5\")\n",
    "# shutil.move(\"/content/model1.h5\", \"/content/drive/MyDrive/fold2_model_combi_f.h5\")\n",
    "# shutil.move(\"/content/model2.h5\", \"/content/drive/MyDrive/fold3_model_combi_f.h5\")\n",
    "# eval scores\n",
    "# shutil.move(\"/content/t_1_v_2_t_3\", \"/content/drive/MyDrive/t_1_v_2_t_3_f\")\n",
    "# shutil.move(\"/content/t_2_v_1_t_3\", \"/content/drive/MyDrive/t_2_v_1_t_3_f\")\n",
    "# shutil.move(\"/content/t_3_v_2_t_1\", \"/content/drive/MyDrive/t_3_v_2_t_1_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCjDwYBc5K_3"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "# moving other collected data to drive\n",
    "shutil.make_archive(\"preds.zip\", \"zip\", \"/content/preds\")\n",
    "shutil.move(\"preds.zip.zip\", \"/content/drive/MyDrive/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
