{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSGwN_XCanQ5",
    "outputId": "54064f73-d7f1-4f28-e3ab-9d04bfd87f61"
   },
   "outputs": [],
   "source": [
    "# mount drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nJp6JEG8cKl",
    "outputId": "bef048b9-3459-4380-94f9-3b7067d3cb60"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# which data to use, normalized images or not\n",
    "NORM = True\n",
    "\n",
    "# get data, might take a while\n",
    "if not NORM:\n",
    "    if os.path.exists(\"/content/drive/MyDrive/NCT-CRC-HE-100K-NONORM.zip?download=1\"):\n",
    "        !unzip '/content/drive/MyDrive/NCT-CRC-HE-100K-NONORM.zip?download=1'\n",
    "    else:\n",
    "        print(\"Data not found, downloading...\")\n",
    "        !wget https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K-NONORM.zip\\?download\\=1\n",
    "        !unzip '/content/NCT-CRC-HE-100K-NONORM.zip?download=1'\n",
    "else:\n",
    "    if os.path.exists(\"/content/drive/MyDrive/NCT-CRC-HE-100K.zip?download=1\"):\n",
    "        !unzip '/content/drive/MyDrive/NCT-CRC-HE-100K.zip?download=1'\n",
    "    else:\n",
    "        print(\"Data not found, downloading...\")\n",
    "        !wget https://zenodo.org/record/1214456/files/NCT-CRC-HE-100K.zip\\?download\\=1\n",
    "        !unzip '/content/NCT-CRC-HE-100K.zip?download=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWa_JXBiaRja"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    "    load_img,\n",
    "    img_to_array,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "MORE = False\n",
    "# Set the path to the NCT-CRC-HE-100K dataset folder\n",
    "if not NORM:\n",
    "    dataset_path = \"/content/NCT-CRC-HE-100K-NONORM\"\n",
    "else:\n",
    "    dataset_path = \"/content/NCT-CRC-HE-100K\"\n",
    "\n",
    "# Set the path to the folder where you want to save the train and test data\n",
    "train_test_path = \"/content/train_test_folder\"\n",
    "\n",
    "# Set the train-test split ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create train and test folders\n",
    "train_path = os.path.join(train_test_path, \"train\")\n",
    "test_path = os.path.join(train_test_path, \"test\")\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "if not MORE:\n",
    "    # Loop through each class folder in the dataset\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_folder_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        # Get the list of images in the class folder\n",
    "        images_list = os.listdir(class_folder_path)\n",
    "\n",
    "        # Shuffle the images randomly\n",
    "        random.shuffle(images_list)\n",
    "\n",
    "        # Split the images into train and test based on the train_ratio\n",
    "        train_images_list = images_list[: int(len(images_list) * train_ratio)]\n",
    "        test_images_list = images_list[int(len(images_list) * train_ratio) :]\n",
    "\n",
    "        # Create a separate directory for each class within the train and test directories\n",
    "        train_class_path = os.path.join(train_path, class_folder)\n",
    "        test_class_path = os.path.join(test_path, class_folder)\n",
    "        os.makedirs(train_class_path, exist_ok=True)\n",
    "        os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "        # Copy the train images to the train class directory\n",
    "        for train_image in train_images_list:\n",
    "            train_image_path = os.path.join(class_folder_path, train_image)\n",
    "            train_image_dest_path = os.path.join(train_class_path, train_image)\n",
    "            shutil.copy(train_image_path, train_image_dest_path)\n",
    "\n",
    "        # Copy the test images to the test class directory\n",
    "        for test_image in test_images_list:\n",
    "            test_image_path = os.path.join(class_folder_path, test_image)\n",
    "            test_image_dest_path = os.path.join(test_class_path, test_image)\n",
    "            shutil.copy(test_image_path, test_image_dest_path)\n",
    "else:\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=25,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"reflect\",\n",
    "    )\n",
    "    train_aug_factor = 1\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_folder_path = os.path.join(dataset_path, class_folder)\n",
    "        images_list = os.listdir(class_folder_path)\n",
    "        random.shuffle(images_list)\n",
    "        train_images_list = images_list[: int(len(images_list) * train_ratio)]\n",
    "        test_images_list = images_list[int(len(images_list) * train_ratio) :]\n",
    "\n",
    "        train_class_path = os.path.join(train_path, class_folder)\n",
    "        test_class_path = os.path.join(test_path, class_folder)\n",
    "        os.makedirs(train_class_path, exist_ok=True)\n",
    "        os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "        # Copy original train images to train_aug directory\n",
    "        for train_image in train_images_list:\n",
    "            train_image_path = os.path.join(class_folder_path, train_image)\n",
    "            train_image_dest_path = os.path.join(train_class_path, train_image)\n",
    "            shutil.copy(train_image_path, train_image_dest_path)\n",
    "\n",
    "        # Generate and copy augmented train images to train_aug directory\n",
    "        for train_image in train_images_list:\n",
    "            train_image_path = os.path.join(class_folder_path, train_image)\n",
    "            train_image_name, train_image_ext = os.path.splitext(\n",
    "                train_image\n",
    "            )  # Split filename and extension\n",
    "            for i in range(train_aug_factor):\n",
    "                train_image_copy_path = os.path.join(\n",
    "                    train_class_path, f\"{train_image_name}_copy_{i}{train_image_ext}\"\n",
    "                )  # Add extension to new filename\n",
    "                shutil.copy(train_image_path, train_image_copy_path)\n",
    "                train_image_copy = img_to_array(load_img(train_image_copy_path))\n",
    "                train_image_copy = train_datagen.random_transform(train_image_copy)\n",
    "                train_image_copy = train_image_copy.astype(\"uint8\")\n",
    "                train_image_copy = Image.fromarray(train_image_copy)\n",
    "                #    print(np.max(train_image_copy), np.min(train_image_copy), type(train_image_copy), np.array(train_image_copy).shape)\n",
    "                train_image_copy.save(train_image_copy_path)\n",
    "\n",
    "        # Copy original test images to test directory\n",
    "        for test_image in test_images_list:\n",
    "            test_image_path = os.path.join(class_folder_path, test_image)\n",
    "            test_image_dest_path = os.path.join(test_class_path, test_image)\n",
    "            shutil.copy(test_image_path, test_image_dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WaIEFQgnTYk",
    "outputId": "37028843-4aa2-4e2d-cf5c-2ea2d7e971bb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate class weights\n",
    "num_classes = len(os.listdir(train_path))\n",
    "train_class_counts = {}\n",
    "for class_folder in os.listdir(train_path):\n",
    "    class_path = os.path.join(train_path, class_folder)\n",
    "    train_class_counts[class_folder] = len(os.listdir(class_path))\n",
    "\n",
    "train_class_weights_dict = {}\n",
    "for idx, class_folder in enumerate(os.listdir(train_path)):\n",
    "    class_weight = sum(train_class_counts.values()) / (\n",
    "        num_classes * train_class_counts[class_folder]\n",
    "    )\n",
    "    train_class_weights_dict[idx] = class_weight\n",
    "\n",
    "print(train_class_weights_dict)\n",
    "train_class_weights = list(train_class_weights_dict.values())\n",
    "train_class_weights = np.array(train_class_weights) / sum(train_class_weights)\n",
    "print(train_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVhJvxCZax3G"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# data generator following tensorflow.keras.utils.Sequence \n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        directory,\n",
    "        batch_size,\n",
    "        target_size=(256, 256),\n",
    "        shuffle=True,\n",
    "        augmentations=None,\n",
    "    ):\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size # (height, width)\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentations = augmentations\n",
    "        self.class_names = sorted(os.listdir(directory))\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.samples = []\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(self.directory, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                self.samples.append((os.path.join(class_dir, filename), i))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.samples) / float(self.batch_size)))\n",
    "\n",
    "    # get and preprocess data\n",
    "    def __getitem__(self, idx):\n",
    "        batch_samples = self.samples[\n",
    "            idx * self.batch_size : (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for sample in batch_samples:\n",
    "            image = cv2.imread(sample[0])\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, self.target_size)\n",
    "            label = sample[1]\n",
    "            if self.augmentations is not None:\n",
    "                augmented = self.augmentations(image=image)\n",
    "                image = augmented[\"image\"]\n",
    "            batch_images.append(image)\n",
    "            batch_labels.append(label)\n",
    "        return np.array(batch_images) / 255.0, tf.keras.utils.to_categorical(\n",
    "            batch_labels, num_classes=self.num_classes\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.samples)\n",
    "\n",
    "# data augmentation using albumentations if needed\n",
    "train_augmentations = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=(-20, 20), p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.25, brightness_limit=0.2, contrast_limit=0.2),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.Blur(p=0.2, blur_limit=(3, 7)),\n",
    "        A.RandomRotate90(p=0.4),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.2, scale_limit=0.2, rotate_limit=20, p=0.5\n",
    "                ),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.18,\n",
    "                    scale_limit=0.18,\n",
    "                    rotate_limit=18,\n",
    "                    border_mode=cv2.BORDER_CONSTANT,\n",
    "                    value=0,\n",
    "                    interpolation=cv2.INTER_NEAREST,\n",
    "                    p=0.15,\n",
    "                ),\n",
    "            ],\n",
    "            p=0.4,\n",
    "        ),\n",
    "        # A.CoarseDropout(max_holes=2, max_height=18, max_width=18, min_holes=1, p=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create generators\n",
    "train_generator = CustomDataGenerator(\n",
    "    directory=train_path,\n",
    "    batch_size=32,\n",
    "    target_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    augmentations=train_augmentations,\n",
    ")\n",
    "\n",
    "test_generator = CustomDataGenerator(\n",
    "    directory=test_path,\n",
    "    batch_size=32,\n",
    "    target_size=(256, 256),\n",
    "    shuffle=False,\n",
    "    augmentations=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "Sr5eJU2Ccela",
    "outputId": "77abc745-b858-402b-93f5-55ad6fe8de21"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get a batch of images and labels from the generator\n",
    "batch_images, batch_labels = train_generator.__getitem__(0)\n",
    "batch_images_1, batch_labels_1 = test_generator.__getitem__(0)\n",
    "\n",
    "# display images and labels\n",
    "def dis_gen(x, y):\n",
    "    n = len(x)\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=n, figsize=(30, 30))\n",
    "    for i in range(n):\n",
    "        axs[i].imshow(x[i])\n",
    "        axs[i].set_title(f\"Label: {y[i]}\")\n",
    "        axs[i].axis(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check(x, y):\n",
    "    print(f\"shapes: x: {x.shape}, y: {y.shape}\")\n",
    "    print(f\"norms: x: {np.min(x), np.max(x)}, y: {np.min(y), np.max(y)}\")\n",
    "    print(f\"types: x: {type(x)}, y; {type(y)}\")\n",
    "\n",
    "\n",
    "dis_gen(batch_images, batch_labels)\n",
    "dis_gen(batch_images_1, batch_labels_1)\n",
    "check(batch_images, batch_labels)\n",
    "check(batch_images_1, batch_labels_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkYiXwv_jGhA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a EfficientNetV2 Model as defined in:\n",
    "Mingxing Tan, Quoc V. Le. (2021).\n",
    "EfficientNetV2: Smaller Models and Faster Training\n",
    "arXiv preprint arXiv:2104.00298.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Implementation by leondgarse:\n",
    "https://github.com/leondgarse\n",
    "changed some activation functions \n",
    "\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Add,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    DepthwiseConv2D,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling2D,\n",
    "    Input,\n",
    "    PReLU,\n",
    "    Reshape,\n",
    "    Multiply,\n",
    "    GroupNormalization,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "# mish activation function has shown better performance than other activation functions in many cases\n",
    "# replacing activation functions with mish activation functions improves performance\n",
    "class Mish(Activation):\n",
    "    \"\"\"\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = \"Mish\"\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "\n",
    "get_custom_objects().update({\"Mish\": Mish(mish)})\n",
    "\n",
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 0.001\n",
    "TORCH_BATCH_NORM_EPSILON = 1e-5\n",
    "CONV_KERNEL_INITIALIZER = keras.initializers.VarianceScaling(\n",
    "    scale=2.0, mode=\"fan_out\", distribution=\"truncated_normal\"\n",
    ")\n",
    "# CONV_KERNEL_INITIALIZER = 'glorot_uniform'\n",
    "\n",
    "BLOCK_CONFIGS = {\n",
    "    \"b0\": {  # width 1.0, depth 1.0\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 48, 96, 112, 192],\n",
    "        \"depthes\": [1, 2, 2, 3, 5, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"b1\": {  # width 1.0, depth 1.1\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 48, 96, 112, 192],\n",
    "        \"depthes\": [2, 3, 3, 4, 6, 9],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"b2\": {  # width 1.1, depth 1.2\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1408,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 56, 104, 120, 208],\n",
    "        \"depthes\": [2, 3, 3, 4, 6, 10],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"b3\": {  # width 1.2, depth 1.4\n",
    "        \"first_conv_filter\": 40,\n",
    "        \"output_conv_filter\": 1536,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 40, 56, 112, 136, 232],\n",
    "        \"depthes\": [2, 3, 3, 5, 7, 12],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"t\": {  # width 1.4 * 0.8, depth 1.8 * 0.9, from timm\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1024,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 40, 48, 104, 128, 208],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 14],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"s\": {  # width 1.4, depth 1.8\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 48, 64, 128, 160, 256],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 15],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"early\": {  # S model discribed in paper early version https://arxiv.org/pdf/2104.00298v2.pdf\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1792,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 48, 64, 128, 160, 272],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 15],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"m\": {  # width 1.6, depth 2.2\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [24, 48, 80, 160, 176, 304, 512],\n",
    "        \"depthes\": [3, 5, 5, 7, 14, 18, 5],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"l\": {  # width 2.0, depth 3.1\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [32, 64, 96, 192, 224, 384, 640],\n",
    "        \"depthes\": [4, 7, 7, 10, 19, 25, 7],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"xl\": {\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [32, 64, 96, 192, 256, 512, 640],\n",
    "        \"depthes\": [4, 8, 8, 16, 24, 32, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "}\n",
    "\n",
    "FILE_HASH_DICT = {\n",
    "    \"b0\": {\n",
    "        \"21k-ft1k\": \"4e4da4eb629897e4d6271e131039fe75\",\n",
    "        \"21k\": \"5dbb4252df24b931e74cdd94d150f25a\",\n",
    "        \"imagenet\": \"9abdc43cb00f4cb06a8bdae881f412d6\",\n",
    "    },\n",
    "    \"b1\": {\n",
    "        \"21k-ft1k\": \"5f1aee82209f4f0f20bd24460270564e\",\n",
    "        \"21k\": \"a50ae65b50ceff7f5283be2f4506d2c2\",\n",
    "        \"imagenet\": \"5d4223b59ff268828d5112a1630e234e\",\n",
    "    },\n",
    "    \"b2\": {\n",
    "        \"21k-ft1k\": \"ec384b84441ddf6419938d1e5a0cbef2\",\n",
    "        \"21k\": \"9f718a8bbb7b63c5313916c5e504790d\",\n",
    "        \"imagenet\": \"1814bc08d4bb7a5e0ed3ccfe1cf18650\",\n",
    "    },\n",
    "    \"b3\": {\n",
    "        \"21k-ft1k\": \"4a27827b0b2df508bed31ae231003bb1\",\n",
    "        \"21k\": \"ade5bdbbdf1d54c4561aa41511525855\",\n",
    "        \"imagenet\": \"cda85b8494c7ec5a68dffb335a254bab\",\n",
    "    },\n",
    "    \"l\": {\n",
    "        \"21k-ft1k\": \"30327edcf1390d10e9a0de42a2d731e3\",\n",
    "        \"21k\": \"7970f913eec1b4918e007c8580726412\",\n",
    "        \"imagenet\": \"2b65f5789f4d2f1bf66ecd6d9c5c2d46\",\n",
    "    },\n",
    "    \"m\": {\n",
    "        \"21k-ft1k\": \"0c236c3020e3857de1e5f2939abd0cc6\",\n",
    "        \"21k\": \"3923c286366b2a5137f39d1e5b14e202\",\n",
    "        \"imagenet\": \"ac3fd0ff91b35d18d1df8f1895efe1d5\",\n",
    "    },\n",
    "    \"s\": {\n",
    "        \"21k-ft1k\": \"93046a0d601da46bfce9d4ca14224c83\",\n",
    "        \"21k\": \"10b05d878b64f796ab984a5316a4a1c3\",\n",
    "        \"imagenet\": \"3b91df2c50c7a56071cca428d53b8c0d\",\n",
    "    },\n",
    "    \"t\": {\"imagenet\": \"4a0ff9cb396665734d7ca590fa29681b\"},\n",
    "    \"xl\": {\n",
    "        \"21k-ft1k\": \"9aaa2bd3c9495b23357bc6593eee5bce\",\n",
    "        \"21k\": \"c97de2770f55701f788644336181e8ee\",\n",
    "    },\n",
    "    \"v1-b0\": {\n",
    "        \"noisy_student\": \"d125a518737c601f8595937219243432\",\n",
    "        \"imagenet\": \"cc7d08887de9df8082da44ce40761986\",\n",
    "    },\n",
    "    \"v1-b1\": {\n",
    "        \"noisy_student\": \"8f44bff58fc5ef99baa3f163b3f5c5e8\",\n",
    "        \"imagenet\": \"a967f7be55a0125c898d650502c0cfd0\",\n",
    "    },\n",
    "    \"v1-b2\": {\n",
    "        \"noisy_student\": \"b4ffed8b9262df4facc5e20557983ef8\",\n",
    "        \"imagenet\": \"6c8d1d3699275c7d1867d08e219e00a7\",\n",
    "    },\n",
    "    \"v1-b3\": {\n",
    "        \"noisy_student\": \"9d696365378a1ebf987d0e46a9d26ddd\",\n",
    "        \"imagenet\": \"d78edb3dc7007721eda781c04bd4af62\",\n",
    "    },\n",
    "    \"v1-b4\": {\n",
    "        \"noisy_student\": \"a0f61b977544493e6926186463d26294\",\n",
    "        \"imagenet\": \"4c83aa5c86d58746a56675565d4f2051\",\n",
    "    },\n",
    "    \"v1-b5\": {\n",
    "        \"noisy_student\": \"c3b6eb3f1f7a1e9de6d9a93e474455b1\",\n",
    "        \"imagenet\": \"0bda50943b8e8d0fadcbad82c17c40f5\",\n",
    "    },\n",
    "    \"v1-b6\": {\n",
    "        \"noisy_student\": \"20dd18b0df60cd7c0387c8af47bd96f8\",\n",
    "        \"imagenet\": \"da13735af8209f675d7d7d03a54bfa27\",\n",
    "    },\n",
    "    \"v1-b7\": {\n",
    "        \"noisy_student\": \"7f6f6dd4e8105e32432607ad28cfad0f\",\n",
    "        \"imagenet\": \"d9c22b5b030d1e4f4c3a96dbf5f21ce6\",\n",
    "    },\n",
    "    \"v1-l2\": {\"noisy_student\": \"5fedc721febfca4b08b03d1f18a4a3ca\"},\n",
    "}\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor=4, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def conv2d_no_bias(\n",
    "    inputs,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=1,\n",
    "    padding=\"VALID\",\n",
    "    use_torch_padding=False,\n",
    "    name=\"\",\n",
    "):\n",
    "    pad = (\n",
    "        max(kernel_size) // 2\n",
    "        if isinstance(kernel_size, (list, tuple))\n",
    "        else kernel_size // 2\n",
    "    )\n",
    "    if use_torch_padding and padding.upper() == \"SAME\" and pad != 0:\n",
    "        inputs = keras.layers.ZeroPadding2D(padding=pad, name=name and name + \"pad\")(\n",
    "            inputs\n",
    "        )\n",
    "        padding = \"VALID\"\n",
    "\n",
    "    return Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "        name=name + \"conv\",\n",
    "    )(inputs)\n",
    "\n",
    "\n",
    "def batchnorm_with_activation(inputs, activation=\"Mish\", use_torch_eps=False, name=\"\"):\n",
    "    \"\"\"Performs a batch normalization followed by an activation.\"\"\"\n",
    "    bn_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    nn = BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=TORCH_BATCH_NORM_EPSILON if use_torch_eps else BATCH_NORM_EPSILON,\n",
    "        name=name + \"bn\",\n",
    "    )(inputs)\n",
    "    if activation:\n",
    "        nn = Activation(activation=activation, name=name + activation)(nn)\n",
    "        # nn = PReLU(shared_axes=[1, 2], alpha_initializer=tf.initializers.Constant(0.25), name=name + \"PReLU\")(nn)\n",
    "    return nn\n",
    "\n",
    "\n",
    "def se_module(inputs, se_ratio=4, name=\"\"):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    h_axis, w_axis = [2, 3] if K.image_data_format() == \"channels_first\" else [1, 2]\n",
    "\n",
    "    filters = inputs.shape[channel_axis]\n",
    "    # reduction = _make_divisible(filters // se_ratio, 8)\n",
    "    reduction = filters // se_ratio\n",
    "    # se = GlobalAveragePooling2D()(inputs)\n",
    "    # se = Reshape((1, 1, filters))(se)\n",
    "    se = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)\n",
    "    se = Conv2D(\n",
    "        reduction,\n",
    "        kernel_size=1,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "        name=name + \"1_conv\",\n",
    "    )(se)\n",
    "    # se = PReLU(shared_axes=[1, 2])(se)\n",
    "    se = Activation(\"Mish\")(se)\n",
    "    se = Conv2D(\n",
    "        filters,\n",
    "        kernel_size=1,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "        name=name + \"2_conv\",\n",
    "    )(se)\n",
    "    se = Activation(\"sigmoid\")(se)\n",
    "    return Multiply()([inputs, se])\n",
    "\n",
    "\n",
    "def MBConv(\n",
    "    inputs,\n",
    "    output_channel,\n",
    "    stride,\n",
    "    expand_ratio,\n",
    "    shortcut,\n",
    "    kernel_size=3,\n",
    "    drop_rate=0,\n",
    "    use_se=0,\n",
    "    is_fused=False,\n",
    "    is_torch_mode=False,\n",
    "    name=\"\",\n",
    "):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    input_channel = inputs.shape[channel_axis]\n",
    "\n",
    "    if is_fused and expand_ratio != 1:\n",
    "        nn = conv2d_no_bias(\n",
    "            inputs,\n",
    "            input_channel * expand_ratio,\n",
    "            (3, 3),\n",
    "            stride,\n",
    "            padding=\"same\",\n",
    "            use_torch_padding=is_torch_mode,\n",
    "            name=name + \"sortcut_\",\n",
    "        )\n",
    "        nn = batchnorm_with_activation(\n",
    "            nn, use_torch_eps=is_torch_mode, name=name + \"sortcut_\"\n",
    "        )\n",
    "    elif expand_ratio != 1:\n",
    "        nn = conv2d_no_bias(\n",
    "            inputs,\n",
    "            input_channel * expand_ratio,\n",
    "            (1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding=\"valid\",\n",
    "            name=name + \"sortcut_\",\n",
    "        )\n",
    "        nn = batchnorm_with_activation(\n",
    "            nn, use_torch_eps=is_torch_mode, name=name + \"sortcut_\"\n",
    "        )\n",
    "    else:\n",
    "        nn = inputs\n",
    "\n",
    "    if not is_fused:\n",
    "        if is_torch_mode and kernel_size // 2 > 0:\n",
    "            nn = keras.layers.ZeroPadding2D(\n",
    "                padding=kernel_size // 2, name=name + \"pad\"\n",
    "            )(nn)\n",
    "            pad = \"VALID\"\n",
    "        else:\n",
    "            pad = \"SAME\"\n",
    "        nn = DepthwiseConv2D(\n",
    "            kernel_size,\n",
    "            padding=pad,\n",
    "            strides=stride,\n",
    "            use_bias=False,\n",
    "            depthwise_initializer=CONV_KERNEL_INITIALIZER,\n",
    "            name=name + \"MB_dw_\",\n",
    "        )(nn)\n",
    "        nn = batchnorm_with_activation(\n",
    "            nn, use_torch_eps=is_torch_mode, name=name + \"MB_dw_\"\n",
    "        )\n",
    "\n",
    "    if use_se:\n",
    "        nn = se_module(nn, se_ratio=4 * expand_ratio, name=name + \"se_\")\n",
    "\n",
    "    # pw-linear\n",
    "    if is_fused and expand_ratio == 1:\n",
    "        nn = conv2d_no_bias(\n",
    "            nn,\n",
    "            output_channel,\n",
    "            (3, 3),\n",
    "            strides=stride,\n",
    "            padding=\"same\",\n",
    "            use_torch_padding=is_torch_mode,\n",
    "            name=name + \"fu_\",\n",
    "        )\n",
    "        nn = batchnorm_with_activation(\n",
    "            nn, use_torch_eps=is_torch_mode, name=name + \"fu_\"\n",
    "        )\n",
    "    else:\n",
    "        nn = conv2d_no_bias(\n",
    "            nn,\n",
    "            output_channel,\n",
    "            (1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding=\"valid\",\n",
    "            name=name + \"MB_pw_\",\n",
    "        )\n",
    "        nn = batchnorm_with_activation(\n",
    "            nn, use_torch_eps=is_torch_mode, activation=None, name=name + \"MB_pw_\"\n",
    "        )\n",
    "\n",
    "    if shortcut:\n",
    "        if drop_rate > 0:\n",
    "            nn = Dropout(drop_rate, noise_shape=(None, 1, 1, 1), name=name + \"drop\")(nn)\n",
    "        return Add()([inputs, nn])\n",
    "    else:\n",
    "        return nn\n",
    "\n",
    "\n",
    "def EfficientNetV2(\n",
    "    model_type,\n",
    "    input_shape=(None, None, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    first_strides=2,\n",
    "    is_torch_mode=False,\n",
    "    drop_connect_rate=0,\n",
    "    classifier_activation=\"softmax\",\n",
    "    include_preprocessing=False,\n",
    "    pretrained=\"imagenet\",\n",
    "    model_name=\"EfficientNetV2\",\n",
    "    kwargs=None,  # Not used, just recieving parameter\n",
    "):\n",
    "    if isinstance(model_type, dict):  # For EfficientNetV1 configures\n",
    "        model_type, blocks_config = model_type.popitem()\n",
    "    else:\n",
    "        blocks_config = BLOCK_CONFIGS.get(model_type.lower(), BLOCK_CONFIGS[\"s\"])\n",
    "    expands = blocks_config[\"expands\"]\n",
    "    out_channels = blocks_config[\"out_channels\"]\n",
    "    depthes = blocks_config[\"depthes\"]\n",
    "    strides = blocks_config[\"strides\"]\n",
    "    use_ses = blocks_config[\"use_ses\"]\n",
    "    first_conv_filter = blocks_config.get(\"first_conv_filter\", out_channels[0])\n",
    "    output_conv_filter = blocks_config.get(\"output_conv_filter\", 1280)\n",
    "    kernel_sizes = blocks_config.get(\"kernel_sizes\", [3] * len(depthes))\n",
    "    # \"torch\" for all V1 models\n",
    "    # for V2 models, \"21k\" pretrained are all \"tf\", \"imagenet\" pretrained \"bx\" models are all \"torch\", [\"s\", \"m\", \"l\", \"xl\"] are \"tf\"\n",
    "    rescale_mode = (\n",
    "        \"tf\"\n",
    "        if pretrained is not None and pretrained.startswith(\"imagenet21k\")\n",
    "        else blocks_config.get(\"rescale_mode\", \"torch\")\n",
    "    )\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    if include_preprocessing and rescale_mode == \"torch\":\n",
    "        channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "        Normalization = (\n",
    "            keras.layers.Normalization\n",
    "            if hasattr(keras.layers, \"Normalization\")\n",
    "            else keras.layers.experimental.preprocessing.Normalization\n",
    "        )\n",
    "        mean = tf.constant([0.485, 0.456, 0.406]) * 255.0\n",
    "        std = (tf.constant([0.229, 0.224, 0.225]) * 255.0) ** 2\n",
    "        nn = Normalization(mean=mean, variance=std, axis=channel_axis)(inputs)\n",
    "    elif include_preprocessing and rescale_mode == \"tf\":\n",
    "        Rescaling = (\n",
    "            keras.layers.Rescaling\n",
    "            if hasattr(keras.layers, \"Rescaling\")\n",
    "            else keras.layers.experimental.preprocessing.Rescaling\n",
    "        )\n",
    "        nn = Rescaling(scale=1.0 / 128.0, offset=-1)(inputs)\n",
    "    else:\n",
    "        nn = inputs\n",
    "    out_channel = _make_divisible(first_conv_filter, 8)\n",
    "    nn = conv2d_no_bias(\n",
    "        nn,\n",
    "        out_channel,\n",
    "        (3, 3),\n",
    "        strides=first_strides,\n",
    "        padding=\"same\",\n",
    "        use_torch_padding=is_torch_mode,\n",
    "        name=\"stem_\",\n",
    "    )\n",
    "    nn = batchnorm_with_activation(nn, use_torch_eps=is_torch_mode, name=\"stem_\")\n",
    "\n",
    "    pre_out = out_channel\n",
    "    global_block_id = 0\n",
    "    total_blocks = sum(depthes)\n",
    "    for id, (expand, out_channel, depth, stride, se, kernel_size) in enumerate(\n",
    "        zip(expands, out_channels, depthes, strides, use_ses, kernel_sizes)\n",
    "    ):\n",
    "        out = _make_divisible(out_channel, 8)\n",
    "        is_fused = True if se == 0 else False\n",
    "        for block_id in range(depth):\n",
    "            stride = stride if block_id == 0 else 1\n",
    "            shortcut = True if out == pre_out and stride == 1 else False\n",
    "            name = \"stack_{}_block{}_\".format(id, block_id)\n",
    "            block_drop_rate = drop_connect_rate * global_block_id / total_blocks\n",
    "            nn = MBConv(\n",
    "                nn,\n",
    "                out,\n",
    "                stride,\n",
    "                expand,\n",
    "                shortcut,\n",
    "                kernel_size,\n",
    "                block_drop_rate,\n",
    "                se,\n",
    "                is_fused,\n",
    "                is_torch_mode,\n",
    "                name=name,\n",
    "            )\n",
    "            pre_out = out\n",
    "            global_block_id += 1\n",
    "\n",
    "    output_conv_filter = _make_divisible(output_conv_filter, 8)\n",
    "    nn = conv2d_no_bias(\n",
    "        nn, output_conv_filter, (1, 1), strides=(1, 1), padding=\"valid\", name=\"post_\"\n",
    "    )\n",
    "    nn = batchnorm_with_activation(nn, use_torch_eps=is_torch_mode, name=\"post_\")\n",
    "\n",
    "    if num_classes > 0:\n",
    "        nn = GlobalAveragePooling2D(name=\"avg_pool\")(nn)\n",
    "        if dropout > 0 and dropout < 1:\n",
    "            nn = Dropout(dropout)(nn)\n",
    "        nn = Dense(\n",
    "            num_classes,\n",
    "            activation=classifier_activation,\n",
    "            dtype=\"float32\",\n",
    "            name=\"predictions\",\n",
    "        )(nn)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=nn, name=model_name)\n",
    "    model.rescale_mode = rescale_mode\n",
    "    reload_model_weights(model, model_type, pretrained)\n",
    "    return model\n",
    "\n",
    "\n",
    "def reload_model_weights(model, model_type, pretrained=\"imagenet\"):\n",
    "    pretrained_dd = {\n",
    "        \"imagenet\": \"imagenet\",\n",
    "        \"imagenet21k\": \"21k\",\n",
    "        \"imagenet21k-ft1k\": \"21k-ft1k\",\n",
    "        \"noisy_student\": \"noisy_student\",\n",
    "    }\n",
    "    if not pretrained in pretrained_dd:\n",
    "        print(\">>>> No pretrained available, model will be randomly initialized\")\n",
    "        return\n",
    "    pre_tt = pretrained_dd[pretrained]\n",
    "    if model_type not in FILE_HASH_DICT or pre_tt not in FILE_HASH_DICT[model_type]:\n",
    "        print(\">>>> No pretrained available, model will be randomly initialized\")\n",
    "        return\n",
    "\n",
    "    if model_type.startswith(\"v1\"):\n",
    "        pre_url = \"https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv1_pretrained/efficientnet{}-{}.h5\"\n",
    "    else:\n",
    "        pre_url = \"https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv2_pretrained/efficientnetv2-{}-{}.h5\"\n",
    "    url = pre_url.format(model_type, pre_tt)\n",
    "    file_name = os.path.basename(url)\n",
    "    file_hash = FILE_HASH_DICT[model_type][pre_tt]\n",
    "\n",
    "    try:\n",
    "        pretrained_model = keras.utils.get_file(\n",
    "            file_name, url, cache_subdir=\"models/efficientnetv2\", file_hash=file_hash\n",
    "        )\n",
    "    except:\n",
    "        print(\"[Error] will not load weights, url not found or download failed:\", url)\n",
    "        return\n",
    "    else:\n",
    "        print(\">>>> Load pretrained from:\", pretrained_model)\n",
    "        model.load_weights(pretrained_model, by_name=True, skip_mismatch=True)\n",
    "\n",
    "\n",
    "def EfficientNetV2B0(\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b0\", model_name=\"EfficientNetV2B0\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2B1(\n",
    "    input_shape=(240, 240, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b1\", model_name=\"EfficientNetV2B1\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2B2(\n",
    "    input_shape=(260, 260, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.3,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b2\", model_name=\"EfficientNetV2B2\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2B3(\n",
    "    input_shape=(300, 300, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.3,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b3\", model_name=\"EfficientNetV2B3\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2T(\n",
    "    input_shape=(320, 320, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    is_torch_mode = True\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"t\", model_name=\"EfficientNetV2T\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2S(\n",
    "    input_shape=(384, 384, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"s\", model_name=\"EfficientNetV2S\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2M(\n",
    "    input_shape=(480, 480, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.3,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"m\", model_name=\"EfficientNetV2M\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2L(\n",
    "    input_shape=(480, 480, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.4,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"l\", model_name=\"EfficientNetV2L\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2XL(\n",
    "    input_shape=(512, 512, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.4,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet21k-ft1k\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"xl\", model_name=\"EfficientNetV2XL\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def get_actual_drop_connect_rates(model):\n",
    "    return [ii.rate for ii in model.layers if isinstance(ii, keras.layers.Dropout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kimSOgK-kgpu"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# metrics\n",
    "def auc(y_true, y_pred):\n",
    "    # First, we need to convert the one-hot encoded labels and predicted probabilities\n",
    "    # into a single label and probability for each sample.\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = y_pred[:, 1:]\n",
    "\n",
    "    # Then we can use the roc_auc_score function to compute the AUC for each class\n",
    "    auc_scores = []\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        auc_scores.append(roc_auc_score(y_true, y_pred[:, i]))\n",
    "\n",
    "    # Return the average AUC across all classes\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def tf_mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.5):\n",
    "        y_pred_ = tf.cast(y_pred > t, tf.int32)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        prec.append(score)\n",
    "    val = K.mean(K.stack(prec), axis=0)\n",
    "    return [val, up_opt]\n",
    "\n",
    "\n",
    "def cross_entropy_balanced(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    count_neg = tf.reduce_sum(1.0 - y_true)\n",
    "    count_pos = tf.reduce_sum(y_true)\n",
    "\n",
    "    beta = count_neg / (count_pos + count_neg)\n",
    "\n",
    "    pos_weight = beta / (1 - beta)\n",
    "\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        logits=y_pred, labels=y_true, pos_weight=pos_weight\n",
    "    )\n",
    "\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "\n",
    "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the specificity metric.\n",
    "    \"\"\"\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    true_negatives_and_false_positives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (true_negatives_and_false_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NARqONvxjVks",
    "outputId": "3cce6c35-416f-440c-e8d3-85f41e1ab395"
   },
   "outputs": [],
   "source": [
    "# get model\n",
    "model = EfficientNetV2B0(num_classes=9, input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hBMNTaE0j7F0",
    "outputId": "e05bedbc-f2c1-4e90-fa97-63f7c9f81704"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "\n",
    "# from tensorflow.keras.utils import array_to_image\n",
    "import csv\n",
    "from keras.optimizers import optimizer\n",
    "from keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    TensorBoard,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install git+https://github.com/artemmavrin/focal-loss.git\n",
    "\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"accuracy\"),\n",
    "    EarlyStopping(monitor=\"accuracy\", patience=3),\n",
    "    TensorBoard(log_dir=\"logs\"),\n",
    "    ReduceLROnPlateau(monitor=\"loss\", patience=1, factor=0.1),\n",
    "]\n",
    "\n",
    "\n",
    "def lerp(start, end, weight):\n",
    "    return start + weight * (end - start)\n",
    "\n",
    "\n",
    "def sparse_lerp(start, end, weight):\n",
    "    # Mathematically equivalent, but you can't subtract a dense Tensor from sparse\n",
    "    # IndexedSlices, so we have to flip it around.\n",
    "    return start + weight * -(start - end)\n",
    "\n",
    "# Lion optimizer\n",
    "class Lion(optimizer.Optimizer):\n",
    "    r\"\"\"Optimizer that implements the Lion algorithm.\n",
    "    Lion was published in the paper \"Symbolic Discovery of Optimization Algorithms\"\n",
    "    which is available at https://arxiv.org/abs/2302.06675\n",
    "    Args:\n",
    "      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n",
    "        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
    "        that takes no arguments and returns the actual value to use. The\n",
    "        learning rate. Defaults to 1e-4.\n",
    "      beta_1: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use. Factor\n",
    "         used to interpolate the current gradient and the momentum. Defaults to 0.9.\n",
    "      beta_2: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use. The\n",
    "        exponential decay rate for the momentum. Defaults to 0.99.\n",
    "    Notes:\n",
    "    The sparse implementation of this algorithm (used when the gradient is an\n",
    "    IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "    lookup in the forward pass) does apply momentum to variable slices even if\n",
    "    they were not used in the forward pass (meaning they have a gradient equal\n",
    "    to zero). Momentum decay (beta2) is also applied to the entire momentum\n",
    "    accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "    behavior (in contrast to some momentum implementations which ignore momentum\n",
    "    unless a variable slice was actually used).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.99,\n",
    "        weight_decay=None,\n",
    "        clipnorm=None,\n",
    "        clipvalue=None,\n",
    "        global_clipnorm=None,\n",
    "        jit_compile=True,\n",
    "        name=\"Lion\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            weight_decay=weight_decay,\n",
    "            clipnorm=clipnorm,\n",
    "            clipvalue=clipvalue,\n",
    "            global_clipnorm=global_clipnorm,\n",
    "            jit_compile=jit_compile,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self._learning_rate = self._build_learning_rate(learning_rate)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def build(self, var_list):\n",
    "        \"\"\"Initialize optimizer variables.\n",
    "        var_list: list of model variables to build Lion variables on.\n",
    "        \"\"\"\n",
    "        super().build(var_list)\n",
    "        if hasattr(self, \"_built\") and self._built:\n",
    "            return\n",
    "        self._built = True\n",
    "        self._emas = []\n",
    "        for var in var_list:\n",
    "            self._emas.append(\n",
    "                self.add_variable_from_reference(\n",
    "                    model_variable=var, variable_name=\"ema\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def update_step(self, gradient, variable):\n",
    "        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n",
    "        lr = tf.cast(self.learning_rate, variable.dtype)\n",
    "        beta_1 = tf.constant(self.beta_1, shape=(1,))\n",
    "        beta_2 = tf.constant(self.beta_2, shape=(1,))\n",
    "\n",
    "        var_key = self._var_key(variable)\n",
    "        ema = self._emas[self._index_dict[var_key]]\n",
    "\n",
    "        if isinstance(gradient, tf.IndexedSlices):\n",
    "            # Sparse gradients.\n",
    "            lerp_fn = sparse_lerp\n",
    "        else:\n",
    "            # Dense gradients.\n",
    "            lerp_fn = lerp\n",
    "\n",
    "        update = lerp_fn(ema, gradient, 1 - beta_1)\n",
    "        update = tf.sign(update)\n",
    "        variable.assign_sub(update * lr)\n",
    "\n",
    "        ema.assign(lerp_fn(ema, gradient, 1 - beta_2))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(self._learning_rate),\n",
    "                \"beta_1\": self.beta_1,\n",
    "                \"beta_2\": self.beta_2,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=4.0):\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.0e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1.0, model_out), gamma))\n",
    "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "\n",
    "    return focal_loss_fixed\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# load already trained model\n",
    "def load_m(model_name):\n",
    "    model = load_model(\n",
    "        model_name,\n",
    "        custom_objects={\n",
    "            \"focal_loss_fixed\": focal_loss,\n",
    "            \"Lion\": Lion,\n",
    "            \"mish\": Mish(mish),\n",
    "            \"f1_m\": f1_m,\n",
    "            \"precision_m\": precision_m,\n",
    "            \"recall_m\": recall_m,\n",
    "        },\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = load_m(\"/content/drive/MyDrive/mod_t_20.04.h5\")\n",
    "model.compile(\n",
    "    optimizer=Lion(6e-6),\n",
    "    loss=focal_loss(),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        f1_m,\n",
    "        precision_m,\n",
    "        recall_m,\n",
    "        tf.keras.metrics.SpecificityAtSensitivity(0.5),\n",
    "    ],\n",
    ")\n",
    "# training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=train_class_weights_dict,\n",
    ")\n",
    "model.save(\"trained_model.h5\")\n",
    "# evaluating the model\n",
    "evals = model.evaluate(test_generator)\n",
    "\n",
    "\n",
    "def save_eval_results(eval_results, filename):\n",
    "    # Open the file in write mode and write the evaluation results to it\n",
    "    with open(filename, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(model.metrics_names)\n",
    "        writer.writerow(eval_results)\n",
    "\n",
    "# plotting the training history as a graph\n",
    "def plot_history(history, call=None):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(history.history), figsize=(20, 5))\n",
    "\n",
    "    for i, metric in enumerate(history.history.keys()):\n",
    "        axs[i].plot(history.history[metric])\n",
    "        axs[i].set_title(metric)\n",
    "        axs[i].set_xlabel(\"Epoch\")\n",
    "        axs[i].set_ylabel(metric)\n",
    "    plt.savefig(f\"{str(history)}_{call}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# for further evaluation\n",
    "def quick_test(test_generator, model, img_size):\n",
    "    # Get a batch of test data\n",
    "    batch_x, batch_y = test_generator.__getitem__(0)\n",
    "\n",
    "    # Generate predictions for the test data\n",
    "    pred_y = model.predict(batch_x)\n",
    "\n",
    "    # Create a pred folder if it doesn't exist\n",
    "    if not os.path.exists(\"pred\"):\n",
    "        os.makedirs(\"pred\")\n",
    "\n",
    "    # Loop through the test data and save each image and its predicted label to the pred folder\n",
    "    for i in range(batch_x.shape[0]):\n",
    "        # Convert the image array to a PIL image\n",
    "        img = array_to_img(batch_x[i])\n",
    "\n",
    "        # Get the predicted label for the image\n",
    "        pred_label = np.argmax(pred_y[i])\n",
    "\n",
    "        # Save the image with its predicted label as the filename\n",
    "        img.save(f\"pred/{pred_label}_{i}.jpg\")\n",
    "\n",
    "\n",
    "save_eval_results(evals, \"HC_100K_Results\")\n",
    "plot_history(history)\n",
    "quick_test(test_generator, model, (256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNvrJ00aIQ3Q"
   },
   "outputs": [],
   "source": [
    "#model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TkdX4uh-gf5K",
    "outputId": "bdf83436-a9ca-4495-c938-fee1f11118ca"
   },
   "outputs": [],
   "source": [
    "# testing other model\n",
    "model = load_m(\"/content/best_model.h5\")\n",
    "model.compile(\n",
    "    optimizer=Lion(6e-6),\n",
    "    loss=focal_loss(),\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        f1_m,\n",
    "        precision_m,\n",
    "        recall_m,\n",
    "        tf.keras.metrics.SpecificityAtSensitivity(0.5),\n",
    "    ],\n",
    ")\n",
    "evals = model.evaluate(test_generator)\n",
    "save_eval_results(evals, \"HC_100K_Results_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mmr1aLcQBKP4"
   },
   "outputs": [],
   "source": [
    "model.save(\"test_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "7efwMp6F18rN",
    "outputId": "4436e27d-c2ed-4c71-ed03-4ce16d14c181"
   },
   "outputs": [],
   "source": [
    "# move models to drive\n",
    "import shutil\n",
    "\n",
    "shutil.copy(\"/content/test_train.h5\", \"/content/drive/MyDrive/mod_t_26.04_2.h5\")\n",
    "shutil.copy(\"/content/best_model.h5\", \"/content/drive/MyDrive/best_mod_b_26.04_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWVzGR7aotvg"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# move other files created during training into zip \n",
    "# to download them all at once\n",
    "\n",
    "def zip_folder(folder_path, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                zip_file.write(os.path.join(root, file))\n",
    "    print(f\"{folder_path} successfully zipped to {zip_path}\")\n",
    "\n",
    "\n",
    "zip_folder(\"/content/pred\", \"/content/pred.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
