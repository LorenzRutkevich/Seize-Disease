{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnNpP8Af23gt",
    "outputId": "7e2f9d24-1453-46ad-ae6c-836cd0cbcfcf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# mount google drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RF_2CruE2rqf",
    "outputId": "f66ff33f-e80a-49cf-cf29-439a57f73e7d"
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install pycocotools\n",
    "!pip install vision-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXun9gY43Z1x",
    "outputId": "930e2139-dff7-466d-b829-8a7daf4adb27"
   },
   "outputs": [],
   "source": [
    "# unzip data into colab\n",
    "!unzip /content/drive/MyDrive/fasterrcnn.zip\n",
    "!unzip /content/drive/MyDrive/CRAG_JSON_2.zip -d CRAG_JSON_2\n",
    "!unzip /content/drive/MyDrive/CRAG_JSON.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xza4ycoI2q_Z",
    "outputId": "cc8160ea-9e81-4966-d59e-65ea3f3defd2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# adapted from https://medium.com/fullstackai/how-to-train-an-object-detector-with-your-own-coco-dataset-in-pytorch-319e7090da5\n",
    "\n",
    "# data loader following torch.utils.data.Dataset\n",
    "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_path: str,\n",
    "        y_path: str,\n",
    "        transforms: list = None,\n",
    "        annotation_style=\"rectangular\",\n",
    "        info: bool = False,\n",
    "        format: str = \"faster rcnn\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.annotation_style = annotation_style\n",
    "        self.format = format\n",
    "        assert self.annotation_style in [\"rectangular\", \"circular\"]\n",
    "        # faster rcnn requires [xmin, ymin, xmax, ymax], normal is [x, y, width, height]\n",
    "        assert self.format in [\"faster rcnn\", \"normal\"]\n",
    "\n",
    "        self.x_path = x_path\n",
    "        self.coco = COCO(y_path)\n",
    "        self.transforms = transforms\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.info = info\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        # filter out images without annotations\n",
    "        # might lead to errors if an image has no annotations\n",
    "        while True:\n",
    "            target = self.coco.loadAnns(self.coco.getAnnIds(self.ids[index]))\n",
    "            if len(target) == 0:\n",
    "                index = np.random.randint(0, len(self.ids))\n",
    "            else:\n",
    "                break\n",
    "        # prepare the image\n",
    "        image_id = self.ids[index]\n",
    "        image = Image.open(\n",
    "            os.path.join(self.x_path, self.coco.loadImgs(image_id)[0][\"file_name\"])\n",
    "        ).convert(\"RGB\")\n",
    "        image = torchvision.transforms.ToTensor()(image)\n",
    "        # augment the image\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        # prepare the target\n",
    "        target = self.coco.loadAnns(self.coco.getAnnIds(image_id))\n",
    "        if self.annotation_style == \"rectangular\":\n",
    "            # \"normal\" format is [x, y, width, height]\n",
    "            if self.format == \"normal\":\n",
    "                boxes = torch.as_tensor(\n",
    "                    [x[\"bbox\"] for x in target], dtype=torch.float32\n",
    "                )\n",
    "            else:\n",
    "                # faster rcnn format is [xmin, ymin, xmax, ymax]\n",
    "                boxes = torch.as_tensor(\n",
    "                    [\n",
    "                        [\n",
    "                            x[\"bbox\"][0],\n",
    "                            x[\"bbox\"][1],\n",
    "                            x[\"bbox\"][0] + x[\"bbox\"][2],\n",
    "                            x[\"bbox\"][1] + x[\"bbox\"][3],\n",
    "                        ]\n",
    "                        for x in target\n",
    "                    ],\n",
    "                    dtype=torch.float32,\n",
    "                )\n",
    "            labels = torch.as_tensor(\n",
    "                [x[\"category_id\"] for x in target], dtype=torch.int64\n",
    "            )\n",
    "            area = torch.as_tensor([x[\"area\"] for x in target], dtype=torch.float32)\n",
    "            iscrowd = torch.as_tensor([x[\"iscrowd\"] for x in target], dtype=torch.int64)\n",
    "\n",
    "            # check for integrity of the boxes\n",
    "            valid_indices = [\n",
    "                i for i, box in enumerate(boxes) if box[2] > box[0] and box[3] > box[1]\n",
    "            ]\n",
    "\n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": labels,\n",
    "                \"image_id\": torch.tensor([image_id]),\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": iscrowd,\n",
    "            }\n",
    "            # format of labels is [x, y, width, height]\n",
    "            # target = [target]\n",
    "            print(len(target), type(target), target) if self.info else None\n",
    "\n",
    "        elif self.annotation_style == \"circular\":\n",
    "            # filter out images without annotations\n",
    "            while True:\n",
    "                target = self.coco.loadAnns(self.coco.getAnnIds(self.ids[index]))\n",
    "                if len(target) == 0:\n",
    "                    index = np.random.randint(0, len(self.ids))\n",
    "                else:\n",
    "                    break\n",
    "            circle_center = torch.as_tensor(\n",
    "                [x[\"circle_center\"] for x in target], dtype=torch.float32\n",
    "            )  # center of the circle is given as [x, y]\n",
    "            circle_radius = torch.as_tensor(\n",
    "                [x[\"circle_radius\"] for x in target], dtype=torch.float32\n",
    "            )\n",
    "            labels = torch.as_tensor(\n",
    "                [x[\"category_id\"] for x in target], dtype=torch.int64\n",
    "            )\n",
    "            area = torch.as_tensor([x[\"area\"] for x in target], dtype=torch.float32)\n",
    "            iscrowd = torch.as_tensor([x[\"iscrowd\"] for x in target], dtype=torch.int64)\n",
    "\n",
    "            # check for integrity of the boxes\n",
    "            valid_indices = [i for i, radius in enumerate(circle_radius) if radius > 0]\n",
    "            if len(valid_indices) == 0:\n",
    "                return None\n",
    "\n",
    "            target = {\n",
    "                \"circle_center\": circle_center,\n",
    "                \"circle_radius\": circle_radius,\n",
    "                \"labels\": labels,\n",
    "                \"image_id\": torch.tensor([image_id]),\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": iscrowd,\n",
    "            }\n",
    "        #    target = [target]\n",
    "        print(\n",
    "            len(target), image.shape, np.min(image.numpy()), np.max(image.numpy())\n",
    "        ) if self.info else None\n",
    "        return image, target\n",
    "\n",
    "    # return the number of samples in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "paths = {\n",
    "    \"train_y_c\": \"/content/CRAG_JSON/train/C_Json/bbox_train.json\",\n",
    "    \"train_x\": \"/content/CRAG_JSON/train/Images/\",\n",
    "    \"train_y\": \"/content/CRAG_JSON/train/Json/bbox_train.json\",\n",
    "    \"test_y\": \"/content/CRAG_JSON/test/Json/bbox_test.json\",\n",
    "    \"test_x\": \"/content/CRAG_JSON/test/Images/\",\n",
    "}\n",
    "\n",
    "paths_2 = {\n",
    "    \"train_y_c\": \"/content/CRAG_JSON_2/CRAG/train/C_JSON/instances_train.json\",\n",
    "    \"train_x\": \"/content/CRAG_JSON_2/CRAG/train/Images/\",\n",
    "    \"train_y\": \"/content/CRAG_JSON_2/CRAG/train/JSON/instances_train.json\",\n",
    "    \"test_y\": \"/content/CRAG_JSON_2/CRAG/test/JSON/instances_test.json\",\n",
    "    \"test_x\": \"/content/CRAG_JSON_2/CRAG/test/Images/\",\n",
    "}\n",
    "\n",
    "# get data loaders \n",
    "train_dataset = ObjectDetectionDataset(\n",
    "    paths_2[\"train_x\"], paths_2[\"train_y\"], annotation_style=\"rectangular\"\n",
    ")\n",
    "test_dataset = ObjectDetectionDataset(\n",
    "    paths_2[\"test_x\"], paths_2[\"test_y\"], annotation_style=\"rectangular\"\n",
    ")\n",
    "train_dataset_c = ObjectDetectionDataset(\n",
    "    paths_2[\"train_x\"], paths_2[\"train_y_c\"], annotation_style=\"circular\"\n",
    ")\n",
    "\n",
    "# collate function for data loader to return a tuple of images and targets\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "train_dataloader_c = torch.utils.data.DataLoader(\n",
    "    train_dataset_c,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# check data integrity\n",
    "def _info_ex(\n",
    "    data_loader,\n",
    "    return_keys: bool = True,\n",
    "    num_examples: int = 1,\n",
    "    print_info: bool = False,\n",
    "    annotation_style: str = \"rectangular\",\n",
    "):\n",
    "    assert annotation_style in [\"rectangular\", \"circular\"]\n",
    "    # circular annotation style has different keys\n",
    "    # it is following circle net format: https://github.com/hrlblab/CircleNet/tree/master\n",
    "    target_info = []\n",
    "    keys = []  # Initialize an empty list to store keys\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        if annotation_style == \"rectangular\":\n",
    "            target_info.append(targets[0][\"boxes\"].shape), target_info.append(\n",
    "                targets[0][\"labels\"].shape\n",
    "            )\n",
    "        else:\n",
    "            target_info.append(targets[0][\"circle_center\"].shape), target_info.append(\n",
    "                targets[0][\"circle_radius\"].shape\n",
    "            )\n",
    "        if print_info:\n",
    "            target_shapes = \", \".join([str(shape) for shape in target_info])\n",
    "            print(\n",
    "                f\"Data Info: \\n Shapes -> Image: {images[0].shape}, Target: {target_shapes}, Image Min: {torch.min(images[0])}, Image Max: {torch.max(images[0])}\\n \"\n",
    "                f\"Types -> Image: {images[0].dtype}, {type(images)} Target: {[target.dtype for target in targets[0].values()]}, {type(targets)}\"\n",
    "            )\n",
    "        if i == num_examples - 1:\n",
    "            break\n",
    "    if return_keys:\n",
    "        keys = [key for key in targets[0].keys()]  # Collect keys after loop completion\n",
    "        return keys\n",
    "\n",
    "\n",
    "_info_ex(train_dataloader, annotation_style=\"rectangular\", print_info=True)\n",
    "\n",
    "# display some examples\n",
    "def _ex(data_loader, annotaitons_style:str = \"rectangular\"):\n",
    "    images, targets = next(iter(data_loader))\n",
    "    if annotaitons_style == \"rectangular\":\n",
    "        \n",
    "        image = images[0]\n",
    "        target = targets[0]\n",
    "        boxes = target['boxes']\n",
    "\n",
    "        image = image.permute(1, 2, 0).cpu().numpy()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "\n",
    "        for bbox in boxes:\n",
    "            x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(image)\n",
    "            \n",
    "    else:\n",
    "        circle_center = targets[0]['circle_center']\n",
    "        circle_radius = targets[0]['circle_radius']\n",
    "        for i in range(len(circle_center)):\n",
    "            x, y = circle_center[i]\n",
    "            r = circle_radius[i]\n",
    "            plt.imshow(images[0].permute(1, 2, 0))\n",
    "            plt.plot([x], [y], 'o')\n",
    "            plt.show()\n",
    "            plt.imshow(images[0].permute(1, 2, 0))\n",
    "            plt.plot([x], [y], 'o')\n",
    "            circle = plt.Circle((x, y), r, color='r', fill=False)\n",
    "            plt.gca().add_patch(circle)\n",
    "            plt.show()\n",
    "\n",
    "#_ex(train_dataloader, annotaitons_style=\"rectangular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBVLY3ew2q_d"
   },
   "outputs": [],
   "source": [
    "# torch faster rcnn models\n",
    "# by @sovit-123 https://github.com/sovit-123\n",
    "# code: https://github.com/sovit-123/fasterrcnn-pytorch-training-pipeline/tree/main\n",
    "\n",
    "\"\"\" from fasterrcnn_mobilevit_xss import create_model as fasterrcnn_mobilevit_xss\n",
    "from fasterrcnn_vitdet_tiny import create_model as fasterrcnn_vitdet_tiny\n",
    "from fasterrcnn_mbv3_nano import create_model as fasterrcnn_mbv3_nano\n",
    "from fasterrcnn_convnext_tiny import create_model as fasterrcnn_convnext_tiny\n",
    "from fasterrcnn_effb0 import create_model as fasterrcnn_effb0\n",
    "from nano_fasterrcnn import create_model as nano_fasterrcnn\n",
    "from fasterrcnn_mini_squeezenet1_1_tiny_head import (\n",
    "    create_model as fasterrcnn_mini_squeezenet1_1_tiny_head,\n",
    ")\n",
    "from fasterrcnn_regnet_y_400mf import create_model as fasterrcnn_regnet_y_400mf\n",
    "from fasterrcnn_resnet50_fpn_v2 import create_model as fasterrcnn_resnet50_fpn_v2\n",
    "from fasterrcnn_resnet18 import create_model as fasterrcnn_resnet18\n",
    "from fasterrcnn_mini_darknet_nano_head import (\n",
    "    create_model as fasterrcnn_mini_darknet_nano_head,\n",
    ")\n",
    "from fasterrcnn_darknet import create_model as fasterrcnn_darknet\n",
    "from fasterrcnn_convnext_small import create_model as fasterrcnn_convnext_small\n",
    "from fasterrcnn_custom_resnet import create_model as fasterrcnn_custom_resnet \"\"\"\n",
    "\n",
    "from fasterrcnn_resnet101 import create_model as fasterrcnn_resnet101\n",
    "\n",
    "from utils import eval_forward\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Lion(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-4,\n",
    "        betas: tuple = (0.9, 0.99),\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        assert lr > 0.0\n",
    "        assert all([0.0 <= beta <= 1.0 for beta in betas])\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in filter(lambda p: p.grad is not None, group[\"params\"]):\n",
    "                # parameter\n",
    "                grad, lr, wd, beta1, beta2, state = (\n",
    "                    p.grad,\n",
    "                    group[\"lr\"],\n",
    "                    group[\"weight_decay\"],\n",
    "                    *group[\"betas\"],\n",
    "                    self.state[p],\n",
    "                )\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "\n",
    "                # Lion optimizer\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "                update = exp_avg.clone().lerp_(grad, 1 - beta1)\n",
    "                p.add_(torch.sign(update), alpha=-lr)\n",
    "                exp_avg.lerp_(grad, 1 - beta2)\n",
    "        return loss\n",
    "\n",
    "# write training and validation losses to csv\n",
    "def write_to_csv(\n",
    "    loss_sum: float,\n",
    "    losses: dict,\n",
    "    epoch: int,\n",
    "    header_written: bool = False,\n",
    "    filename: str = f\"train_{datetime.date.today()}.csv\",\n",
    "):\n",
    "    with open(filename, \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not header_written:\n",
    "            writer.writerow([\"epoch\", \"loss\", losses.keys()])\n",
    "            header_written = True\n",
    "        writer.writerow([epoch + 1, loss_sum, losses.values()])\n",
    "    return header_written\n",
    "\n",
    "# train and test function\n",
    "def obj_inference(\n",
    "    model,\n",
    "    train_generator,\n",
    "    test_generator,\n",
    "    device: str,\n",
    "    num_epochs: int = 5,\n",
    "    test_after_epoch: bool = True,\n",
    "    save_model: bool = True,\n",
    "    save_path: str = \"model_epoch_\",\n",
    "    save_to_csv: bool = True,\n",
    "):\n",
    "    start = time.time()\n",
    "    lr_checker = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch = 0\n",
    "        train_loss_oa = 0\n",
    "        if len(lr_checker) > 2:\n",
    "            # Check if validation loss increased for the last two epochs\n",
    "            if lr_checker[-1] > lr_checker[-2] and lr_checker[-2] > lr_checker[-3]:\n",
    "                print(\"Learning rate reduced\")\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    # reduce learning rate by .1 and weight decay by .1\n",
    "                    param_group[\"lr\"] = param_group[\"lr\"] * 0.1\n",
    "                    param_group[\"weight_decay\"] = param_group[\"weight_decay\"] * 0.1\n",
    "        for x, y in train_generator:\n",
    "            batch += 1\n",
    "            x = list(target.to(device) for target in x)\n",
    "            y = [{k: v.to(device) for k, v in target.items()} for target in y]\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(x, y)\n",
    "            train_loss = sum(loss for loss in loss_dict.values())\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_oa += train_loss  # overall loss\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start\n",
    "            avg_time_per_batch = elapsed_time / batch\n",
    "            remaining_time = avg_time_per_batch * (len(train_generator) - batch)\n",
    "            print(\n",
    "                f\"\\rEpoch: {epoch + 1}, Batch: {batch}/{len(train_generator)}, Train Loss: {train_loss:.4f}, Elapsed Time: {elapsed_time:.2f}s/{elapsed_time/60:.2f}m, Remaining Time (epoch): {remaining_time:.2f}s/{remaining_time/60:.2f}m\",\n",
    "                end=\"\",\n",
    "            )\n",
    "            if save_to_csv:\n",
    "                header_written = write_to_csv(\n",
    "                    train_loss,\n",
    "                    loss_dict,\n",
    "                    epoch,\n",
    "                    header_written=True if batch == 1 else False,\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}, Train Loss: {train_loss_oa / len(train_generator):.4f}\"\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), save_path + str(epoch + 1))\n",
    "            torch.save(model, save_path + \"cm_\" + str(epoch + 1))\n",
    "        del loss_dict, train_loss_oa, train_loss\n",
    "        model.eval()\n",
    "        val_loss_oa = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_generator:\n",
    "                x = list(target.to(device) for target in x)\n",
    "                y = [{k: v.to(device) for k, v in target.items()} for target in y]\n",
    "                loss_dict, detections = eval_forward(model, x, y)\n",
    "                val_loss = sum(loss for loss in loss_dict.values())\n",
    "                val_loss_oa += val_loss\n",
    "                print(\n",
    "                    f\"\\rValidation Loss: {val_loss:.4f}, Overall Losses: {loss_dict}\",\n",
    "                    end=\"\",\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "        validation_loss = val_loss_oa / len(test_dataloader)\n",
    "        lr_checker.append(validation_loss)\n",
    "        print(f\"Validation Loss: {validation_loss:.4f}\\n\")\n",
    "        if save_to_csv:\n",
    "            header_written = write_to_csv(\n",
    "                validation_loss,\n",
    "                loss_dict,\n",
    "                epoch,\n",
    "                header_written=True if batch == 1 else False,\n",
    "                filename=f\"test_{datetime.date.today()}.csv\",\n",
    "            )\n",
    "        del loss_dict, val_loss_oa, val_loss\n",
    "\n",
    "\n",
    "model = fasterrcnn_resnet101(num_classes=2, pretrained=True, coco_model=False)\n",
    "optimizer = Lion(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "num_epochs = 30\n",
    "\n",
    "obj_inference(model, train_dataloader, test_dataloader, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vq3giwsTNKDK"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load model trained model\n",
    "model.load_state_dict(\n",
    "    torch.load(\"/path/to/saved/model.pth\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "model.eval() # model needs to be set to eval mode for it to be able to only take in an image and return predictions\n",
    "test_images, ground_truth = next(iter(test_dataloader))\n",
    "test_images = list(image.to(device) for image in test_images)\n",
    "test_outputs = model(test_images)\n",
    "test_output = [\n",
    "    {k: v.to(torch.device(\"cpu\")) for k, v in t.items()} for t in test_outputs\n",
    "]\n",
    "# print(test_output)\n",
    "\n",
    "# predict on a batch of images\n",
    "def display_test(test_images, predictions, ground_truth, image_index):\n",
    "    image = test_images[image_index]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pred_boxes = predictions[image_index][\"boxes\"].detach().numpy()\n",
    "    for bbox in pred_boxes:\n",
    "        x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "        x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    gt_boxes = ground_truth[image_index][\"boxes\"].detach().numpy()\n",
    "    for bbox in gt_boxes:\n",
    "        x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "        x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    image = test_images[image_index]\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    gt_boxes = ground_truth[image_index][\"boxes\"].detach().numpy()\n",
    "    for bbox in gt_boxes:\n",
    "        x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "        x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    image = test_images[image_index]\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pred_boxes = predictions[image_index][\"boxes\"].detach().numpy()\n",
    "    for bbox in pred_boxes:\n",
    "        x1, y1, x2, y2 = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "        x, y, w, h = x1, y1, x2 - x1, y2 - y1\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (x, y),\n",
    "            (x + w, y + h),\n",
    "            (0, 255, 0),\n",
    "        )\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    display_test(test_images, test_output, ground_truth, i)\n",
    "\n",
    "\n",
    "def IOU(box1, box2):\n",
    "    x1, y1, w1, h1 = box1[0], box1[1], box1[2], box1[3]\n",
    "\n",
    "    x2, y2, w2, h2 = box2[0], box2[1], box2[2], box2[3]\n",
    "\n",
    "    w_intersection = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "\n",
    "    h_intersection = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "\n",
    "    if w_intersection <= 0 or h_intersection <= 0:\n",
    "        return 0\n",
    "\n",
    "    I = w_intersection * h_intersection\n",
    "\n",
    "    U = w1 * h1 + w2 * h2 - I\n",
    "\n",
    "    return I / U\n",
    "\n",
    "\n",
    "\"\"\" for i in range(len(ground_truth[1]['boxes'])):\n",
    "    box1 = test_outputs[1]['boxes'][i].detach().numpy()\n",
    "    box1 = box1[0], box1[1], box1[2]-box1[0], box1[3]-box1[1]\n",
    "\n",
    "    box2 = ground_truth[1]['boxes'][i].detach().numpy()\n",
    "    box2 = box2[0], box2[1], box2[2]-box2[0], box2[3]-box2[1]\n",
    "\n",
    "    print(IOU(box1, box2)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjMgF-I-GDjM"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# move trained model to drive\n",
    "shutil.move(\n",
    "    \"/content/model_epoch_4\",\n",
    "    \"/content/drive/MyDrive/model_epoch_4_24.08_fasterrcnn_2.pth\",\n",
    ")\n",
    "# shutil.move(\"/content/model_epoch_7\", \"/content/drive/MyDrive/model_epoch_7_22.08_fasterrcnn_vitdet_tiny.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
