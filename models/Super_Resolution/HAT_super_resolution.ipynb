{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoDeD4KJpFN5",
    "outputId": "f96e4afe-90a6-44c0-f65d-a01e081f030c"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "# for colab use\n",
    "missing_libraries = []\n",
    "try:\n",
    "    from einops import rearrange\n",
    "except ImportError:\n",
    "    missing_libraries.append(\"einops\")\n",
    "try:\n",
    "    import triton\n",
    "    import triton.language as tl\n",
    "except ImportError:\n",
    "    missing_libraries.append(\"triton\")\n",
    "try:\n",
    "    from basicsr.utils.registry import ARCH_REGISTRY\n",
    "    from basicsr.archs.arch_util import to_2tuple, trunc_normal_\n",
    "except ImportError:\n",
    "    missing_libraries.append(\"basicsr\")\n",
    "# try:\n",
    "#    import torchmetrics\n",
    "# except ImportError:\n",
    "#    missing_libraries.append(\"torchmetrics\")\n",
    "try:\n",
    "    from torchgeometry.image import get_gaussian_kernel2d\n",
    "except ImportError:\n",
    "    missing_libraries.append(\"torchgeometry\")\n",
    "\n",
    "if missing_libraries:\n",
    "    print(\"The following libraries are missing: \" + \", \".join(missing_libraries))\n",
    "    print(\"Installing missing libraries...\")\n",
    "    if \"einops\" in missing_libraries:\n",
    "        !pip install einops\n",
    "        from einops import rearrange\n",
    "    if \"triton\" in missing_libraries:\n",
    "        !pip install triton\n",
    "        import triton\n",
    "        import triton.language as tl\n",
    "    if \"basicsr\" in missing_libraries:\n",
    "        !pip install basicsr\n",
    "        from basicsr.utils.registry import ARCH_REGISTRY\n",
    "        from basicsr.archs.arch_util import to_2tuple, trunc_normal_\n",
    "    if \"torchgeometry\" in missing_libraries:\n",
    "        !pip install torchgeometry\n",
    "        from torchgeometry.image import get_gaussian_kernel2d\n",
    "    #    if \"torchmetrics\" in missing_libraries:\n",
    "    #        !pip install torchmetrics\n",
    "    #        from torchgeometry.image import get_gaussian_kernel2d\n",
    "    print(\"Libraries installed successfully.\")\n",
    "else:\n",
    "    print(\"All required libraries are already installed.\")\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n",
    "    \"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (\n",
    "        x.ndim - 1\n",
    "    )  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "\n",
    "    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention used in RCAN.\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "        squeeze_factor (int): Channel squeeze factor. Default: 16.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat, squeeze_factor=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(num_feat, num_feat // squeeze_factor, 1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_feat // squeeze_factor, num_feat, 1, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class CAB(nn.Module):\n",
    "    def __init__(self, num_feat, compress_ratio=3, squeeze_factor=30):\n",
    "        super(CAB, self).__init__()\n",
    "\n",
    "        self.cab = nn.Sequential(\n",
    "            nn.Conv2d(num_feat, num_feat // compress_ratio, 3, 1, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(num_feat // compress_ratio, num_feat, 3, 1, 1),\n",
    "            ChannelAttention(num_feat, squeeze_factor),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cab(x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (b, h, w, c)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*b, window_size, window_size, c)\n",
    "    \"\"\"\n",
    "    b, h, w, c = x.shape\n",
    "    x = x.view(b, h // window_size, window_size, w // window_size, window_size, c)\n",
    "    windows = (\n",
    "        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, c)\n",
    "    )\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*b, window_size, window_size, c)\n",
    "        window_size (int): Window size\n",
    "        h (int): Height of image\n",
    "        w (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (b, h, w, c)\n",
    "    \"\"\"\n",
    "    b = int(windows.shape[0] / (h * w / window_size / window_size))\n",
    "    x = windows.view(\n",
    "        b, h // window_size, w // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, rpi, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*b, n, c)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        b_, n, c = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(b_, n, 3, self.num_heads, c // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            -1,\n",
    "        )  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(\n",
    "            2, 0, 1\n",
    "        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]\n",
    "            attn = attn.view(b_ // nw, nw, self.num_heads, n, n) + mask.unsqueeze(\n",
    "                1\n",
    "            ).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b_, n, c)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HAB(nn.Module):\n",
    "    r\"\"\"Hybrid Attention Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        compress_ratio=3,\n",
    "        squeeze_factor=30,\n",
    "        conv_scale=0.01,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert (\n",
    "            0 <= self.shift_size < self.window_size\n",
    "        ), \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.conv_scale = conv_scale\n",
    "        self.conv_block = CAB(\n",
    "            num_feat=dim, compress_ratio=compress_ratio, squeeze_factor=squeeze_factor\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_size, rpi_sa, attn_mask):\n",
    "        h, w = x_size\n",
    "        b, _, c = x.shape\n",
    "        # assert seq_len == h * w, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(b, h, w, c)\n",
    "\n",
    "        # Conv_X\n",
    "        conv_x = self.conv_block(x.permute(0, 3, 1, 2))\n",
    "        conv_x = conv_x.permute(0, 2, 3, 1).contiguous().view(b, h * w, c)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(\n",
    "                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "            attn_mask = attn_mask\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(\n",
    "            shifted_x, self.window_size\n",
    "        )  # nw*b, window_size, window_size, c\n",
    "        x_windows = x_windows.view(\n",
    "            -1, self.window_size * self.window_size, c\n",
    "        )  # nw*b, window_size*window_size, c\n",
    "\n",
    "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
    "        attn_windows = self.attn(x_windows, rpi=rpi_sa, mask=attn_mask)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, c)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, h, w)  # b h' w' c\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            attn_x = torch.roll(\n",
    "                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "        else:\n",
    "            attn_x = shifted_x\n",
    "        attn_x = attn_x.view(b, h * w, c)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(attn_x) + conv_x * self.conv_scale\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\"Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: b, h*w, c\n",
    "        \"\"\"\n",
    "        h, w = self.input_resolution\n",
    "        b, seq_len, c = x.shape\n",
    "        assert seq_len == h * w, \"input feature has wrong size\"\n",
    "        assert h % 2 == 0 and w % 2 == 0, f\"x size ({h}*{w}) are not even.\"\n",
    "\n",
    "        x = x.view(b, h, w, c)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # b h/2 w/2 c\n",
    "        x1 = x[:, 1::2, 0::2, :]  # b h/2 w/2 c\n",
    "        x2 = x[:, 0::2, 1::2, :]  # b h/2 w/2 c\n",
    "        x3 = x[:, 1::2, 1::2, :]  # b h/2 w/2 c\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # b h/2 w/2 4*c\n",
    "        x = x.view(b, -1, 4 * c)  # b h/2*w/2 4*c\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class OCAB(nn.Module):\n",
    "    # overlapping cross-attention block\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        window_size,\n",
    "        overlap_ratio,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        mlp_ratio=2,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "        self.overlap_win_size = int(window_size * overlap_ratio) + window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.unfold = nn.Unfold(\n",
    "            kernel_size=(self.overlap_win_size, self.overlap_win_size),\n",
    "            stride=window_size,\n",
    "            padding=(self.overlap_win_size - window_size) // 2,\n",
    "        )\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros(\n",
    "                (window_size + self.overlap_win_size - 1)\n",
    "                * (window_size + self.overlap_win_size - 1),\n",
    "                num_heads,\n",
    "            )\n",
    "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_size, rpi):\n",
    "        h, w = x_size\n",
    "        b, _, c = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(b, h, w, c)\n",
    "\n",
    "        qkv = self.qkv(x).reshape(b, h, w, 3, c).permute(3, 0, 4, 1, 2)  # 3, b, c, h, w\n",
    "        q = qkv[0].permute(0, 2, 3, 1)  # b, h, w, c\n",
    "        kv = torch.cat((qkv[1], qkv[2]), dim=1)  # b, 2*c, h, w\n",
    "\n",
    "        # partition windows\n",
    "        q_windows = window_partition(\n",
    "            q, self.window_size\n",
    "        )  # nw*b, window_size, window_size, c\n",
    "        q_windows = q_windows.view(\n",
    "            -1, self.window_size * self.window_size, c\n",
    "        )  # nw*b, window_size*window_size, c\n",
    "\n",
    "        kv_windows = self.unfold(kv)  # b, c*w*w, nw\n",
    "        kv_windows = rearrange(\n",
    "            kv_windows,\n",
    "            \"b (nc ch owh oww) nw -> nc (b nw) (owh oww) ch\",\n",
    "            nc=2,\n",
    "            ch=c,\n",
    "            owh=self.overlap_win_size,\n",
    "            oww=self.overlap_win_size,\n",
    "        ).contiguous()  # 2, nw*b, ow*ow, c\n",
    "        k_windows, v_windows = kv_windows[0], kv_windows[1]  # nw*b, ow*ow, c\n",
    "\n",
    "        b_, nq, _ = q_windows.shape\n",
    "        _, n, _ = k_windows.shape\n",
    "        d = self.dim // self.num_heads\n",
    "        q = q_windows.reshape(b_, nq, self.num_heads, d).permute(\n",
    "            0, 2, 1, 3\n",
    "        )  # nw*b, nH, nq, d\n",
    "        k = k_windows.reshape(b_, n, self.num_heads, d).permute(\n",
    "            0, 2, 1, 3\n",
    "        )  # nw*b, nH, n, d\n",
    "        v = v_windows.reshape(b_, n, self.num_heads, d).permute(\n",
    "            0, 2, 1, 3\n",
    "        )  # nw*b, nH, n, d\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[rpi.view(-1)].view(\n",
    "            self.window_size * self.window_size,\n",
    "            self.overlap_win_size * self.overlap_win_size,\n",
    "            -1,\n",
    "        )  # ws*ws, wse*wse, nH\n",
    "        relative_position_bias = relative_position_bias.permute(\n",
    "            2, 0, 1\n",
    "        ).contiguous()  # nH, ws*ws, wse*wse\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn_windows = (attn @ v).transpose(1, 2).reshape(b_, nq, self.dim)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(\n",
    "            -1, self.window_size, self.window_size, self.dim\n",
    "        )\n",
    "        x = window_reverse(attn_windows, self.window_size, h, w)  # b h w c\n",
    "        x = x.view(b, h * w, self.dim)\n",
    "\n",
    "        x = self.proj(x) + shortcut\n",
    "\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttenBlocks(nn.Module):\n",
    "    \"\"\"A series of attention blocks for one RHAG.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        compress_ratio,\n",
    "        squeeze_factor,\n",
    "        conv_scale,\n",
    "        overlap_ratio,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                HAB(\n",
    "                    dim=dim,\n",
    "                    input_resolution=input_resolution,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                    compress_ratio=compress_ratio,\n",
    "                    squeeze_factor=squeeze_factor,\n",
    "                    conv_scale=conv_scale,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # OCAB\n",
    "        self.overlap_attn = OCAB(\n",
    "            dim=dim,\n",
    "            input_resolution=input_resolution,\n",
    "            window_size=window_size,\n",
    "            overlap_ratio=overlap_ratio,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(\n",
    "                input_resolution, dim=dim, norm_layer=norm_layer\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, x_size, params):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, x_size, params[\"rpi_sa\"], params[\"attn_mask\"])\n",
    "\n",
    "        x = self.overlap_attn(x, x_size, params[\"rpi_oca\"])\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RHAG(nn.Module):\n",
    "    \"\"\"Residual Hybrid Attention Group (RHAG).\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        img_size: Input image size.\n",
    "        patch_size: Patch size.\n",
    "        resi_connection: The convolutional block before residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        compress_ratio,\n",
    "        squeeze_factor,\n",
    "        conv_scale,\n",
    "        overlap_ratio,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "        img_size=224,\n",
    "        patch_size=4,\n",
    "        resi_connection=\"1conv\",\n",
    "    ):\n",
    "        super(RHAG, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        self.residual_group = AttenBlocks(\n",
    "            dim=dim,\n",
    "            input_resolution=input_resolution,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            compress_ratio=compress_ratio,\n",
    "            squeeze_factor=squeeze_factor,\n",
    "            conv_scale=conv_scale,\n",
    "            overlap_ratio=overlap_ratio,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            drop=drop,\n",
    "            attn_drop=attn_drop,\n",
    "            drop_path=drop_path,\n",
    "            norm_layer=norm_layer,\n",
    "            downsample=downsample,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "        )\n",
    "\n",
    "        if resi_connection == \"1conv\":\n",
    "            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n",
    "        elif resi_connection == \"identity\":\n",
    "            self.conv = nn.Identity()\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=0,\n",
    "            embed_dim=dim,\n",
    "            norm_layer=None,\n",
    "        )\n",
    "\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=0,\n",
    "            embed_dim=dim,\n",
    "            norm_layer=None,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_size, params):\n",
    "        return (\n",
    "            self.patch_embed(\n",
    "                self.conv(\n",
    "                    self.patch_unembed(self.residual_group(x, x_size, params), x_size)\n",
    "                )\n",
    "            )\n",
    "            + x\n",
    "        )\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\"Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [\n",
    "            img_size[0] // patch_size[0],\n",
    "            img_size[1] // patch_size[1],\n",
    "        ]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose(1, 2)  # b Ph*Pw c\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Module):\n",
    "    r\"\"\"Image to Patch Unembedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [\n",
    "            img_size[0] // patch_size[0],\n",
    "            img_size[1] // patch_size[1],\n",
    "        ]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(x.shape[0], self.embed_dim, x_size[0], x_size[1])\n",
    "        )  # b Ph*Pw c\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"Upsample module.\n",
    "\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"scale {scale} is not supported. \" \"Supported scales: 2^n and 3.\"\n",
    "            )\n",
    "        super(Upsample, self).__init__(*m)\n",
    "\n",
    "\n",
    "# @ARCH_REGISTRY.register()\n",
    "class HAT(nn.Module):\n",
    "    r\"\"\"Hybrid Attention Transformer\n",
    "        A PyTorch implementation of : `Activating More Pixels in Image Super-Resolution Transformer`.\n",
    "        Some codes are based on SwinIR.\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 64\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
    "        img_range: Image range. 1. or 255.\n",
    "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
    "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=64,\n",
    "        patch_size=1,\n",
    "        in_chans=3,\n",
    "        embed_dim=96,\n",
    "        depths=(6, 6, 6, 6),\n",
    "        num_heads=(6, 6, 6, 6),\n",
    "        window_size=7,\n",
    "        compress_ratio=3,\n",
    "        squeeze_factor=30,\n",
    "        conv_scale=0.01,\n",
    "        overlap_ratio=0.5,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        ape=False,\n",
    "        patch_norm=True,\n",
    "        use_checkpoint=False,\n",
    "        upscale=2,\n",
    "        img_range=1.0,\n",
    "        upsampler=\"\",\n",
    "        resi_connection=\"1conv\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(HAT, self).__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "\n",
    "        num_in_ch = in_chans\n",
    "        num_out_ch = in_chans\n",
    "        num_feat = 64\n",
    "        self.img_range = img_range\n",
    "        if in_chans == 3:\n",
    "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n",
    "        else:\n",
    "            self.mean = torch.zeros(1, 1, 1, 1)\n",
    "        self.upscale = upscale\n",
    "        self.upsampler = upsampler\n",
    "\n",
    "        # relative position index\n",
    "        relative_position_index_SA = self.calculate_rpi_sa()\n",
    "        relative_position_index_OCA = self.calculate_rpi_oca()\n",
    "        self.register_buffer(\"relative_position_index_SA\", relative_position_index_SA)\n",
    "        self.register_buffer(\"relative_position_index_OCA\", relative_position_index_OCA)\n",
    "\n",
    "        # ------------------------- 1, shallow feature extraction ------------------------- #\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n",
    "\n",
    "        # ------------------------- 2, deep feature extraction ------------------------- #\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=embed_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # merge non-overlapping patches into image\n",
    "        self.patch_unembed = PatchUnEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=embed_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,\n",
    "        )\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, num_patches, embed_dim)\n",
    "            )\n",
    "            trunc_normal_(self.absolute_pos_embed, std=0.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "\n",
    "        # build Residual Hybrid Attention Groups (RHAG)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = RHAG(\n",
    "                dim=embed_dim,\n",
    "                input_resolution=(patches_resolution[0], patches_resolution[1]),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                compress_ratio=compress_ratio,\n",
    "                squeeze_factor=squeeze_factor,\n",
    "                conv_scale=conv_scale,\n",
    "                overlap_ratio=overlap_ratio,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[\n",
    "                    sum(depths[:i_layer]) : sum(depths[: i_layer + 1])\n",
    "                ],  # no impact on SR results\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                resi_connection=resi_connection,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "\n",
    "        # build the last conv layer in deep feature extraction\n",
    "        if resi_connection == \"1conv\":\n",
    "            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n",
    "        elif resi_connection == \"identity\":\n",
    "            self.conv_after_body = nn.Identity()\n",
    "\n",
    "        # ------------------------- 3, high quality image reconstruction ------------------------- #\n",
    "        if self.upsampler == \"pixelshuffle\":\n",
    "            # for classical SR\n",
    "            self.conv_before_upsample = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, num_feat, 3, 1, 1), nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            self.upsample = Upsample(upscale, num_feat)\n",
    "            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def calculate_rpi_sa(self):\n",
    "        # calculate relative position index for SA\n",
    "        coords_h = torch.arange(self.window_size)\n",
    "        coords_w = torch.arange(self.window_size)\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = (\n",
    "            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        )  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0\n",
    "        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        return relative_position_index\n",
    "\n",
    "    def calculate_rpi_oca(self):\n",
    "        # calculate relative position index for OCA\n",
    "        window_size_ori = self.window_size\n",
    "        window_size_ext = self.window_size + int(self.overlap_ratio * self.window_size)\n",
    "\n",
    "        coords_h = torch.arange(window_size_ori)\n",
    "        coords_w = torch.arange(window_size_ori)\n",
    "        coords_ori = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, ws, ws\n",
    "        coords_ori_flatten = torch.flatten(coords_ori, 1)  # 2, ws*ws\n",
    "\n",
    "        coords_h = torch.arange(window_size_ext)\n",
    "        coords_w = torch.arange(window_size_ext)\n",
    "        coords_ext = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, wse, wse\n",
    "        coords_ext_flatten = torch.flatten(coords_ext, 1)  # 2, wse*wse\n",
    "\n",
    "        relative_coords = (\n",
    "            coords_ext_flatten[:, None, :] - coords_ori_flatten[:, :, None]\n",
    "        )  # 2, ws*ws, wse*wse\n",
    "\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0\n",
    "        ).contiguous()  # ws*ws, wse*wse, 2\n",
    "        relative_coords[:, :, 0] += (\n",
    "            window_size_ori - window_size_ext + 1\n",
    "        )  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += window_size_ori - window_size_ext + 1\n",
    "\n",
    "        relative_coords[:, :, 0] *= window_size_ori + window_size_ext - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        return relative_position_index\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        # calculate attention mask for SW-MSA\n",
    "        h, w = x_size\n",
    "        img_mask = torch.zeros((1, h, w, 1))  # 1 h w 1\n",
    "        h_slices = (\n",
    "            slice(0, -self.window_size),\n",
    "            slice(-self.window_size, -self.shift_size),\n",
    "            slice(-self.shift_size, None),\n",
    "        )\n",
    "        w_slices = (\n",
    "            slice(0, -self.window_size),\n",
    "            slice(-self.window_size, -self.shift_size),\n",
    "            slice(-self.shift_size, None),\n",
    "        )\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(\n",
    "            img_mask, self.window_size\n",
    "        )  # nw, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(\n",
    "            attn_mask == 0, float(0.0)\n",
    "        )\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"absolute_pos_embed\"}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"relative_position_bias_table\"}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x_size = (x.shape[2], x.shape[3])\n",
    "\n",
    "        # Calculate attention mask and relative position index in advance to speed up inference.\n",
    "        # The original code is very time-cosuming for large window size.\n",
    "        attn_mask = self.calculate_mask(x_size).to(x.device)\n",
    "        params = {\n",
    "            \"attn_mask\": attn_mask,\n",
    "            \"rpi_sa\": self.relative_position_index_SA,\n",
    "            \"rpi_oca\": self.relative_position_index_OCA,\n",
    "        }\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x_size, params)\n",
    "\n",
    "        x = self.norm(x)  # b seq_len c\n",
    "        x = self.patch_unembed(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mean = self.mean.type_as(x)\n",
    "        x = (x - self.mean) * self.img_range\n",
    "\n",
    "        if self.upsampler == \"pixelshuffle\":\n",
    "            # for classical SR\n",
    "            x = self.conv_first(x)\n",
    "            x = self.conv_after_body(self.forward_features(x)) + x\n",
    "            x = self.conv_before_upsample(x)\n",
    "            x = self.conv_last(self.upsample(x))\n",
    "\n",
    "        x = x / self.img_range + self.mean\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# initialize model\n",
    "model = HAT(\n",
    "    img_size=64,\n",
    "    upsampler=\"pixelshuffle\",\n",
    "    window_size=8,\n",
    "    img_range=1.0,\n",
    "    mean=0.0,\n",
    "    overlap_ratio=0.5,\n",
    "    shift_size=4,\n",
    "    depth=8,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=0.05,\n",
    "    attn_drop_rate=0.0,\n",
    "    drop_path_rate=0.1,\n",
    "    ape=False,\n",
    "    patch_norm=True,\n",
    "    use_checkpoint=False,\n",
    "    depths=(4, 4, 4, 4),\n",
    "    num_heads=(4, 4, 4, 4),\n",
    "    upscale=4,\n",
    ")\n",
    "\n",
    "# print trainable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eishbiURr-Kb",
    "outputId": "392967d2-12fb-4888-9ecf-836badac6f40"
   },
   "outputs": [],
   "source": [
    "# get datasets\n",
    "!unzip /content/drive/MyDrive/super_resolution_256_128.zip\n",
    "!unzip /content/drive/MyDrive/super_resolution_256_128_hysto.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "joc8JmssrnQ6",
    "outputId": "11d9904c-578a-47ff-f869-e568216e2622"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set logging to remove unnecessary information\n",
    "def set_tf_loglevel(level):\n",
    "    if level >= logging.FATAL:\n",
    "        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "    if level >= logging.ERROR:\n",
    "        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "    if level >= logging.WARNING:\n",
    "        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "    else:\n",
    "        os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "    logging.getLogger(\"tensorflow\").setLevel(level)\n",
    "\n",
    "\n",
    "set_tf_loglevel(logging.FATAL)  # suppress warnings etc.\n",
    "\n",
    "# train and test paths\n",
    "paths = {\n",
    "    \"test_x\": \"/content/super_resolution_256_128/SR_TEST/128x128\",\n",
    "    \"test_y\": \"/content/super_resolution_256_128/SR_TEST/256x256\",\n",
    "    \"test_x_wq\": \"/content/super_resolution_256_128/SR_TEST/128x128_wq\",\n",
    "    \"train_x\": \"/content/super_resolution_256_128/SR_TRAIN/128x128\",\n",
    "    \"train_y\": \"/content/super_resolution_256_128/SR_TRAIN/256x256\",\n",
    "    \"train_x_wq\": \"/content/super_resolution_256_128/SR_TRAIN/128x128_wq\",\n",
    "}\n",
    "\n",
    "\"\"\" paths = {\n",
    "    'test_x': '/content/super_resolution_256_128_hysto/SR_TEST/128x128',\n",
    "    'test_y': '/content/super_resolution_256_128_hysto/SR_TEST/256x256',\n",
    "    'test_x_wq': '/content/super_resolution_256_128_hysto/SR_TEST/128x128_wq',\n",
    "    'train_x': '/content/super_resolution_256_128_hysto/SR_TRAIN/128x128',\n",
    "    'train_y': '/content/super_resolution_256_128_hysto/SR_TRAIN/256x256',\n",
    "    'train_x_wq': '/content/super_resolution_256_128_hysto/SR_TRAIN/128x128_wq',\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# data generator for super resolution\n",
    "class SRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        paths,\n",
    "        size_x=(64, 64),\n",
    "        size_y=(256, 256),\n",
    "        info=True,\n",
    "        train=True,\n",
    "        use_batch=False,\n",
    "        restorate=False,\n",
    "        resto_model=None,\n",
    "        resto_model_tiling=None,\n",
    "        tiling=False,\n",
    "        save_path=\"./resto/\",\n",
    "        use_wq=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if restorate:\n",
    "            import tensorflow as tf\n",
    "\n",
    "            print(\n",
    "                \"Restoration may introduce unwanted artifacts, takes longer and may produce unnatural results. Use with caution.\"\n",
    "            ) if info else None\n",
    "        self.use_wq = use_wq\n",
    "        self.resto_model = (\n",
    "            None\n",
    "            if not restorate or resto_model is None\n",
    "            else tf.keras.models.load_model(resto_model)\n",
    "        )\n",
    "        self.resto_model_tiling = (\n",
    "            None\n",
    "            if not restorate or resto_model_tiling is None\n",
    "            else tf.keras.models.load_model(resto_model_tiling)\n",
    "        )\n",
    "        self.tiling = tiling # if restoration by tiling is wanted\n",
    "        self.save_path = save_path\n",
    "        self.restorate = restorate # if restoration is wanted\n",
    "        self.use_batch = use_batch\n",
    "        self.info = info # if info should be printed\n",
    "        self.size_x = size_x\n",
    "        self.size_y = size_y\n",
    "        self.image_paths_y = (\n",
    "            self._get_image_paths(paths[\"train_y\"])\n",
    "            if train\n",
    "            else self._get_image_paths(paths[\"test_y\"])\n",
    "        )\n",
    "        self.image_paths_x = (\n",
    "            self._get_image_paths(paths[\"train_x\"])\n",
    "            if train and not use_wq\n",
    "            else self._get_image_paths(paths[\"train_x_wq\"])\n",
    "            if train\n",
    "            else self._get_image_paths(paths[\"test_x\"])\n",
    "            if not use_wq\n",
    "            else self._get_image_paths(paths[\"test_x_wq\"])\n",
    "        )\n",
    "        self.num_samples = len(self.image_paths_x)\n",
    "        self.indexes = np.arange(self.num_samples)\n",
    "\n",
    "    # get the paths of all images\n",
    "    def _get_image_paths(self, directory):\n",
    "        image_paths = []\n",
    "        for filename in os.listdir(directory):\n",
    "            image_paths.append(os.path.join(directory, filename))\n",
    "        print(f\"Found {len(image_paths)} images in {directory}\") if self.info else None\n",
    "        return image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index, save_images=True):\n",
    "        # load images\n",
    "        image_x = self._load_image(self.image_paths_x[index])\n",
    "        image_y = self._load_image(self.image_paths_y[index], batch=\"y\")\n",
    "\n",
    "        # normalize images\n",
    "        image_x = image_x / 255.0\n",
    "        image_y = image_y / 255.0\n",
    "\n",
    "        # apply restoration if wanted\n",
    "        if self.restorate:\n",
    "            print(\"resto in:\", image_x.shape) if self.info else None\n",
    "            image_y = self._apply_restoration(\n",
    "                image_y, self.resto_model, self.resto_model_tiling, self.tiling\n",
    "            )\n",
    "            print(\"returned resto:\", image_y.shape) if self.info else None\n",
    "            if save_images:\n",
    "                image_y_s = np.transpose(image_y, (1, 2, 0))\n",
    "                image_y_s = image_y_s * 255.0\n",
    "                image_y_s = image_y_s.astype(np.uint8)\n",
    "                image_y_s = Image.fromarray(image_y_s)\n",
    "                os.makedirs(self.save_path, exist_ok=True)\n",
    "                image_y_s.save(\n",
    "                    f\"./{self.save_path}/restorated_{self.image_paths_x[index].split('/')[-1]}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"saved restorated_{self.image_paths_x[index].split('/')[-1]}\"\n",
    "                ) if self.info else None\n",
    "            return torch.from_numpy(image_x).float(), torch.from_numpy(image_y).float()\n",
    "\n",
    "        # return images\n",
    "        return torch.from_numpy(image_x).float(), torch.from_numpy(image_y).float()\n",
    "\n",
    "    def _apply_restoration(\n",
    "        self, image, model_normal, model_tiling, use_tiling=False\n",
    "    ):  # non tiling for 1 pred on 256x256, tiling for splitting 256x256 into 4 128x128 and then merging\n",
    "        # for worst case only as it may produce unnaturally looking images !!!\n",
    "        # not working with torch.utils.data.DataLoader() as a tf model is used\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        print(\"restoration in:\", image.shape) if self.info else None\n",
    "        if not use_tiling:\n",
    "            restorated = model_normal.predict(image)\n",
    "            restorated = np.squeeze(restorated, axis=0)\n",
    "            restorated = np.transpose(restorated, (2, 0, 1))\n",
    "            restorated = (restorated - np.min(restorated)) / (\n",
    "                np.max(restorated) - np.min(restorated)\n",
    "            )\n",
    "            print(\n",
    "                \"restored shape:\",\n",
    "                restorated.shape,\n",
    "                \"restored min/max\",\n",
    "                np.min(restorated),\n",
    "                np.max(restorated),\n",
    "            ) if self.info else None\n",
    "        else:\n",
    "            # tiling\n",
    "            print(\"Tiling restoration\") if self.info else None\n",
    "            patches = []\n",
    "            for i in range(0, image.shape[1], 128):\n",
    "                for j in range(0, image.shape[2], 128):\n",
    "                    patch = image[:, i : i + 128, j : j + 128, :]\n",
    "                    patches.append(patch)\n",
    "            patches = np.array(patches)\n",
    "            print(\"patches shape:\", patches.shape) if self.info else None\n",
    "            restored_patches = []\n",
    "            for patch in patches:\n",
    "                restored_patch = model_tiling.predict(patch)\n",
    "                restored_patches.append(restored_patch)\n",
    "            restored_patches = np.array(restored_patches)\n",
    "            print(\n",
    "                \"restored patches shape:\", restored_patches.shape\n",
    "            ) if self.info else None\n",
    "            restorated = np.zeros((image.shape[1], image.shape[2], 3))\n",
    "            for i in range(0, image.shape[1], 128):\n",
    "                for j in range(0, image.shape[2], 128):\n",
    "                    patch = restored_patches[0]\n",
    "                    restorated[i : i + 128, j : j + 128, :] = patch\n",
    "                    restored_patches = restored_patches[1:]\n",
    "            restorated = np.transpose(restorated, (2, 0, 1))\n",
    "            restorated = (restorated - np.min(restorated)) / (\n",
    "                np.max(restorated) - np.min(restorated)\n",
    "            )\n",
    "        return restorated\n",
    "\n",
    "    # load image and convert to numpy array in the right format\n",
    "    def _load_image(self, path, batch=\"x\"):\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = image.resize(\n",
    "            self.size_x if batch == \"x\" else self.size_y, resample=Image.BICUBIC\n",
    "        )\n",
    "        image = np.array(image)\n",
    "        print(image.shape) if self.info else None\n",
    "        if self.use_batch:\n",
    "            image = np.transpose(image, (0, 3, 1, 2))\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        print(\"_load_image out:\", image.shape) if self.info else None\n",
    "        return image\n",
    "\n",
    "# normal generator without further restoration\n",
    "train_data = SRDataset(\n",
    "    paths,\n",
    "    train=True,\n",
    "    info=False,\n",
    "    restorate=False,\n",
    "    resto_model=\"/content/super_resolution/super resolution/hat_pt/NafNet_5M4F_256\",\n",
    "    tiling=True,\n",
    "    resto_model_tiling=\"/content/super_resolution/super resolution/hat_pt/NafNet_8M_128\",\n",
    "    use_wq=False,\n",
    ")\n",
    "test_data = SRDataset(\n",
    "    paths,\n",
    "    train=False,\n",
    "    info=False,\n",
    "    restorate=False,\n",
    "    resto_model=\"/content/super_resolution/super resolution/hat_pt/NafNet_5M4F_256\",\n",
    "    use_wq=False,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "# set parameters for data generator\n",
    "gen_params_train = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "gen_params_test = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 0,\n",
    "}\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(\n",
    "    train_data, **gen_params_train\n",
    ")  # to batchable python generator\n",
    "test_generator = torch.utils.data.DataLoader(test_data, **gen_params_test)\n",
    "\n",
    "\n",
    "# display test images and further information\n",
    "def display_images(generator, amount=7, info=True):\n",
    "    fig, axes = plt.subplots(nrows=amount, ncols=2, figsize=(7.5, 21.5))\n",
    "    for i in range(amount):\n",
    "        x, y = generator[i]\n",
    "        x, y = np.transpose(x, (1, 2, 0)), np.transpose(y, (1, 2, 0))\n",
    "        if x.shape[-1] == 1:\n",
    "            x = np.repeat(x, 3, axis=-1)\n",
    "        if y.shape[-1] == 1:\n",
    "            y = np.repeat(y, 3, axis=-1)\n",
    "        axes[i, 0].imshow(x)\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        if info:\n",
    "            axes[i, 0].set_title(f\"Low Res\\nMin: {x.min():.2f}, Max: {x.max():.2f}\")\n",
    "        axes[i, 1].imshow(y)\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        if info:\n",
    "            axes[i, 1].set_title(f\"High Res\\nMin: {y.min():.2f}, Max: {y.max():.2f}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_images(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHjaL362swdY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from math import exp\n",
    "from torchgeometry.image import get_gaussian_kernel2d\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# model optimizer\n",
    "class Lion(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-4,\n",
    "        betas: tuple = (0.9, 0.99),\n",
    "        weight_decay: float = 0.0,\n",
    "    ):\n",
    "        assert lr > 0.0\n",
    "        assert all([0.0 <= beta <= 1.0 for beta in betas])\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in filter(lambda p: p.grad is not None, group[\"params\"]):\n",
    "                # parameter\n",
    "                grad, lr, wd, beta1, beta2, state = (\n",
    "                    p.grad,\n",
    "                    group[\"lr\"],\n",
    "                    group[\"weight_decay\"],\n",
    "                    *group[\"betas\"],\n",
    "                    self.state[p],\n",
    "                )\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                exp_avg = state[\"exp_avg\"]\n",
    "\n",
    "                # Lion optimizer\n",
    "                p.data.mul_(1 - lr * wd)\n",
    "                update = exp_avg.clone().lerp_(grad, 1 - beta1)\n",
    "                p.add_(torch.sign(update), alpha=-lr)\n",
    "                exp_avg.lerp_(grad, 1 - beta2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "#! https://github.com/styler00dollar/pytorch-loss-functions; @styler00dollar\n",
    "\n",
    "\n",
    "def _binarize(y_data, threshold):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        y_data : [float] 4-d tensor in [batch_size, channels, img_rows, img_cols]\n",
    "        threshold : [float] [0.0, 1.0]\n",
    "    return 4-d binarized y_data\n",
    "    \"\"\"\n",
    "    y_data[y_data < threshold] = 0.0\n",
    "    y_data[y_data >= threshold] = 1.0\n",
    "    return y_data\n",
    "\n",
    "# metrics for super resolution\n",
    "class PSNRMetric(object):\n",
    "    def __init__(self, des=\"Peak Signal to Noise Ratio\"):\n",
    "        self.des = des\n",
    "        self.metric_value = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"PSNR\"\n",
    "\n",
    "    def __call__(self, y_pred, y_true, dim=1, threshold=None):\n",
    "        if threshold:\n",
    "            y_pred = _binarize(y_pred, threshold)\n",
    "        mse = torch.mean((y_pred - y_true) ** 2)\n",
    "        self.metric_value = 10 * torch.log10(1 / mse)\n",
    "        return self.metric_value\n",
    "\n",
    "    def value(self):\n",
    "        return self.metric_value\n",
    "\n",
    "\n",
    "class SSIMMetric(object):\n",
    "    def __init__(self, des=\"structural similarity index\"):\n",
    "        self.des = des\n",
    "        self.metric_value = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"SSIM\"\n",
    "\n",
    "    def gaussian(self, w_size, sigma):\n",
    "        gauss = torch.Tensor(\n",
    "            [\n",
    "                math.exp(-((x - w_size // 2) ** 2) / float(2 * sigma**2))\n",
    "                for x in range(w_size)\n",
    "            ]\n",
    "        )\n",
    "        return gauss / gauss.sum()\n",
    "\n",
    "    def create_window(self, w_size, channel=1):\n",
    "        _1D_window = self.gaussian(w_size, 1.5).unsqueeze(1)\n",
    "        _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "        window = _2D_window.expand(channel, 1, w_size, w_size).contiguous()\n",
    "        return window\n",
    "\n",
    "    def __call__(self, y_pred, y_true, w_size=11, size_average=True, full=False):\n",
    "        if torch.max(y_pred) > 128:\n",
    "            max_val = 255\n",
    "        else:\n",
    "            max_val = 1\n",
    "\n",
    "        if torch.min(y_pred) < -0.5:\n",
    "            min_val = -1\n",
    "        else:\n",
    "            min_val = 0\n",
    "        L = max_val - min_val\n",
    "\n",
    "        padd = 0\n",
    "        (_, channel, height, width) = y_pred.size()\n",
    "        window = self.create_window(w_size, channel=channel).to(y_pred.device)\n",
    "\n",
    "        mu1 = F.conv2d(y_pred, window, padding=padd, groups=channel)\n",
    "        mu2 = F.conv2d(y_true, window, padding=padd, groups=channel)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        sigma1_sq = (\n",
    "            F.conv2d(y_pred * y_pred, window, padding=padd, groups=channel) - mu1_sq\n",
    "        )\n",
    "        sigma2_sq = (\n",
    "            F.conv2d(y_true * y_true, window, padding=padd, groups=channel) - mu2_sq\n",
    "        )\n",
    "        sigma12 = (\n",
    "            F.conv2d(y_pred * y_true, window, padding=padd, groups=channel) - mu1_mu2\n",
    "        )\n",
    "\n",
    "        C1 = (0.01 * L) ** 2\n",
    "        C2 = (0.03 * L) ** 2\n",
    "\n",
    "        v1 = 2.0 * sigma12 + C2\n",
    "        v2 = sigma1_sq + sigma2_sq + C2\n",
    "        cs = torch.mean(v1 / v2)\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "        if size_average:\n",
    "            ret = ssim_map.mean()\n",
    "        else:\n",
    "            ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "        self.metric_value = ret\n",
    "        if full:\n",
    "            return ret, cs\n",
    "        return ret\n",
    "\n",
    "    def value(self):\n",
    "        return self.metric_value\n",
    "\n",
    "\n",
    "class SSIM(nn.Module):\n",
    "    r\"\"\"Creates a criterion that measures the Structural Similarity (SSIM)\n",
    "    index between each element in the input `x` and target `y`.\n",
    "\n",
    "    The index can be described as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "      \\text{SSIM}(x, y) = \\frac{(2\\mu_x\\mu_y+c_1)(2\\sigma_{xy}+c_2)}\n",
    "      {(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2+\\sigma_y^2+c_2)}\n",
    "\n",
    "    where:\n",
    "      - :math:`c_1=(k_1 L)^2` and :math:`c_2=(k_2 L)^2` are two variables to\n",
    "        stabilize the division with weak denominator.\n",
    "      - :math:`L` is the dynamic range of the pixel-values (typically this is\n",
    "        :math:`2^{\\#\\text{bits per pixel}}-1`).\n",
    "\n",
    "    the loss, or the Structural dissimilarity (DSSIM) can be finally described\n",
    "    as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "      \\text{loss}(x, y) = \\frac{1 - \\text{SSIM}(x, y)}{2}\n",
    "\n",
    "    Arguments:\n",
    "        window_size (int): the size of the kernel.\n",
    "        max_val (float): the dynamic range of the images. Default: 1.\n",
    "        reduction (str, optional): Specifies the reduction to apply to the\n",
    "         output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
    "         'mean': the sum of the output will be divided by the number of elements\n",
    "         in the output, 'sum': the output will be summed. Default: 'none'.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: the ssim index.\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(B, C, H, W)`\n",
    "        - Target :math:`(B, C, H, W)`\n",
    "        - Output: scale, if reduction is 'none', then :math:`(B, C, H, W)`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> input1 = torch.rand(1, 4, 5, 5)\n",
    "        >>> input2 = torch.rand(1, 4, 5, 5)\n",
    "        >>> ssim = tgm.losses.SSIM(5, reduction='none')\n",
    "        >>> loss = ssim(input1, input2)  # 1x4x5x5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, window_size: int, reduction: str = \"none\", max_val: float = 1.0\n",
    "    ) -> None:\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size: int = window_size\n",
    "        self.max_val: float = max_val\n",
    "        self.reduction: str = reduction\n",
    "\n",
    "        self.window: torch.Tensor = get_gaussian_kernel2d(\n",
    "            (window_size, window_size), (1.5, 1.5)\n",
    "        )\n",
    "        self.padding: int = self.compute_zero_padding(window_size)\n",
    "\n",
    "        self.C1: float = (0.01 * self.max_val) ** 2\n",
    "        self.C2: float = (0.03 * self.max_val) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_zero_padding(kernel_size: int) -> int:\n",
    "        \"\"\"Computes zero padding.\"\"\"\n",
    "        return (kernel_size - 1) // 2\n",
    "\n",
    "    def filter2D(\n",
    "        self, input: torch.Tensor, kernel: torch.Tensor, channel: int\n",
    "    ) -> torch.Tensor:\n",
    "        return F.conv2d(input, kernel, padding=self.padding, groups=channel)\n",
    "\n",
    "    def forward(self, img1: torch.Tensor, img2: torch.Tensor) -> torch.Tensor:\n",
    "        if not torch.is_tensor(img1):\n",
    "            raise TypeError(\n",
    "                \"Input img1 type is not a torch.Tensor. Got {}\".format(type(img1))\n",
    "            )\n",
    "        if not torch.is_tensor(img2):\n",
    "            raise TypeError(\n",
    "                \"Input img2 type is not a torch.Tensor. Got {}\".format(type(img2))\n",
    "            )\n",
    "        if not len(img1.shape) == 4:\n",
    "            raise ValueError(\n",
    "                \"Invalid img1 shape, we expect BxCxHxW. Got: {}\".format(img1.shape)\n",
    "            )\n",
    "        if not len(img2.shape) == 4:\n",
    "            raise ValueError(\n",
    "                \"Invalid img2 shape, we expect BxCxHxW. Got: {}\".format(img2.shape)\n",
    "            )\n",
    "        if not img1.shape == img2.shape:\n",
    "            raise ValueError(\n",
    "                \"img1 and img2 shapes must be the same. Got: {}\".format(\n",
    "                    img1.shape, img2.shape\n",
    "                )\n",
    "            )\n",
    "        if not img1.device == img2.device:\n",
    "            raise ValueError(\n",
    "                \"img1 and img2 must be in the same device. Got: {}\".format(\n",
    "                    img1.device, img2.device\n",
    "                )\n",
    "            )\n",
    "        if not img1.dtype == img2.dtype:\n",
    "            raise ValueError(\n",
    "                \"img1 and img2 must be in the same dtype. Got: {}\".format(\n",
    "                    img1.dtype, img2.dtype\n",
    "                )\n",
    "            )\n",
    "        # prepare kernel\n",
    "        b, c, h, w = img1.shape\n",
    "        tmp_kernel: torch.Tensor = self.window.to(img1.device).to(img1.dtype)\n",
    "        kernel: torch.Tensor = tmp_kernel.repeat(c, 1, 1, 1)\n",
    "\n",
    "        # compute local mean per channel\n",
    "        mu1: torch.Tensor = self.filter2D(img1, kernel, c)\n",
    "        mu2: torch.Tensor = self.filter2D(img2, kernel, c)\n",
    "\n",
    "        mu1_sq = mu1.pow(2)\n",
    "        mu2_sq = mu2.pow(2)\n",
    "        mu1_mu2 = mu1 * mu2\n",
    "\n",
    "        # compute local sigma per channel\n",
    "        sigma1_sq = self.filter2D(img1 * img1, kernel, c) - mu1_sq\n",
    "        sigma2_sq = self.filter2D(img2 * img2, kernel, c) - mu2_sq\n",
    "        sigma12 = self.filter2D(img1 * img2, kernel, c) - mu1_mu2\n",
    "\n",
    "        ssim_map = ((2 * mu1_mu2 + self.C1) * (2 * sigma12 + self.C2)) / (\n",
    "            (mu1_sq + mu2_sq + self.C1) * (sigma1_sq + sigma2_sq + self.C2)\n",
    "        )\n",
    "\n",
    "        loss = torch.clamp(1.0 - ssim_map, min=0, max=1) / 2.0\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = torch.mean(loss)\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = torch.sum(loss)\n",
    "        elif self.reduction == \"none\":\n",
    "            pass\n",
    "        return loss\n",
    "\n",
    "# loss functions\n",
    "def ssim_loss(\n",
    "    img1: torch.Tensor,\n",
    "    img2: torch.Tensor,\n",
    "    window_size: int = 11,\n",
    "    reduction: str = \"mean\",\n",
    "    max_val: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Function that measures the Structural Similarity (SSIM) index between\n",
    "    each element in the input `x` and target `y`.\n",
    "\n",
    "    See :class:`torchgeometry.losses.SSIM` for details.\n",
    "    \"\"\"\n",
    "    return SSIM(window_size, reduction, max_val)(img1, img2).mean()\n",
    "\n",
    "\n",
    "def smooth_l1(\n",
    "    input,\n",
    "    target,\n",
    "    size_average=None,\n",
    "    reduce=None,\n",
    "    reduction: str = \"mean\",\n",
    "    beta: float = 1.0,\n",
    "):\n",
    "    return F.smooth_l1_loss(input, target, size_average, reduce, reduction, beta)\n",
    "\n",
    "\n",
    "# combined loss to get best outcomes for super resolution\n",
    "# benefits of l1 and ssim combined\n",
    "class CombinedLoss(torch.nn.Module):\n",
    "    def __init__(self, counter_weight: float = 0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.counter_weight = counter_weight # weigh one loss more than the other\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        ssim_loss_value = ssim_loss(preds, target)\n",
    "        l1_loss_value = smooth_l1(preds, target)\n",
    "\n",
    "        combined_loss = ssim_loss_value * self.counter_weight + l1_loss_value * (\n",
    "            1 - self.counter_weight\n",
    "        )\n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-mvFccqHQHE",
    "outputId": "152e103e-fbaf-44f3-f532-e1af8b67df3b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# save training progress to csv\n",
    "def write_to_csv(\n",
    "    loss,\n",
    "    metrics,\n",
    "    epoch,\n",
    "    header_written=False,\n",
    "    filename: str = f\"train_{datetime.date.today()}.csv\",\n",
    "):\n",
    "    with open(filename, \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not header_written:\n",
    "            writer.writerow([\"epoch\", \"loss\", \"PSNR\", \"SSIM\"])\n",
    "            header_written = True\n",
    "        writer.writerow([epoch + 1, loss, metrics[\"PSNR\"], metrics[\"SSIM\"]])\n",
    "    return header_written\n",
    "\n",
    "# train step function\n",
    "def train_step(\n",
    "    model,\n",
    "    train_generator,\n",
    "    metrics,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    save_path=\"model_epoch_\",\n",
    "    epochs: int = 10,\n",
    "    save: bool = False,\n",
    "    test_after_epoch: bool = False,\n",
    "    test_generator= None,\n",
    "    load_saved_model: bool = False,\n",
    "    saved_model_path: str = None,\n",
    "):\n",
    "    if load_saved_model:\n",
    "        model.load_state_dict(torch.load(saved_model_path))\n",
    "    model.train()\n",
    "    total_loss = 0 # for loss calculation\n",
    "    total_samples = 0 # for loss calculation\n",
    "    start = time.time() # time measurement\n",
    "    num_batches = len(train_generator)\n",
    "    loss_checker = [] # \"lr scheduler\"\n",
    "    loss_down = 0 # if loss increases 6 times in a row, training is stopped\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch = 0\n",
    "        total_loss_epoch = 0\n",
    "        total_samples_epoch = 0\n",
    "        average_metrics = []\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\\n-------------------------------\")\n",
    "        for x, y in train_generator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch += 1\n",
    "            if len(loss_checker) > 1:\n",
    "                if loss_down < 6:\n",
    "                    # decrease learning rate if loss increases\n",
    "                    if loss_checker[-1] > loss_checker[-2]:\n",
    "                        print(\n",
    "                            f\"Loss increased from {loss_checker[-2]} to {loss_checker[-1]}\\n\",\n",
    "                            end=\"\\n\",\n",
    "                            flush=True,\n",
    "                        )\n",
    "                        optimizer.param_groups[0][\"lr\"] *= 0.1\n",
    "                        loss_down += 1\n",
    "                else:\n",
    "                    raise Exception(\"Loss increased too many times\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            total_loss_epoch += loss.item() * batch_size\n",
    "            total_samples_epoch += batch_size\n",
    "\n",
    "            for metric in metrics.values():\n",
    "                metric(outputs, y)\n",
    "\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_loss_epoch = total_loss_epoch / total_samples_epoch\n",
    "            metric_values = {\n",
    "                metric_name: metric.value().item()\n",
    "                for metric_name, metric in metrics.items()\n",
    "            }\n",
    "            # round metric values to 4 digits\n",
    "            metric_values = {\n",
    "                metric_name: round(metric_value, 4)\n",
    "                for metric_name, metric_value in metric_values.items()\n",
    "            }\n",
    "            average_metrics.append(metric_values)\n",
    "\n",
    "            # time measurement\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start\n",
    "            avg_time_per_batch = elapsed_time / batch\n",
    "            remainig_batches = num_batches - batch\n",
    "            remaining_time = remainig_batches * avg_time_per_batch\n",
    "            print(\n",
    "                f\"\\r Batch [{batch}/{num_batches}] Loss: {avg_loss:.4f} Metrics: {metric_values} Elapsed Time: {elapsed_time:.2f}s/{elapsed_time/60:.2f}m Remaining Time (epoch): {remaining_time:.2f}s/{remaining_time/60:.2f}m\",\n",
    "                flush=True,\n",
    "                end=\" \",\n",
    "            )\n",
    "            header_written = write_to_csv(\n",
    "                avg_loss, metric_values, epoch, header_written if batch > 1 else False\n",
    "            )\n",
    "\n",
    "        # calculate average loss and metric values\n",
    "        avg_loss = total_loss / total_samples\n",
    "        avg_loss_epoch = total_loss_epoch / total_samples_epoch\n",
    "        print(avg_loss)\n",
    "        avg_metrics = {\n",
    "            metric_name: sum([metric[metric_name] for metric in average_metrics])\n",
    "            / len(average_metrics)\n",
    "            for metric_name in average_metrics[0].keys()\n",
    "        }\n",
    "        avg_metrics = {\n",
    "            metric_name: round(metric_value, 4)\n",
    "            for metric_name, metric_value in avg_metrics.items()\n",
    "        }\n",
    "        # save loss for \"lr scheduler\"\n",
    "        loss_checker.append(avg_loss)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} finished. \\n Avg Loss: {avg_loss_epoch:.4f} Avg Metrics: {avg_metrics}\"\n",
    "        )\n",
    "        if save:\n",
    "            torch.save(model.state_dict(), f\"{save_path}{epoch + 1}.pth\")\n",
    "            print(f\"Model state dict saved at {save_path}{epoch + 1}.pth\")\n",
    "            torch.save(model, f\"{save_path}{epoch + 1}_cm.pth\")\n",
    "            print(f\"Complete model saved at {save_path}{epoch + 1}_cm.pth\")\n",
    "\n",
    "        # test the model after each epoch\n",
    "        if test_after_epoch:\n",
    "            test_step(model, test_generator, metrics, loss_fn, device)\n",
    "\n",
    "# test step function\n",
    "def test_step(\n",
    "    model,\n",
    "    test_generator,\n",
    "    metrics,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    load_saved_model: bool = False,\n",
    "    saved_model_path: str = None,\n",
    "):\n",
    "    if load_saved_model:\n",
    "        model.load_state_dict(torch.load(saved_model_path))\n",
    "    # set model to evaluation mode and initialize loss and metric\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    avg_metrics = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_generator:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            batch_size = x.size(0)\n",
    "\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            for metric in metrics.values():\n",
    "                metric(outputs, y)\n",
    "\n",
    "            metric_values = {\n",
    "                metric_name: metric.value().item()\n",
    "                for metric_name, metric in metrics.items()\n",
    "            }\n",
    "            # round metric values to 4 digits\n",
    "            metric_values = {\n",
    "                metric_name: round(metric_value, 4)\n",
    "                for metric_name, metric_value in metric_values.items()\n",
    "            }\n",
    "            avg_metrics.append(metric_values)\n",
    "\n",
    "    # calculate average loss and metric values\n",
    "    avg_metrics = {\n",
    "        metric_name: sum([metric[metric_name] for metric in avg_metrics])\n",
    "        / len(avg_metrics)\n",
    "        for metric_name in avg_metrics[0].keys()\n",
    "    }\n",
    "    avg_metrics = {\n",
    "        metric_name: round(metric_value, 4)\n",
    "        for metric_name, metric_value in avg_metrics.items()\n",
    "    }\n",
    "    avg_loss = total_loss / total_samples\n",
    "    print(f\"Test - Avg Loss (Batch): {avg_loss:.4f} Avg Metrics: {avg_metrics}\")\n",
    "    write_to_csv(\n",
    "        avg_loss,\n",
    "        metric_values,\n",
    "        0,\n",
    "        header_written=False,\n",
    "        filename=f\"test_{datetime.date.today()}.csv\",\n",
    "    )\n",
    "\n",
    "\n",
    "# model loss and optimizer\n",
    "optimizer = Lion(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "# loss_fn = torch.nn.MSELoss()\n",
    "loss_fn = CombinedLoss(counter_weight=0.6)\n",
    "num_epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# metrics for super resolution\n",
    "metrics = {\"PSNR\": PSNRMetric(), \"SSIM\": SSIMMetric()}\n",
    "\n",
    "# train and test the model\n",
    "train_step(\n",
    "    model,\n",
    "    train_generator,\n",
    "    metrics,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    save=True,\n",
    "    save_path=\"model_epoch_\",\n",
    "    epochs=num_epochs,\n",
    "    test_after_epoch=True,\n",
    "    test_generator=test_generator,\n",
    "    saved_model_path=\"/content/drive/MyDrive/model_epoch_1.pth\", # load saved model\n",
    "    load_saved_model=False, # load saved model\n",
    ")\n",
    "# test_step(model, test_data, metrics, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xPAswswxJ-9X",
    "outputId": "f91fdcb2-b390-455f-bb1b-1ac070d6308f"
   },
   "outputs": [],
   "source": [
    "# move model to drive\n",
    "import shutil\n",
    "\n",
    "shutil.copy(\"model_epoch_2.pth\", \"/content/drive/MyDrive/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
