{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qp52fTBUfRT",
    "outputId": "6156fcf7-e6f1-4f8b-83b7-1eca89277ff3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# mount google drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wv7Tf8G5UlSe",
    "outputId": "a5292eec-421e-446f-e934-4782de357ab2"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "#!wget \"https://zenodo.org/record/3632035/files/ICIAR2018_BACH_Challenge.zip?download=1\"\n",
    "#!wget \"https://zenodo.org/record/3632035/files/ICIAR2018_BACH_Challenge_TestDataset.zip?download=1\"\n",
    "!unzip \"/content/drive/MyDrive/ICIAR2018_BACH.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bFMfhcQU8S6"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# metrics\n",
    "def auc(y_true, y_pred):\n",
    "    # First, we need to convert the one-hot encoded labels and predicted probabilities\n",
    "    # into a single label and probability for each sample.\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = y_pred[:, 1:]\n",
    "\n",
    "    # Then we can use the roc_auc_score function to compute the AUC for each class\n",
    "    auc_scores = []\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        auc_scores.append(roc_auc_score(y_true, y_pred[:, i]))\n",
    "\n",
    "    # Return the average AUC across all classes\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "\n",
    "def dice_score(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "\n",
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1, 2))\n",
    "    sum_ = tf.reduce_sum(y_true + y_pred, axis=(1, 2))\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    jd = (1 - jac) * smooth\n",
    "    return tf.reduce_mean(jd)\n",
    "\n",
    "\n",
    "def tf_mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.5):\n",
    "        y_pred_ = tf.cast(y_pred > t, tf.int32)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        prec.append(score)\n",
    "    val = K.mean(K.stack(prec), axis=0)\n",
    "    return [val, up_opt]\n",
    "\n",
    "\n",
    "def cross_entropy_balanced(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    count_neg = tf.reduce_sum(1.0 - y_true)\n",
    "    count_pos = tf.reduce_sum(y_true)\n",
    "\n",
    "    beta = count_neg / (count_pos + count_neg)\n",
    "\n",
    "    pos_weight = beta / (1 - beta)\n",
    "\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(\n",
    "        logits=y_pred, labels=y_true, pos_weight=pos_weight\n",
    "    )\n",
    "\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "\n",
    "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n",
    "\n",
    "\n",
    "def pixel_error(y_true, y_pred):\n",
    "    pred = tf.cast(tf.greater(y_pred, 0.5), tf.int32)\n",
    "    error = tf.cast(tf.not_equal(pred, tf.cast(y_true, tf.int32)), tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4juusIVfSeEP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    "    load_img,\n",
    "    img_to_array,\n",
    ")\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "# data preprocessing\n",
    "parent_folder_path = \"/content/ICIAR2018_BACH_Challenge_JPG/\" # folder in which the classes are stored as subfolders\n",
    "target_size = (256, 256)\n",
    "test_ratio = 0.3  # ratio compared to the original dataset size\n",
    "num_augmented_images = 10 # number of augmented images per image\n",
    "\n",
    "\n",
    "# augmentation methods\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Rotate(limit=20, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
    "                A.RandomGamma(gamma_limit=(80, 120)),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Affine(scale=(0.8, 1.2), p=0.5),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.2, scale_limit=0.2, rotate_limit=20, p=0.5\n",
    "                ),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.18,\n",
    "                    scale_limit=0.18,\n",
    "                    rotate_limit=18,\n",
    "                    border_mode=cv2.BORDER_CONSTANT,\n",
    "                    value=0,\n",
    "                    interpolation=cv2.INTER_NEAREST,\n",
    "                    p=0.15,\n",
    "                ),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "for class_name in os.listdir(parent_folder_path):\n",
    "    class_folder_path = os.path.join(parent_folder_path, class_name)\n",
    "    if not os.path.isdir(class_folder_path):\n",
    "        continue\n",
    "\n",
    "    test_class_folder_path = os.path.join(\n",
    "        parent_folder_path, \"..\", parent_folder_path, class_name\n",
    "    )\n",
    "    os.makedirs(test_class_folder_path, exist_ok=True)\n",
    "\n",
    "    img_names = [\n",
    "        img_name\n",
    "        for img_name in os.listdir(class_folder_path)\n",
    "        if img_name.lower().endswith((\".tif\", \".png\", \".jpg\", \".jpeg\"))\n",
    "    ]\n",
    "\n",
    "    np.random.shuffle(img_names)\n",
    "    num_test_images = int(len(img_names) * test_ratio)\n",
    "\n",
    "    for img_name in tqdm(\n",
    "        img_names[:num_test_images], colour=\"#009fe9\", desc=\"Creating Test\"\n",
    "    ):\n",
    "        img = Image.open(os.path.join(class_folder_path, img_name))\n",
    "        img = img.resize(target_size)\n",
    "        img.save(os.path.join(test_class_folder_path, img_name))\n",
    "        os.remove(os.path.join(class_folder_path, img_name))\n",
    "\n",
    "    for img_name in tqdm(\n",
    "        img_names[num_test_images:], colour=\"#009fe9\", desc=\"Augmenting\"\n",
    "    ):\n",
    "        img = Image.open(os.path.join(class_folder_path, img_name))\n",
    "        img = img.resize(target_size)\n",
    "        img_array = np.array(img)\n",
    "\n",
    "        # Apply the transformations to the image array\n",
    "        transformed = transform(image=img_array)\n",
    "        img_array_aug = transformed[\"image\"]\n",
    "\n",
    "        # Convert the augmented image back to PIL image\n",
    "        img_aug = Image.fromarray(np.uint8(img_array_aug))\n",
    "\n",
    "        # Save the augmented image\n",
    "        for i in range(num_augmented_images):\n",
    "            img_aug.save(\n",
    "                os.path.join(\n",
    "                    class_folder_path, f\"aug_{img_name.split('.')[0]}_{i}.jpeg\"\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGk7rrVoJQSH",
    "outputId": "1745e6ee-424d-4318-fadc-dcf9b8ffcc83"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_path = \"/content/ICIAR2018_BACH_Challenge_JPG\"\n",
    "test_path = \"/content/ICIAR2018_BACH_test\"\n",
    "\n",
    "# check number of images in each class\n",
    "def check_nums(dataset_path):\n",
    "    class_folders = os.listdir(dataset_path)\n",
    "    for class_folder in class_folders:\n",
    "        class_folder_path = os.path.join(dataset_path, class_folder)\n",
    "        if os.path.isdir(class_folder_path):\n",
    "            num_images = len(os.listdir(class_folder_path))\n",
    "            print(f\"Class {class_folder} has {num_images} images.\")\n",
    "\n",
    "\n",
    "check_nums(train_path)\n",
    "check_nums(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLin27PDVXhM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# data generator following keras.utils.Sequence\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        directory,\n",
    "        batch_size,\n",
    "        target_size=(150, 150),\n",
    "        shuffle=True,\n",
    "        augmentations=None,\n",
    "    ):\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentations = augmentations\n",
    "        self.class_names = sorted(os.listdir(directory))\n",
    "        self.num_classes = len(self.class_names)\n",
    "        self.samples = []\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(self.directory, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                self.samples.append((os.path.join(class_dir, filename), i))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.samples) / float(self.batch_size)))\n",
    "\n",
    "    # get images and labels and preprocess them\n",
    "    def __getitem__(self, idx):\n",
    "        batch_samples = self.samples[\n",
    "            idx * self.batch_size : (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for sample in batch_samples:\n",
    "            image = cv2.imread(sample[0])\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, self.target_size)\n",
    "            label = sample[1]\n",
    "            if self.augmentations is not None:\n",
    "                augmented = self.augmentations(image=image)\n",
    "                image = augmented[\"image\"]\n",
    "            batch_images.append(image)\n",
    "            batch_labels.append(label)\n",
    "        return np.array(batch_images) / 255.0, tf.keras.utils.to_categorical(\n",
    "            batch_labels, num_classes=self.num_classes\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.samples)\n",
    "\n",
    "\n",
    "# data augmentation if needed\n",
    "train_augmentations = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.3, brightness_limit=0.2, contrast_limit=0.2),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.GaussianBlur(p=0.25, blur_limit=(3, 7)),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=(-20, 20), p=0.5, interpolation=cv2.INTER_NEAREST),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.18,\n",
    "                    scale_limit=0.18,\n",
    "                    rotate_limit=18,\n",
    "                    border_mode=cv2.BORDER_CONSTANT,\n",
    "                    value=0,\n",
    "                    interpolation=cv2.INTER_NEAREST,\n",
    "                ),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.25,\n",
    "                    scale_limit=0.25,\n",
    "                    rotate_limit=25,\n",
    "                    interpolation=cv2.INTER_NEAREST,\n",
    "                ),\n",
    "            ],\n",
    "            p=0.4,\n",
    "        ),\n",
    "        A.RGBShift(p=0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 32 # batch size\n",
    "image_size = 256 # height and width of images\n",
    "\n",
    "\n",
    "# get data generators\n",
    "def get_fold_generator(\n",
    "    fold_path,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(image_size, image_size),\n",
    "    shuffle=True,\n",
    "    augmentations=train_augmentations,\n",
    "    mode=\"TRAIN\",\n",
    "):\n",
    "    if mode == \"TRAIN\":\n",
    "        generator = CustomDataGenerator(\n",
    "            directory=fold_path,\n",
    "            batch_size=batch_size,\n",
    "            target_size=target_size,\n",
    "            shuffle=shuffle,\n",
    "            augmentations=augmentations,\n",
    "        )\n",
    "    elif mode == \"VAL\":\n",
    "        generator = CustomDataGenerator(\n",
    "            directory=fold_path,\n",
    "            batch_size=batch_size,\n",
    "            target_size=target_size,\n",
    "            shuffle=shuffle,\n",
    "            augmentations=augmentations,\n",
    "        )\n",
    "    elif mode == \"TEST\":\n",
    "        generator = CustomDataGenerator(\n",
    "            directory=fold_path,\n",
    "            batch_size=batch_size,\n",
    "            target_size=target_size,\n",
    "            shuffle=False,\n",
    "            augmentations=None,\n",
    "        )\n",
    "    return generator\n",
    "\n",
    "\n",
    "train_generator = get_fold_generator(\n",
    "    \"/content/ICIAR2018_BACH_Challenge_JPG/\", mode=\"TRAIN\"\n",
    ")\n",
    "test_generator = get_fold_generator(\"/content/ICIAR2018_BACH_test/\", mode=\"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "12mTiChgVZy6",
    "outputId": "dfc8ed8c-97ee-4f10-df29-2080fe99bde0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_x, batch_y = train_generator.__getitem__(0)\n",
    "batch_x1, batch_y1 = test_generator.__getitem__(0)\n",
    "\n",
    "# check data integrity\n",
    "def check(x, y):\n",
    "    print(f\"shapes: x: {x.shape}, y: {y.shape}\")\n",
    "    print(f\"norms: x: {np.min(x), np.max(x)}, y: {np.min(y), np.max(y)}\")\n",
    "    print(f\"types: x: {type(x)}, y; {type(y)}\")\n",
    "\n",
    "\n",
    "check(batch_x, batch_y)\n",
    "check(batch_x1, batch_y1)\n",
    "\n",
    "\n",
    "# display sample images\n",
    "def dis_gen(x, y):\n",
    "    n = len(x)\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=n, figsize=(30, 30))\n",
    "    for i in range(n):\n",
    "        axs[i].imshow(x[i])\n",
    "        axs[i].set_title(f\"Label: {y[i]}\")\n",
    "        axs[i].axis(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dis_gen(batch_x, batch_y)\n",
    "dis_gen(batch_x1, batch_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RSfo1T2HVfYV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a EfficientNetV2 Model as defined in:\n",
    "Mingxing Tan, Quoc V. Le. (2021).\n",
    "EfficientNetV2: Smaller Models and Faster Training\n",
    "arXiv preprint arXiv:2104.00298.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Implementation by leondgarse:\n",
    "https://github.com/leondgarse\n",
    "slightly modified by using Mish activation function \n",
    "\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Add,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    DepthwiseConv2D,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling2D,\n",
    "    Input,\n",
    "    PReLU,\n",
    "    Reshape,\n",
    "    Multiply,\n",
    "    GroupNormalization,\n",
    "    MultiHeadAttention,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "# using mish activation function instead of relu or gelu\n",
    "class Mish(Activation):\n",
    "    \"\"\"\n",
    "    Mish Activation Function.\n",
    "    .. math::\n",
    "        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))\n",
    "    Shape:\n",
    "        - Input: Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "        - Output: Same shape as the input.\n",
    "    Examples:\n",
    "        >>> X = Activation('Mish', name=\"conv1_act\")(X_input)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = \"Mish\"\n",
    "\n",
    "\n",
    "def mish(inputs):\n",
    "    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "\n",
    "get_custom_objects().update({\"Mish\": Mish(mish)})\n",
    "\n",
    "BATCH_NORM_DECAY = 0.9\n",
    "BATCH_NORM_EPSILON = 0.001\n",
    "TORCH_BATCH_NORM_EPSILON = 1e-5\n",
    "CONV_KERNEL_INITIALIZER = keras.initializers.VarianceScaling(\n",
    "    scale=2.0, mode=\"fan_out\", distribution=\"truncated_normal\", seed=42\n",
    ")\n",
    "# CONV_KERNEL_INITIALIZER = 'glorot_uniform'\n",
    "\n",
    "BLOCK_CONFIGS = {\n",
    "    \"b0\": {  # width 1.0, depth 1.0\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 48, 96, 112, 192],\n",
    "        \"depthes\": [1, 2, 2, 3, 5, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"b1\": {  # width 1.0, depth 1.1\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 48, 96, 112, 192],\n",
    "        \"depthes\": [2, 3, 3, 4, 6, 9],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"b2\": {  # width 1.1, depth 1.2\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1408,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 32, 56, 104, 120, 208],\n",
    "        \"depthes\": [2, 3, 3, 4, 6, 10],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"b3\": {  # width 1.2, depth 1.4\n",
    "        \"first_conv_filter\": 40,\n",
    "        \"output_conv_filter\": 1536,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [16, 40, 56, 112, 136, 232],\n",
    "        \"depthes\": [2, 3, 3, 5, 7, 12],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"t\": {  # width 1.4 * 0.8, depth 1.8 * 0.9, from timm\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1024,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 40, 48, 104, 128, 208],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 14],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"torch\",\n",
    "    },\n",
    "    \"s\": {  # width 1.4, depth 1.8\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 48, 64, 128, 160, 256],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 15],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"early\": {  # S model discribed in paper early version https://arxiv.org/pdf/2104.00298v2.pdf\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1792,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6],\n",
    "        \"out_channels\": [24, 48, 64, 128, 160, 272],\n",
    "        \"depthes\": [2, 4, 4, 6, 9, 15],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"m\": {  # width 1.6, depth 2.2\n",
    "        \"first_conv_filter\": 24,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [24, 48, 80, 160, 176, 304, 512],\n",
    "        \"depthes\": [3, 5, 5, 7, 14, 18, 5],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"l\": {  # width 2.0, depth 3.1\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [32, 64, 96, 192, 224, 384, 640],\n",
    "        \"depthes\": [4, 7, 7, 10, 19, 25, 7],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "    \"xl\": {\n",
    "        \"first_conv_filter\": 32,\n",
    "        \"output_conv_filter\": 1280,\n",
    "        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n",
    "        \"out_channels\": [32, 64, 96, 192, 256, 512, 640],\n",
    "        \"depthes\": [4, 8, 8, 16, 24, 32, 8],\n",
    "        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n",
    "        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n",
    "        \"rescale_mode\": \"tf\",\n",
    "    },\n",
    "}\n",
    "\n",
    "FILE_HASH_DICT = {\n",
    "    \"b0\": {\n",
    "        \"21k-ft1k\": \"4e4da4eb629897e4d6271e131039fe75\",\n",
    "        \"21k\": \"5dbb4252df24b931e74cdd94d150f25a\",\n",
    "        \"imagenet\": \"9abdc43cb00f4cb06a8bdae881f412d6\",\n",
    "    },\n",
    "    \"b1\": {\n",
    "        \"21k-ft1k\": \"5f1aee82209f4f0f20bd24460270564e\",\n",
    "        \"21k\": \"a50ae65b50ceff7f5283be2f4506d2c2\",\n",
    "        \"imagenet\": \"5d4223b59ff268828d5112a1630e234e\",\n",
    "    },\n",
    "    \"b2\": {\n",
    "        \"21k-ft1k\": \"ec384b84441ddf6419938d1e5a0cbef2\",\n",
    "        \"21k\": \"9f718a8bbb7b63c5313916c5e504790d\",\n",
    "        \"imagenet\": \"1814bc08d4bb7a5e0ed3ccfe1cf18650\",\n",
    "    },\n",
    "    \"b3\": {\n",
    "        \"21k-ft1k\": \"4a27827b0b2df508bed31ae231003bb1\",\n",
    "        \"21k\": \"ade5bdbbdf1d54c4561aa41511525855\",\n",
    "        \"imagenet\": \"cda85b8494c7ec5a68dffb335a254bab\",\n",
    "    },\n",
    "    \"l\": {\n",
    "        \"21k-ft1k\": \"30327edcf1390d10e9a0de42a2d731e3\",\n",
    "        \"21k\": \"7970f913eec1b4918e007c8580726412\",\n",
    "        \"imagenet\": \"2b65f5789f4d2f1bf66ecd6d9c5c2d46\",\n",
    "    },\n",
    "    \"m\": {\n",
    "        \"21k-ft1k\": \"0c236c3020e3857de1e5f2939abd0cc6\",\n",
    "        \"21k\": \"3923c286366b2a5137f39d1e5b14e202\",\n",
    "        \"imagenet\": \"ac3fd0ff91b35d18d1df8f1895efe1d5\",\n",
    "    },\n",
    "    \"s\": {\n",
    "        \"21k-ft1k\": \"93046a0d601da46bfce9d4ca14224c83\",\n",
    "        \"21k\": \"10b05d878b64f796ab984a5316a4a1c3\",\n",
    "        \"imagenet\": \"3b91df2c50c7a56071cca428d53b8c0d\",\n",
    "    },\n",
    "    \"t\": {\"imagenet\": \"4a0ff9cb396665734d7ca590fa29681b\"},\n",
    "    \"xl\": {\n",
    "        \"21k-ft1k\": \"9aaa2bd3c9495b23357bc6593eee5bce\",\n",
    "        \"21k\": \"c97de2770f55701f788644336181e8ee\",\n",
    "    },\n",
    "    \"v1-b0\": {\n",
    "        \"noisy_student\": \"d125a518737c601f8595937219243432\",\n",
    "        \"imagenet\": \"cc7d08887de9df8082da44ce40761986\",\n",
    "    },\n",
    "    \"v1-b1\": {\n",
    "        \"noisy_student\": \"8f44bff58fc5ef99baa3f163b3f5c5e8\",\n",
    "        \"imagenet\": \"a967f7be55a0125c898d650502c0cfd0\",\n",
    "    },\n",
    "    \"v1-b2\": {\n",
    "        \"noisy_student\": \"b4ffed8b9262df4facc5e20557983ef8\",\n",
    "        \"imagenet\": \"6c8d1d3699275c7d1867d08e219e00a7\",\n",
    "    },\n",
    "    \"v1-b3\": {\n",
    "        \"noisy_student\": \"9d696365378a1ebf987d0e46a9d26ddd\",\n",
    "        \"imagenet\": \"d78edb3dc7007721eda781c04bd4af62\",\n",
    "    },\n",
    "    \"v1-b4\": {\n",
    "        \"noisy_student\": \"a0f61b977544493e6926186463d26294\",\n",
    "        \"imagenet\": \"4c83aa5c86d58746a56675565d4f2051\",\n",
    "    },\n",
    "    \"v1-b5\": {\n",
    "        \"noisy_student\": \"c3b6eb3f1f7a1e9de6d9a93e474455b1\",\n",
    "        \"imagenet\": \"0bda50943b8e8d0fadcbad82c17c40f5\",\n",
    "    },\n",
    "    \"v1-b6\": {\n",
    "        \"noisy_student\": \"20dd18b0df60cd7c0387c8af47bd96f8\",\n",
    "        \"imagenet\": \"da13735af8209f675d7d7d03a54bfa27\",\n",
    "    },\n",
    "    \"v1-b7\": {\n",
    "        \"noisy_student\": \"7f6f6dd4e8105e32432607ad28cfad0f\",\n",
    "        \"imagenet\": \"d9c22b5b030d1e4f4c3a96dbf5f21ce6\",\n",
    "    },\n",
    "    \"v1-l2\": {\"noisy_student\": \"5fedc721febfca4b08b03d1f18a4a3ca\"},\n",
    "}\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor=4, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "def conv2d_no_bias(\n",
    "    inputs,\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=1,\n",
    "    padding=\"VALID\",\n",
    "    use_torch_padding=False,\n",
    "    name=\"\",\n",
    "):\n",
    "    pad = (\n",
    "        max(kernel_size) // 2\n",
    "        if isinstance(kernel_size, (list, tuple))\n",
    "        else kernel_size // 2\n",
    "    )\n",
    "    if use_torch_padding and padding.upper() == \"SAME\" and pad != 0:\n",
    "        inputs = keras.layers.ZeroPadding2D(padding=pad, name=name and name + \"pad\")(\n",
    "            inputs\n",
    "        )\n",
    "        padding = \"VALID\"\n",
    "\n",
    "    return Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "        name=name + \"conv\",\n",
    "    )(inputs)\n",
    "\n",
    "\n",
    "def self_attention(inputs, name):\n",
    "    \"\"\"\n",
    "    Self-attention block for 2D inputs. Adapted from https://github.com/philipperemy/keras-attention-mechanism.\n",
    "    \"\"\"\n",
    "    channels = inputs.shape[-1]\n",
    "    x = Conv2D(\n",
    "        channels // 8,\n",
    "        (1, 1),\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        name=name + \"_conv1\",\n",
    "    )(inputs)\n",
    "    x = BatchNormalization(name=name + \"_bn1\")(x)\n",
    "    x = Activation(\"relu\", name=name + \"_relu1\")(x)\n",
    "    x = Conv2D(\n",
    "        channels // 8,\n",
    "        (3, 3),\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        name=name + \"_conv2\",\n",
    "    )(x)\n",
    "    x = BatchNormalization(name=name + \"_bn2\")(x)\n",
    "    x = Activation(\"relu\", name=name + \"_relu2\")(x)\n",
    "    x = Conv2D(\n",
    "        channels,\n",
    "        (1, 1),\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        name=name + \"_conv3\",\n",
    "    )(x)\n",
    "    x = BatchNormalization(name=name + \"_bn3\")(x)\n",
    "    x = Activation(\"sigmoid\", name=name + \"_sigmoid\")(x)\n",
    "    x = Multiply(name=name + \"_multiply\")([inputs, x])\n",
    "    x = Add(name=name + \"_add\")([inputs, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def batchnorm_with_activation(inputs, activation=\"Mish\", use_torch_eps=False, name=\"\"):\n",
    "    \"\"\"Performs a batch normalization followed by an activation.\"\"\"\n",
    "    bn_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    nn = BatchNormalization(\n",
    "        axis=bn_axis,\n",
    "        momentum=BATCH_NORM_DECAY,\n",
    "        epsilon=TORCH_BATCH_NORM_EPSILON if use_torch_eps else BATCH_NORM_EPSILON,\n",
    "        name=name + \"bn\",\n",
    "    )(inputs)\n",
    "    if activation:\n",
    "        nn = Activation(activation=activation, name=name + activation)(nn)\n",
    "        # nn = PReLU(shared_axes=[1, 2], alpha_initializer=tf.initializers.Constant(0.25), name=name + \"PReLU\")(nn)\n",
    "    return nn\n",
    "\n",
    "\n",
    "def groupnorm_with_activation(inputs, activation=\"Mish\", num_groups=8, name=\"\"):\n",
    "    \"\"\"Performs group normalization followed by an activation.\"\"\"\n",
    "    bn_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    nn = GroupNormalization(\n",
    "        axis=bn_axis,\n",
    "        groups=num_groups,\n",
    "        name=name + \"gn\",\n",
    "    )(inputs)\n",
    "    if activation:\n",
    "        nn = Activation(activation=activation, name=name + activation)(nn)\n",
    "    return nn\n",
    "\n",
    "\n",
    "def se_module(inputs, se_ratio=4, name=\"\"):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    h_axis, w_axis = [2, 3] if K.image_data_format() == \"channels_first\" else [1, 2]\n",
    "\n",
    "    filters = inputs.shape[channel_axis]\n",
    "    # reduction = _make_divisible(filters // se_ratio, 8)\n",
    "    reduction = filters // se_ratio\n",
    "    # se = GlobalAveragePooling2D()(inputs)\n",
    "    # se = Reshape((1, 1, filters))(se)\n",
    "    se = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)\n",
    "    se = Conv2D(\n",
    "        reduction,\n",
    "        kernel_size=1,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "        name=name + \"1_conv\",\n",
    "    )(se)\n",
    "    # se = PReLU(shared_axes=[1, 2])(se)\n",
    "    se = Activation(\"Mish\")(se)\n",
    "    se = Conv2D(\n",
    "        filters,\n",
    "        kernel_size=1,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "        name=name + \"2_conv\",\n",
    "    )(se)\n",
    "    se = Activation(\"sigmoid\")(se)\n",
    "    return Multiply()([inputs, se])\n",
    "\n",
    "\n",
    "def MBConv(\n",
    "    inputs,\n",
    "    output_channel,\n",
    "    stride,\n",
    "    expand_ratio,\n",
    "    shortcut,\n",
    "    kernel_size=3,\n",
    "    drop_rate=0,\n",
    "    use_se=0,\n",
    "    is_fused=False,\n",
    "    is_torch_mode=False,\n",
    "    name=\"\",\n",
    "    use_group=False,\n",
    "    use_self_attention=False,\n",
    "):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    input_channel = inputs.shape[channel_axis]\n",
    "\n",
    "    if is_fused and expand_ratio != 1:\n",
    "        nn = conv2d_no_bias(\n",
    "            inputs,\n",
    "            input_channel * expand_ratio,\n",
    "            (3, 3),\n",
    "            stride,\n",
    "            padding=\"same\",\n",
    "            use_torch_padding=is_torch_mode,\n",
    "            name=name + \"sortcut_\",\n",
    "        )\n",
    "        if use_group:\n",
    "            nn = groupnorm_with_activation(\n",
    "                nn, activation=\"Mish\", name=name + \"shortcut_\"\n",
    "            )\n",
    "        else:\n",
    "            nn = batchnorm_with_activation(\n",
    "                nn, use_torch_eps=is_torch_mode, name=name + \"sortcut_\"\n",
    "            )\n",
    "    elif expand_ratio != 1:\n",
    "        nn = conv2d_no_bias(\n",
    "            inputs,\n",
    "            input_channel * expand_ratio,\n",
    "            (1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding=\"valid\",\n",
    "            name=name + \"sortcut_\",\n",
    "        )\n",
    "        if use_group:\n",
    "            nn = groupnorm_with_activation(\n",
    "                nn, activation=\"Mish\", name=name + \"shortcut_\"\n",
    "            )\n",
    "        else:\n",
    "            nn = batchnorm_with_activation(\n",
    "                nn, use_torch_eps=is_torch_mode, name=name + \"sortcut_\"\n",
    "            )\n",
    "    else:\n",
    "        nn = inputs\n",
    "    #   if use_self_attention:\n",
    "    #    nn = self_attention(nn, name=name+\"_self_attention_1_\")\n",
    "\n",
    "    if not is_fused:\n",
    "        if is_torch_mode and kernel_size // 2 > 0:\n",
    "            nn = keras.layers.ZeroPadding2D(\n",
    "                padding=kernel_size // 2, name=name + \"pad\"\n",
    "            )(nn)\n",
    "            pad = \"VALID\"\n",
    "        else:\n",
    "            pad = \"SAME\"\n",
    "        nn = DepthwiseConv2D(\n",
    "            kernel_size,\n",
    "            padding=pad,\n",
    "            strides=stride,\n",
    "            use_bias=False,\n",
    "            depthwise_initializer=CONV_KERNEL_INITIALIZER,\n",
    "            name=name + \"MB_dw_\",\n",
    "        )(nn)\n",
    "        if use_group:\n",
    "            nn = groupnorm_with_activation(nn, activation=\"Mish\", name=name + \"MB_dv_\")\n",
    "        else:\n",
    "            nn = batchnorm_with_activation(\n",
    "                nn, use_torch_eps=is_torch_mode, name=name + \"MB_dv_\"\n",
    "            )\n",
    "    #  if use_self_attention:\n",
    "    #   nn = self_attention(nn, name=name+\"_self_attention_2_\")\n",
    "\n",
    "    if use_se:\n",
    "        nn = se_module(nn, se_ratio=4 * expand_ratio, name=name + \"se_\")\n",
    "\n",
    "    # pw-linear\n",
    "    if is_fused and expand_ratio == 1:\n",
    "        nn = conv2d_no_bias(\n",
    "            nn,\n",
    "            output_channel,\n",
    "            (3, 3),\n",
    "            strides=stride,\n",
    "            padding=\"same\",\n",
    "            use_torch_padding=is_torch_mode,\n",
    "            name=name + \"fu_\",\n",
    "        )\n",
    "        if use_group:\n",
    "            nn = groupnorm_with_activation(nn, activation=\"Mish\", name=name + \"fu_\")\n",
    "        else:\n",
    "            nn = batchnorm_with_activation(\n",
    "                nn, use_torch_eps=is_torch_mode, name=name + \"fu_\"\n",
    "            )\n",
    "    else:\n",
    "        nn = conv2d_no_bias(\n",
    "            nn,\n",
    "            output_channel,\n",
    "            (1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding=\"valid\",\n",
    "            name=name + \"MB_pw_\",\n",
    "        )\n",
    "        if use_group:\n",
    "            nn = groupnorm_with_activation(nn, activation=\"Mish\", name=name + \"MB_pw_\")\n",
    "        else:\n",
    "            nn = batchnorm_with_activation(\n",
    "                nn, use_torch_eps=is_torch_mode, name=name + \"MB_pw_\"\n",
    "            )\n",
    "    if use_self_attention:\n",
    "        nn = self_attention(nn, name=name + \"_self_attention_3_\")\n",
    "    if shortcut:\n",
    "        if drop_rate > 0:\n",
    "            nn = Dropout(drop_rate, noise_shape=(None, 1, 1, 1), name=name + \"drop\")(nn)\n",
    "        return Add()([inputs, nn])\n",
    "    else:\n",
    "        return nn\n",
    "\n",
    "\n",
    "def EfficientNetV2(\n",
    "    model_type,\n",
    "    input_shape=(None, None, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    first_strides=2,\n",
    "    is_torch_mode=False,\n",
    "    drop_connect_rate=0,\n",
    "    classifier_activation=\"softmax\",\n",
    "    include_preprocessing=False,\n",
    "    pretrained=\"imagenet\",\n",
    "    model_name=\"EfficientNetV2\",\n",
    "    kwargs=None,  # Not used, just recieving parameter\n",
    "):\n",
    "    if isinstance(model_type, dict):  # For EfficientNetV1 configures\n",
    "        model_type, blocks_config = model_type.popitem()\n",
    "    else:\n",
    "        blocks_config = BLOCK_CONFIGS.get(model_type.lower(), BLOCK_CONFIGS[\"s\"])\n",
    "    expands = blocks_config[\"expands\"]\n",
    "    out_channels = blocks_config[\"out_channels\"]\n",
    "    depthes = blocks_config[\"depthes\"]\n",
    "    strides = blocks_config[\"strides\"]\n",
    "    use_ses = blocks_config[\"use_ses\"]\n",
    "    first_conv_filter = blocks_config.get(\"first_conv_filter\", out_channels[0])\n",
    "    output_conv_filter = blocks_config.get(\"output_conv_filter\", 1280)\n",
    "    kernel_sizes = blocks_config.get(\"kernel_sizes\", [3] * len(depthes))\n",
    "    # \"torch\" for all V1 models\n",
    "    # for V2 models, \"21k\" pretrained are all \"tf\", \"imagenet\" pretrained \"bx\" models are all \"torch\", [\"s\", \"m\", \"l\", \"xl\"] are \"tf\"\n",
    "    rescale_mode = (\n",
    "        \"tf\"\n",
    "        if pretrained is not None and pretrained.startswith(\"imagenet21k\")\n",
    "        else blocks_config.get(\"rescale_mode\", \"torch\")\n",
    "    )\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    if include_preprocessing and rescale_mode == \"torch\":\n",
    "        channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "        Normalization = (\n",
    "            keras.layers.Normalization\n",
    "            if hasattr(keras.layers, \"Normalization\")\n",
    "            else keras.layers.experimental.preprocessing.Normalization\n",
    "        )\n",
    "        mean = tf.constant([0.485, 0.456, 0.406]) * 255.0\n",
    "        std = (tf.constant([0.229, 0.224, 0.225]) * 255.0) ** 2\n",
    "        nn = Normalization(mean=mean, variance=std, axis=channel_axis)(inputs)\n",
    "    elif include_preprocessing and rescale_mode == \"tf\":\n",
    "        Rescaling = (\n",
    "            keras.layers.Rescaling\n",
    "            if hasattr(keras.layers, \"Rescaling\")\n",
    "            else keras.layers.experimental.preprocessing.Rescaling\n",
    "        )\n",
    "        nn = Rescaling(scale=1.0 / 128.0, offset=-1)(inputs)\n",
    "    else:\n",
    "        nn = inputs\n",
    "    out_channel = _make_divisible(first_conv_filter, 8)\n",
    "    nn = conv2d_no_bias(\n",
    "        nn,\n",
    "        out_channel,\n",
    "        (3, 3),\n",
    "        strides=first_strides,\n",
    "        padding=\"same\",\n",
    "        use_torch_padding=is_torch_mode,\n",
    "        name=\"stem_\",\n",
    "    )\n",
    "    nn = batchnorm_with_activation(nn, use_torch_eps=is_torch_mode, name=\"stem_\")\n",
    "\n",
    "    pre_out = out_channel\n",
    "    global_block_id = 0\n",
    "    total_blocks = sum(depthes)\n",
    "    for id, (expand, out_channel, depth, stride, se, kernel_size) in enumerate(\n",
    "        zip(expands, out_channels, depthes, strides, use_ses, kernel_sizes)\n",
    "    ):\n",
    "        out = _make_divisible(out_channel, 8)\n",
    "        is_fused = True if se == 0 else False\n",
    "        for block_id in range(depth):\n",
    "            stride = stride if block_id == 0 else 1\n",
    "            shortcut = True if out == pre_out and stride == 1 else False\n",
    "            name = \"stack_{}_block{}_\".format(id, block_id)\n",
    "            block_drop_rate = drop_connect_rate * global_block_id / total_blocks\n",
    "            nn = MBConv(\n",
    "                nn,\n",
    "                out,\n",
    "                stride,\n",
    "                expand,\n",
    "                shortcut,\n",
    "                kernel_size,\n",
    "                block_drop_rate,\n",
    "                se,\n",
    "                is_fused,\n",
    "                is_torch_mode,\n",
    "                name=name,\n",
    "            )\n",
    "            pre_out = out\n",
    "            global_block_id += 1\n",
    "\n",
    "    output_conv_filter = _make_divisible(output_conv_filter, 8)\n",
    "    nn = conv2d_no_bias(\n",
    "        nn, output_conv_filter, (1, 1), strides=(1, 1), padding=\"valid\", name=\"post_\"\n",
    "    )\n",
    "    nn = batchnorm_with_activation(nn, use_torch_eps=is_torch_mode, name=\"post_\")\n",
    "\n",
    "    if num_classes > 0:\n",
    "        nn = GlobalAveragePooling2D(name=\"avg_pool\")(nn)\n",
    "        if dropout > 0 and dropout < 1:\n",
    "            nn = Dropout(dropout)(nn)\n",
    "        nn = Dense(\n",
    "            num_classes,\n",
    "            activation=classifier_activation,\n",
    "            dtype=\"float32\",\n",
    "            name=\"predictions\",\n",
    "        )(nn)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=nn, name=model_name)\n",
    "    model.rescale_mode = rescale_mode\n",
    "    reload_model_weights(model, model_type, pretrained)\n",
    "    return model\n",
    "\n",
    "\n",
    "def reload_model_weights(model, model_type, pretrained=\"imagenet\"):\n",
    "    pretrained_dd = {\n",
    "        \"imagenet\": \"imagenet\",\n",
    "        \"imagenet21k\": \"21k\",\n",
    "        \"imagenet21k-ft1k\": \"21k-ft1k\",\n",
    "        \"noisy_student\": \"noisy_student\",\n",
    "    }\n",
    "    if not pretrained in pretrained_dd:\n",
    "        print(\">>>> No pretrained available, model will be randomly initialized\")\n",
    "        return\n",
    "    pre_tt = pretrained_dd[pretrained]\n",
    "    if model_type not in FILE_HASH_DICT or pre_tt not in FILE_HASH_DICT[model_type]:\n",
    "        print(\">>>> No pretrained available, model will be randomly initialized\")\n",
    "        return\n",
    "\n",
    "    if model_type.startswith(\"v1\"):\n",
    "        pre_url = \"https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv1_pretrained/efficientnet{}-{}.h5\"\n",
    "    else:\n",
    "        pre_url = \"https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv2_pretrained/efficientnetv2-{}-{}.h5\"\n",
    "    url = pre_url.format(model_type, pre_tt)\n",
    "    file_name = os.path.basename(url)\n",
    "    file_hash = FILE_HASH_DICT[model_type][pre_tt]\n",
    "\n",
    "    try:\n",
    "        pretrained_model = keras.utils.get_file(\n",
    "            file_name, url, cache_subdir=\"models/efficientnetv2\", file_hash=file_hash\n",
    "        )\n",
    "    except:\n",
    "        print(\"[Error] will not load weights, url not found or download failed:\", url)\n",
    "        return\n",
    "    else:\n",
    "        print(\">>>> Load pretrained from:\", pretrained_model)\n",
    "        model.load_weights(pretrained_model, by_name=True, skip_mismatch=True)\n",
    "\n",
    "\n",
    "def EfficientNetV2B0(\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b0\", model_name=\"EfficientNetV2B0\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2B1(\n",
    "    input_shape=(240, 240, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b1\", model_name=\"EfficientNetV2B1\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2B2(\n",
    "    input_shape=(260, 260, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.3,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b2\", model_name=\"EfficientNetV2B2\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2B3(\n",
    "    input_shape=(300, 300, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.3,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"b3\", model_name=\"EfficientNetV2B3\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2T(\n",
    "    input_shape=(320, 320, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    is_torch_mode = True\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"t\", model_name=\"EfficientNetV2T\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2S(\n",
    "    input_shape=(384, 384, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"s\", model_name=\"EfficientNetV2S\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2M(\n",
    "    input_shape=(480, 480, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.3,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"m\", model_name=\"EfficientNetV2M\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2L(\n",
    "    input_shape=(480, 480, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.4,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"l\", model_name=\"EfficientNetV2L\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def EfficientNetV2XL(\n",
    "    input_shape=(512, 512, 3),\n",
    "    num_classes=1000,\n",
    "    dropout=0.4,\n",
    "    classifier_activation=\"softmax\",\n",
    "    pretrained=\"imagenet21k-ft1k\",\n",
    "    **kwargs\n",
    "):\n",
    "    return EfficientNetV2(\n",
    "        model_type=\"xl\", model_name=\"EfficientNetV2XL\", **locals(), **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def get_actual_drop_connect_rates(model):\n",
    "    return [ii.rate for ii in model.layers if isinstance(ii, keras.layers.Dropout)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhIy3LnUVoE3",
    "outputId": "16f17e24-e713-4658-b0a9-b65946e6ba22"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = EfficientNetV2B1(num_classes=batch_y.shape[-1], input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fDja-urXVstX",
    "outputId": "cbf7d39b-4e9e-4f4a-cda6-456d58e49dd2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "#from tensorflow.keras.utils import array_to_image\n",
    "import csv\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional, Callable\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import optimizer\n",
    "!pip install git+https://github.com/artemmavrin/focal-loss.git\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='loss'),\n",
    "    EarlyStopping(monitor='loss', patience=5),\n",
    "    TensorBoard(log_dir='logs'),\n",
    "    ReduceLROnPlateau(monitor='loss', patience=2, factor=0.1),\n",
    "]\n",
    "\n",
    "# functions\n",
    "\n",
    "def focal_loss(gamma=2., alpha=4.):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# update functions\n",
    "\n",
    "@tf.function\n",
    "def update_fn(p, grad, exp_avg, lr, wd, beta1, beta2):\n",
    "    # stepweight decay\n",
    "\n",
    "    p.assign(p * (1 - lr * wd))\n",
    "\n",
    "    # weight update\n",
    "\n",
    "    update = tf.raw_ops.LinSpace(start=1.0, stop=0.0, num=1, name=None)[0]*exp_avg + (1 - tf.raw_ops.LinSpace(start=1.0, stop=0.0, num=1, name=None)[0])*grad\n",
    "    p.assign_add(tf.sign(update) * -lr)\n",
    "\n",
    "    # decay the momentum running average coefficient\n",
    "\n",
    "    exp_avg.assign(exp_avg * beta2 + grad * (1 - beta2))\n",
    "\n",
    "# class\n",
    "def lerp(start, end, weight):\n",
    "    return start + weight * (end - start)\n",
    "\n",
    "\n",
    "def sparse_lerp(start, end, weight):\n",
    "    # Mathematically equivalent, but you can't subtract a dense Tensor from sparse\n",
    "    # IndexedSlices, so we have to flip it around.\n",
    "    return start + weight * -(start - end)\n",
    "\n",
    "# Lion optimizer\n",
    "class Lion(optimizer.Optimizer):\n",
    "    \"\"\"Optimizer that implements the Lion algorithm.\n",
    "    Lion was published in the paper \"Symbolic Discovery of Optimization Algorithms\"\n",
    "    which is available at https://arxiv.org/abs/2302.06675\n",
    "    Args:\n",
    "      learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n",
    "        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
    "        that takes no arguments and returns the actual value to use. The\n",
    "        learning rate. Defaults to 1e-4.\n",
    "      beta_1: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use. Factor\n",
    "         used to interpolate the current gradient and the momentum. Defaults to 0.9.\n",
    "      beta_2: A float value or a constant float tensor, or a callable\n",
    "        that takes no arguments and returns the actual value to use. The\n",
    "        exponential decay rate for the momentum. Defaults to 0.99.\n",
    "    Notes:\n",
    "    The sparse implementation of this algorithm (used when the gradient is an\n",
    "    IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "    lookup in the forward pass) does apply momentum to variable slices even if\n",
    "    they were not used in the forward pass (meaning they have a gradient equal\n",
    "    to zero). Momentum decay (beta2) is also applied to the entire momentum\n",
    "    accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "    behavior (in contrast to some momentum implementations which ignore momentum\n",
    "    unless a variable slice was actually used).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.99,\n",
    "        weight_decay=None,\n",
    "        clipnorm=None,\n",
    "        clipvalue=None,\n",
    "        global_clipnorm=None,\n",
    "        jit_compile=True,\n",
    "        name=\"Lion\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            weight_decay=weight_decay,\n",
    "            clipnorm=clipnorm,\n",
    "            clipvalue=clipvalue,\n",
    "            global_clipnorm=global_clipnorm,\n",
    "            jit_compile=jit_compile,\n",
    "            **kwargs\n",
    "        )\n",
    "        self._learning_rate = self._build_learning_rate(learning_rate)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def build(self, var_list):\n",
    "        \"\"\"Initialize optimizer variables.\n",
    "          var_list: list of model variables to build Lion variables on.\n",
    "        \"\"\"\n",
    "        super().build(var_list)\n",
    "        if hasattr(self, \"_built\") and self._built:\n",
    "            return\n",
    "        self._built = True\n",
    "        self._emas = []\n",
    "        for var in var_list:\n",
    "            self._emas.append(\n",
    "                self.add_variable_from_reference(\n",
    "                    model_variable=var, variable_name=\"ema\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def update_step(self, gradient, variable):\n",
    "        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n",
    "        lr = tf.cast(self.learning_rate, variable.dtype)\n",
    "        beta_1 = tf.constant(self.beta_1, shape=(1,))\n",
    "        beta_2 = tf.constant(self.beta_2, shape=(1,))\n",
    "\n",
    "        var_key = self._var_key(variable)\n",
    "        ema = self._emas[self._index_dict[var_key]]\n",
    "\n",
    "        if isinstance(gradient, tf.IndexedSlices):\n",
    "            # Sparse gradients.\n",
    "            lerp_fn = sparse_lerp\n",
    "        else:\n",
    "            # Dense gradients.\n",
    "            lerp_fn = lerp\n",
    "\n",
    "        update = lerp_fn(ema, gradient, 1 - beta_1)\n",
    "        update = tf.sign(update)\n",
    "        variable.assign_sub(update * lr)\n",
    "\n",
    "        ema.assign(lerp_fn(ema, gradient, 1 - beta_2))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(\n",
    "                    self._learning_rate\n",
    "                ),\n",
    "                \"beta_1\": self.beta_1,\n",
    "                \"beta_2\": self.beta_2,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.compile(optimizer=Lion(learning_rate=5e-5, weight_decay=5e-6), loss=focal_loss(), metrics=[\"accuracy\", f1_m, precision_m, recall_m, tf.keras.metrics.SpecificityAtSensitivity(0.5)])\n",
    "history = model.fit(train_generator, epochs=60, callbacks=callbacks)\n",
    "model.save(\"tumor_model.h5\")\n",
    "evals = model.evaluate(test_generator)\n",
    "\n",
    "# Save the evaluation results to a csv file\n",
    "def save_eval_results(eval_results, filename):\n",
    "    # Open the file in write mode and write the evaluation results to it\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(model.metrics_names)\n",
    "        writer.writerow(eval_results)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training history as a graph\n",
    "def plot_history(history, call=None):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(history.history), figsize=(20, 5))\n",
    "\n",
    "    for i, metric in enumerate(history.history.keys()):\n",
    "        axs[i].plot(history.history[metric])\n",
    "        axs[i].set_title(metric)\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "        axs[i].set_ylabel(metric)\n",
    "    plt.savefig(f'{str(history)}_{call}.png')\n",
    "    plt.show()\n",
    "\n",
    "# Test the model on a batch of test data\n",
    "def quick_test(test_generator, model, img_size):\n",
    "    # Get a batch of test data\n",
    "    batch_x, batch_y = test_generator.__getitem__(0)\n",
    "\n",
    "    # Generate predictions for the test data\n",
    "    pred_y = model.predict(batch_x)\n",
    "\n",
    "    # Create a pred folder if it doesn't exist\n",
    "    if not os.path.exists(\"pred\"):\n",
    "        os.makedirs(\"pred\")\n",
    "\n",
    "    # Loop through the test data and save each image and its predicted label to the pred folder\n",
    "    for i in range(batch_x.shape[0]):\n",
    "        # Convert the image array to a PIL image\n",
    "        img = array_to_img(batch_x[i])\n",
    "\n",
    "        # Get the predicted label for the image\n",
    "        pred_label = np.argmax(pred_y[i])\n",
    "\n",
    "        # Save the image with its predicted label as the filename\n",
    "        img.save(f\"pred/{pred_label}_{i}.jpg\")\n",
    "\n",
    "save_eval_results(evals, \"pannuke_results\")\n",
    "plot_history(history)\n",
    "quick_test(test_generator, model, (256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Bux4KwvVWE--",
    "outputId": "1f2ac446-9f9d-4e1f-b8a5-26d88cc7e435"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# move models to drive\n",
    "shutil.move(\"/content/best_model.h5\", \"/content/drive/MyDrive/tumor_b.h5\")\n",
    "shutil.move(\"/content/tumor_model.h5\", \"/content/drive/MyDrive/tumor.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
